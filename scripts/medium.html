
<!doctype html><html lang="en"><head><title data-rh="true">AutoHyDE: Making HyDE Better for Advanced LLM RAG | by Ian Ho | Apr, 2024 | Towards Data Science</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2024-04-04T17:06:00.920Z"/><meta data-rh="true" name="title" content="AutoHyDE: Making HyDE Better for Advanced LLM RAG | by Ian Ho | Apr, 2024 | Towards Data Science"/><meta data-rh="true" property="og:title" content="AutoHyDE: Making HyDE Better for Advanced LLM RAG"/><meta data-rh="true" property="al:android:url" content="medium://p/619e58cdbd8e"/><meta data-rh="true" property="al:ios:url" content="medium://p/619e58cdbd8e"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="In the field of Retrieval Augmented Generation (RAG), Hypothetical Document Embeddings (HyDE) have proven to be a powerful form of query rewriting to improve the relevance of retrieved documents. For…"/><meta data-rh="true" property="og:description" content="Introducing AutoHyDE, a framework for improving the effectiveness, coverage and adaptability of HyDE for Advanced LLM RAG Applications"/><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/autohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e"/><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/autohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/resize:fit:1200/1*BoEb2eI8BFOectBr5y0zjA.png"/><meta data-rh="true" property="article:author" content="https://ianhojy.medium.com"/><meta data-rh="true" name="author" content="Ian Ho"/><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" property="twitter:title" content="AutoHyDE: Making HyDE Better for Advanced LLM RAG"/><meta data-rh="true" name="twitter:site" content="@TDataScience"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/619e58cdbd8e"/><meta data-rh="true" property="twitter:description" content="Introducing AutoHyDE, a framework for improving the effectiveness, coverage and adaptability of HyDE for Advanced LLM RAG Applications"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/resize:fit:1200/1*BoEb2eI8BFOectBr5y0zjA.png"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="19 min read"/><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/resize:fill:256:256/1*VzTUkfeGymHP4Bvav-T-lA.png"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://ianhojy.medium.com"/><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/autohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/619e58cdbd8e"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:1200\u002F1*BoEb2eI8BFOectBr5y0zjA.png"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fautohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e","dateCreated":"2024-04-04T17:06:00.920Z","datePublished":"2024-04-04T17:06:00.920Z","dateModified":"2024-04-22T01:48:24.768Z","headline":"AutoHyDE: Making HyDE Better for Advanced LLM RAG - Towards Data Science","name":"AutoHyDE: Making HyDE Better for Advanced LLM RAG - Towards Data Science","description":"In the field of Retrieval Augmented Generation (RAG), Hypothetical Document Embeddings (HyDE) have proven to be a powerful form of query rewriting to improve the relevance of retrieved documents. For…","identifier":"619e58cdbd8e","author":{"@type":"Person","name":"Ian Ho","url":"https:\u002F\u002Fianhojy.medium.com"},"creator":["Ian Ho"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":192,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:384\u002F1*cFFKn8rFH4ZndmaYeAs6iQ.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fautohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e","isAccessibleForFree":"False","hasPart":{"@type":"WebPageElement","isAccessibleForFree":"False","cssSelector":".meteredContent"}}</script><style type="text/css" data-fela-rehydration="559" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}
    /*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
    }/* Gray DOCTYPE selectors like WebKit */
    .xml .hljs-meta {color: #c0c0c0;
    }.hljs-comment,
    .hljs-quote {color: #007400;
    }.hljs-tag,
    .hljs-attribute,
    .hljs-keyword,
    .hljs-selector-tag,
    .hljs-literal,
    .hljs-name {color: #aa0d91;
    }.hljs-variable,
    .hljs-template-variable {color: #3F6E74;
    }.hljs-code,
    .hljs-string,
    .hljs-meta .hljs-string {color: #c41a16;
    }.hljs-regexp,
    .hljs-link {color: #0E0EFF;
    }.hljs-title,
    .hljs-symbol,
    .hljs-bullet,
    .hljs-number {color: #1c00cf;
    }.hljs-section,
    .hljs-meta {color: #643820;
    }.hljs-title.class_,
    .hljs-class .hljs-title,
    .hljs-type,
    .hljs-built_in,
    .hljs-params {color: #5c2699;
    }.hljs-attr {color: #836C28;
    }.hljs-subst {color: #000;
    }.hljs-formula {background-color: #eee;font-style: italic;
    }.hljs-addition {background-color: #baeeba;
    }.hljs-deletion {background-color: #ffc8bd;
    }.hljs-selector-id,
    .hljs-selector-class {color: #9b703f;
    }.hljs-doctag,
    .hljs-strong {font-weight: bold;
    }.hljs-emphasis {font-style: italic;
    }
    </style><style type="text/css" data-fela-rehydration="559" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-webkit-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{fill:rgba(0, 0, 0, 1)}.av{height:22px}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.dv{margin-left:8px}.dw{color:#6B6B6B}.dx{font-size:13px}.dy{flex:1}.dz{height:100%}.ea{height:25px}.eb{fill:rgba(41, 41, 41, 1)}.ee{margin-right:32px}.ef{position:relative}.eg{fill:#6B6B6B}.ej{background:transparent}.ek svg{margin-left:4px}.el svg{fill:#6B6B6B}.eo{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.ep{position:absolute}.er{box-sizing:border-box}.es{top:-4px}.et{right:-4px}.eu{width:16px}.fa{margin:0 24px}.fe{background:rgba(255, 255, 255, 1)}.ff{border:1px solid #F2F2F2}.fg{box-shadow:0 1px 4px #F2F2F2}.fh{max-height:100vh}.fi{overflow-y:auto}.fj{left:0}.fk{top:calc(100vh + 100px)}.fl{bottom:calc(100vh + 100px)}.fm{width:10px}.fn{pointer-events:none}.ft{margin-right:4px}.fu{margin-top:2px}.fv{box-sizing:content-box}.fw{word-break:break-word}.fx{word-wrap:break-word}.fy:after{display:block}.fz:after{content:""}.ga:after{clear:both}.gb{line-height:1.23}.gc{letter-spacing:0}.gd{font-style:normal}.ge{font-weight:700}.gu{margin-top:0px}.gv{margin-bottom:-0.27em}.gw{line-height:1.394}.hm{@media all and (max-width: 551.98px):8px}.hn{@media all and (min-width: 552px) and (max-width: 727.98px):8px}.ho{@media all and (min-width: 728px) and (max-width: 903.98px):16px}.hp{@media all and (min-width: 904px) and (max-width: 1079.98px):16px}.hq{@media all and (min-width: 1080px):16px}.hw{align-items:baseline}.hx{width:48px}.hy{height:48px}.hz{border:2px solid rgba(255, 255, 255, 1)}.ia{z-index:0}.ib{box-shadow:none}.ic{border:1px solid rgba(0, 0, 0, 0.05)}.id{margin-left:-12px}.ie{width:28px}.if{height:28px}.ig{z-index:1}.ih{width:24px}.ii{margin-bottom:2px}.ij{flex-wrap:nowrap}.ik{font-size:16px}.il{line-height:24px}.in{margin:0 8px}.io{display:inline}.ip{color:rgba(102, 138, 170, 1)}.iq{fill:rgba(102, 138, 170, 1)}.ir:disabled{opacity:0.3}.iu{flex:0 0 auto}.ix{flex-wrap:wrap}.ja{white-space:pre-wrap}.jb{overflow:hidden}.jc{max-height:20px}.jd{text-overflow:ellipsis}.je{display:-webkit-box}.jf{-webkit-line-clamp:1}.jg{-webkit-box-orient:vertical}.jh{word-break:break-all}.jj{padding-left:8px}.jk{padding-right:8px}.kl> *{flex-shrink:0}.km{overflow-x:scroll}.kn::-webkit-scrollbar{display:none}.ko{scrollbar-width:none}.kp{-ms-overflow-style:none}.kq{width:74px}.kr{flex-direction:row}.ks{z-index:2}.kv{-webkit-user-select:none}.kw{border:0}.kx{cursor:progress}.ky{fill:rgba(117, 117, 117, 1)}.lb{opacity:0.25}.lc{outline:0}.ld{user-select:none}.le> svg{pointer-events:none}.ln{margin-left:4px}.lo{opacity:1}.lp{padding:4px 0}.ls{padding:8px 2px}.lv svg path{fill:#6B6B6B}.lw path{fill:#242424}.lx{display:inline-flex}.md{max-width:100%}.me svg{color:#6B6B6B}.mv{margin-left:auto}.mw{margin-right:auto}.mx{max-width:1230px}.nd{clear:both}.nf{cursor:zoom-in}.ng{z-index:auto}.ni{height:auto}.nj{margin-top:10px}.nk{text-align:center}.nl{max-width:728px}.no{line-height:1.12}.np{letter-spacing:-0.022em}.nq{font-weight:600}.oj{margin-bottom:-0.28em}.ok{line-height:1.58}.ol{letter-spacing:-0.004em}.om{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.pf{margin-bottom:-0.46em}.pl{box-shadow:inset 3px 0 0 0 #242424}.pm{padding-left:23px}.pn{margin-left:-20px}.po{font-style:italic}.pp{padding:2px 4px}.pq{font-size:75%}.pr> strong{font-family:inherit}.ps{font-family:source-code-pro, Menlo, Monaco, "Courier New", Courier, monospace}.pt{text-decoration:underline}.pu{max-width:1696px}.pv{margin-top:32px}.pw{margin-bottom:14px}.px{padding-top:24px}.py{padding-bottom:10px}.pz{background-color:#000000}.qa{height:3px}.qb{width:3px}.qc{margin-right:20px}.qi{line-height:1.18}.qy{margin-bottom:-0.31em}.qz{max-width:2592px}.ra{font-style:inherit}.rb{max-width:1012px}.rc{list-style-type:disc}.rd{margin-left:30px}.re{padding-left:0px}.rk{max-width:878px}.rl{margin:auto}.rm{padding-bottom:100%}.rn{height:0}.ro{overflow-x:auto}.rp{padding:32px}.rq{border:1px solid #E5E5E5}.rr{line-height:1.4}.rs{margin-top:-0.2em}.rt{margin-bottom:-0.2em}.ru{white-space:pre}.rv{min-width:fit-content}.rw{max-width:780px}.rx{max-width:1068px}.ry{margin-top:16px}.rz{margin-bottom:26px}.sa{margin-top:6px}.sb{margin-top:8px}.sc{margin-right:8px}.sd{padding:8px 16px}.se{border-radius:100px}.sf{transition:background 300ms ease}.sh{white-space:nowrap}.si{border-top:none}.so{height:52px}.sp{max-height:52px}.sq{position:static}.ss{max-width:155px}.td{align-items:flex-end}.te{width:76px}.tf{height:76px}.tg{border:2px solid #F9F9F9}.th{height:72px}.ti{width:72px}.tj{margin-left:-16px}.tk{width:36px}.tl{height:36px}.tm{color:#F2F2F2}.tn{fill:#F2F2F2}.to{background:#F2F2F2}.tp{border-color:#F2F2F2}.tv:disabled{cursor:inherit !important}.tw:disabled:hover{background:rgba(102, 138, 170, 1)}.tx:disabled:hover{border-color:rgba(102, 138, 170, 1)}.ty{border-radius:99em}.tz{width:auto}.ua{border-width:1px}.ub{border-style:solid}.uc{text-decoration:none}.ud{stroke:#F2F2F2}.ue{font-weight:500}.uf{font-size:24px}.ug{line-height:30px}.uh{letter-spacing:-0.016em}.ui{height:0px}.uj{border-bottom:solid 1px #E5E5E5}.uk{margin-top:72px}.ul{padding:24px 0}.um{margin-bottom:0px}.un{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.eh:hover{color:#242424}.ei:hover{fill:#242424}.em:hover svg{fill:#242424}.eq:hover{background-color:rgba(0, 0, 0, 0.1)}.im:hover{text-decoration:underline}.is:hover:not(:disabled){color:rgba(90, 118, 144, 1)}.it:hover:not(:disabled){fill:rgba(90, 118, 144, 1)}.la:hover{fill:rgba(117, 117, 117, 1)}.lq:hover{fill:#000000}.lr:hover p{color:#000000}.lt:hover:not(:disabled) svg path{fill:#000000}.mf:hover svg{color:#000000}.sg:hover{background-color:#F2F2F2}.tq:hover{background:#F2F2F2}.tr:hover{border-color:#F2F2F2}.ts:hover{cursor:wait}.tt:hover{color:#F2F2F2}.tu:hover{fill:#F2F2F2}.bc:focus-within path{fill:#242424}.kz:focus{fill:rgba(117, 117, 117, 1)}.lu:focus svg path{fill:#000000}.mg:focus svg{color:#000000}.nh:focus{transform:scale(1.01)}.lf:active{border-style:none}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ed{display:flex}.ez{margin-bottom:68px}.fd{max-width:680px}.fs{margin-top:40px}.gr{font-size:42px}.gs{line-height:52px}.gt{letter-spacing:-0.011em}.hj{font-size:22px}.hk{margin-top:0.92em}.hl{line-height:28px}.hv{align-items:center}.jx{border-top:solid 1px #F2F2F2}.jy{border-bottom:solid 1px #F2F2F2}.jz{margin:32px 0 0}.ka{padding:3px 8px}.kj> *{margin-right:24px}.kk> :last-child{margin-right:0}.lm{margin-top:0px}.mc{margin:0}.nc{margin-top:56px}.of{font-size:24px}.og{margin-top:1.95em}.oh{line-height:30px}.oi{letter-spacing:-0.016em}.pb{font-size:20px}.pc{margin-top:0.94em}.pd{line-height:32px}.pe{letter-spacing:-0.003em}.pk{margin-top:2.14em}.qh{margin-top:1.25em}.qv{margin-top:1.72em}.qw{line-height:24px}.qx{letter-spacing:0}.rj{margin-top:1.14em}.sn{margin-bottom:88px}.sx{display:inline-block}.tc{padding-top:72px}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.ll{margin-top:0px}.nm{margin-left:auto}.nn{text-align:center}.sw{display:inline-block}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.lk{margin-top:0px}.sv{display:inline-block}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.li{margin-top:0px}.lj{margin-right:0px}.su{display:inline-block}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.ds{justify-content:center}.ev{margin-bottom:4px}.fo{margin-top:32px}.gf{font-size:32px}.gg{line-height:38px}.gh{letter-spacing:-0.014em}.gx{font-size:18px}.gy{margin-top:0.79em}.gz{line-height:24px}.hr{align-items:flex-start}.iv{flex-direction:column}.iy{margin-bottom:2px}.jl{margin:24px -24px 0}.jm{padding:0}.kb> *{margin-right:8px}.kc> :last-child{margin-right:24px}.kt{margin-left:0px}.lg{margin-top:0px}.lh{margin-right:0px}.ly{margin:0}.mh{border:1px solid #F2F2F2}.mi{border-radius:99em}.mj{padding:0px 16px 0px 12px}.mk{height:38px}.ml{align-items:center}.mn svg{margin-right:8px}.my{margin-top:40px}.nr{font-size:20px}.ns{margin-top:1.2em}.nt{letter-spacing:0}.on{margin-top:0.67em}.oo{line-height:28px}.op{letter-spacing:-0.003em}.pg{margin-top:1.56em}.qd{margin-top:0.93em}.qj{font-size:16px}.qk{margin-top:1.23em}.ql{line-height:20px}.rf{margin-top:1.34em}.sj{margin-bottom:80px}.st{display:inline-block}.sy{padding-top:48px}.mm:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.ec{display:flex}.ey{margin-bottom:68px}.fc{max-width:680px}.fr{margin-top:40px}.go{font-size:42px}.gp{line-height:52px}.gq{letter-spacing:-0.011em}.hg{font-size:22px}.hh{margin-top:0.92em}.hi{line-height:28px}.hu{align-items:center}.jt{border-top:solid 1px #F2F2F2}.ju{border-bottom:solid 1px #F2F2F2}.jv{margin:32px 0 0}.jw{padding:3px 8px}.kh> *{margin-right:24px}.ki> :last-child{margin-right:0}.mb{margin:0}.nb{margin-top:56px}.ob{font-size:24px}.oc{margin-top:1.95em}.od{line-height:30px}.oe{letter-spacing:-0.016em}.ox{font-size:20px}.oy{margin-top:0.94em}.oz{line-height:32px}.pa{letter-spacing:-0.003em}.pj{margin-top:2.14em}.qg{margin-top:1.25em}.qs{margin-top:1.72em}.qt{line-height:24px}.qu{letter-spacing:0}.ri{margin-top:1.14em}.sm{margin-bottom:88px}.tb{padding-top:72px}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.du{justify-content:center}.ex{margin-bottom:68px}.fb{max-width:680px}.fq{margin-top:40px}.gl{font-size:42px}.gm{line-height:52px}.gn{letter-spacing:-0.011em}.hd{font-size:22px}.he{margin-top:0.92em}.hf{line-height:28px}.ht{align-items:center}.jp{border-top:solid 1px #F2F2F2}.jq{border-bottom:solid 1px #F2F2F2}.jr{margin:32px 0 0}.js{padding:3px 8px}.kf> *{margin-right:24px}.kg> :last-child{margin-right:0}.ma{margin:0}.na{margin-top:56px}.nx{font-size:24px}.ny{margin-top:1.95em}.nz{line-height:30px}.oa{letter-spacing:-0.016em}.ot{font-size:20px}.ou{margin-top:0.94em}.ov{line-height:32px}.ow{letter-spacing:-0.003em}.pi{margin-top:2.14em}.qf{margin-top:1.25em}.qp{margin-top:1.72em}.qq{line-height:24px}.qr{letter-spacing:0}.rh{margin-top:1.14em}.sl{margin-bottom:88px}.ta{padding-top:72px}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dt{justify-content:center}.ew{margin-bottom:4px}.fp{margin-top:32px}.gi{font-size:32px}.gj{line-height:38px}.gk{letter-spacing:-0.014em}.ha{font-size:18px}.hb{margin-top:0.79em}.hc{line-height:24px}.hs{align-items:flex-start}.iw{flex-direction:column}.iz{margin-bottom:2px}.jn{margin:24px 0 0}.jo{padding:0}.kd> *{margin-right:8px}.ke> :last-child{margin-right:8px}.ku{margin-left:0px}.lz{margin:0}.mo{border:1px solid #F2F2F2}.mp{border-radius:99em}.mq{padding:0px 16px 0px 12px}.mr{height:38px}.ms{align-items:center}.mu svg{margin-right:8px}.mz{margin-top:40px}.nu{font-size:20px}.nv{margin-top:1.2em}.nw{letter-spacing:0}.oq{margin-top:0.67em}.or{line-height:28px}.os{letter-spacing:-0.003em}.ph{margin-top:1.56em}.qe{margin-top:0.93em}.qm{font-size:16px}.qn{margin-top:1.23em}.qo{line-height:20px}.rg{margin-top:1.34em}.sk{margin-bottom:80px}.sz{padding-top:48px}.mt:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="print">.sr{display:none}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.ji{max-height:none}</style><style type="text/css" data-fela-rehydration="559" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.ne{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="l c"><div class="l m n o c"><div class="am q r s ds u dt w du i d y z"><a class="dw ag dx be ak b am an ao ap aq ar as at dy s u w i d q ca dz z" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F619e58cdbd8e&amp;%7Efeature=LiOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;source=---two_column_layout_nav----------------------------------" rel="noopener follow">Open in app<svg width="10" height="10" viewBox="0 0 10 10" fill="none" class="dv"><path d="M.98 8.48a.37.37 0 1 0 .54.54l-.54-.54zm7.77-7.23h.38c0-.2-.17-.38-.38-.38v.38zM8.37 6.5a.37.37 0 1 0 .76 0h-.76zM3.5.87a.37.37 0 1 0 0 .76V.88zM1.52 9.03l7.5-7.5-.54-.54-7.5 7.5.54.54zm6.86-7.77V6.5h.74V1.25h-.74zm-4.88.38h5.25V.88H3.5v.74z" fill="currentColor"></path></svg></a></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" href="https://medium.com/?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><svg viewBox="0 0 1043.63 592.71" class="ea eb"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a><div class="aw h"><div class="ab ax ay az ba q bb bc"><div class="bl" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bm bn ab"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ax bd be bf z bg bh bi bj bk" placeholder="Search" value=""/></div></div></div><div class="h k w ec ed"><div class="ee ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerWriteButton" href="https://medium.com/new-story?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ef eg ab q eh ei"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Write"><path d="M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z" fill="currentColor"></path><path d="M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2" stroke="currentColor"></path></svg><div class="dv l">Write</div></div></a></div></div><div class="k j i d"><div class="ee ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSearchButton" href="https://medium.com/search?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ef eg ab q eh ei"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Search"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div></a></div></div><div class="ee ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerNotificationButton" href="https://medium.com/me/notifications?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ef eg ab q eh ei"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Notifications"><path d="M15 18.5a3 3 0 1 1-6 0" stroke="currentColor" stroke-linecap="round"></path><path d="M5.5 10.53V9a6.5 6.5 0 0 1 13 0v1.53c0 1.42.56 2.78 1.57 3.79l.03.03c.26.26.4.6.4.97v2.93c0 .14-.11.25-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.93c0-.37.14-.71.4-.97l.03-.03c1-1 1.57-2.37 1.57-3.79z" stroke="currentColor" stroke-linejoin="round"></path></svg></div></a></div><div class="l" aria-hidden="false"><button class="ax ej am ab q ao ek el em" aria-label="user options menu" data-testid="headerUserIcon"><div class="l ef"><div class="l ef"><img alt="Akash Mishra" class="l er bx by bz cw" src="https://miro.medium.com/v2/resize:fill:64:64/0*kMyeT04xC4BXpBc6.jpg" width="32" height="32" loading="lazy"/><div class="eo bx l by bz ep n ax eq"></div></div><svg width="40" height="40" viewBox="0 0 40 40" fill="none" role="presentation" aria-hidden="true" focusable="false" class="ep es et dg eu"><mask id="star-four-with-outline_svg__a" maskUnits="userSpaceOnUse" x="1" y="1" width="38" height="38" fill="#000"><path fill="#fff" d="M1 1h38v38H1z"></path><path d="M24.7 25.43l-3.55 9.77a1.22 1.22 0 0 1-2.3 0l-3.55-9.77a1.24 1.24 0 0 0-.73-.73L4.8 21.15a1.22 1.22 0 0 1 0-2.3l9.77-3.55a1.24 1.24 0 0 0 .73-.73l3.55-9.77a1.22 1.22 0 0 1 2.3 0l3.55 9.77a1.24 1.24 0 0 0 .73.73l9.77 3.55a1.22 1.22 0 0 1 0 2.3l-9.77 3.55a1.24 1.24 0 0 0-.73.73z"></path></mask><path d="M24.7 25.43l-3.55 9.77a1.22 1.22 0 0 1-2.3 0l-3.55-9.77a1.24 1.24 0 0 0-.73-.73L4.8 21.15a1.22 1.22 0 0 1 0-2.3l9.77-3.55a1.24 1.24 0 0 0 .73-.73l3.55-9.77a1.22 1.22 0 0 1 2.3 0l3.55 9.77a1.24 1.24 0 0 0 .73.73l9.77 3.55a1.22 1.22 0 0 1 0 2.3l-9.77 3.55a1.24 1.24 0 0 0-.73.73z" fill="#FFC017"></path><path d="M21.15 35.2l2.81 1.05v-.02l-2.81-1.03zm-2.3 0l-2.82 1.03v.02l2.82-1.05zm-3.55-9.77l2.82-1.02v-.01l-2.82 1.03zm-.73-.73l1.03-2.82-1.03 2.82zM4.8 21.15l-1.05 2.81h.02l1.03-2.81zm0-2.3l-1.03-2.82h-.02l1.05 2.82zm9.77-3.55l1.02 2.82h.01l-1.03-2.82zm.73-.73l2.82 1.03-2.82-1.03zm3.55-9.77l-2.81-1.05v.02l2.81 1.03zM20 4v3-3zm1.15.8l2.82-1.03v-.02L21.14 4.8zm3.55 9.77l-2.82 1.02v.01l2.82-1.03zm.73.73l-1.03 2.82 1.03-2.82zm9.77 3.55l1.05-2.81h-.02l-1.03 2.81zm0 2.3l1.03 2.82h.02l-1.05-2.82zm-9.77 3.55l-1.02-2.82h-.01l1.03 2.82zm-3.55-.3l-3.55 9.78 5.64 2.05 3.55-9.77-5.64-2.05zm-3.54 9.76c.12-.34.35-.64.65-.84l3.42 4.92c.71-.49 1.25-1.19 1.55-2l-5.62-2.08zm.65-.84c.3-.2.65-.32 1.01-.32v6c.86 0 1.7-.26 2.41-.76L19 33.32zM20 33c.36 0 .72.11 1.01.32l-3.42 4.92c.7.5 1.55.76 2.41.76v-6zm1.01.32c.3.2.53.5.65.84l-5.62 2.09c.3.8.84 1.5 1.55 2L21 33.31zm.66.86l-3.55-9.77-5.64 2.05 3.55 9.77 5.64-2.05zm-3.55-9.78a4.24 4.24 0 0 0-.99-1.53L12.9 27.1c-.18-.18-.32-.4-.4-.64l5.63-2.07zm-.99-1.53a4.24 4.24 0 0 0-1.53-.99l-2.07 5.63a1.76 1.76 0 0 1-.64-.4l4.24-4.24zm-1.54-.99l-9.77-3.55-2.05 5.64 9.77 3.55 2.05-5.64zm-9.75-3.54c.34.12.64.35.84.65L1.76 22.4c.49.71 1.19 1.25 2 1.55l2.08-5.62zm.84.65c.2.3.32.65.32 1.01H1c0 .86.26 1.7.76 2.41L6.68 19zM7 20c0 .36-.11.72-.32 1.01L1.76 17.6c-.5.7-.76 1.55-.76 2.41h6zm-.32 1.01c-.2.3-.5.53-.84.65l-2.09-5.62c-.8.3-1.5.84-2 1.55L6.69 21zm-.86.66l9.77-3.55-2.05-5.64-9.77 3.55 2.05 5.64zm9.78-3.55c.58-.22 1.1-.55 1.53-.99L12.9 12.9c.18-.18.4-.32.64-.4l2.07 5.63zm1.53-.99c.44-.43.77-.95.99-1.53l-5.63-2.07c.08-.24.22-.46.4-.64l4.24 4.24zm.99-1.54l3.55-9.77-5.64-2.05-3.55 9.77 5.64 2.05zm3.54-9.75c-.12.34-.35.64-.65.84L17.6 1.76c-.71.49-1.25 1.19-1.55 2l5.62 2.08zm-.65.84c-.3.2-.65.32-1.01.32V1c-.86 0-1.7.26-2.41.76L21 6.68zM20 7c-.36 0-.72-.11-1.01-.32l3.42-4.92C21.71 1.26 20.86 1 20 1v6zm-1.01-.32c-.3-.2-.53-.5-.65-.84l5.62-2.09c-.3-.8-.84-1.5-1.55-2L19 6.69zm-.66-.86l3.55 9.77 5.64-2.05-3.55-9.77-5.64 2.05zm3.55 9.78c.22.58.55 1.1.99 1.53l4.24-4.24c.18.18.32.4.4.64l-5.63 2.07zm.99 1.53c.43.44.95.77 1.53.99l2.07-5.63c.24.08.46.22.64.4l-4.24 4.24zm1.54.99l9.77 3.55 2.05-5.64-9.77-3.55-2.05 5.64zm9.75 3.54a1.78 1.78 0 0 1-.84-.65l4.92-3.42a4.22 4.22 0 0 0-2-1.55l-2.08 5.62zm-.84-.65c-.2-.3-.32-.65-.32-1.01h6c0-.86-.26-1.7-.76-2.41L33.32 21zM33 20c0-.36.11-.72.32-1.01l4.92 3.42c.5-.7.76-1.55.76-2.41h-6zm.32-1.01c.2-.3.5-.53.84-.65l2.09 5.62c.8-.3 1.5-.84 2-1.55L33.31 19zm.86-.66l-9.77 3.55 2.05 5.64 9.77-3.55-2.05-5.64zm-9.78 3.55c-.58.22-1.1.55-1.53.99l4.24 4.24c-.18.18-.4.32-.64.4l-2.07-5.63zm-1.53.99c-.44.43-.77.95-.99 1.53l5.63 2.07c-.08.24-.22.46-.4.64l-4.24-4.24z" fill="#fff" mask="url(#star-four-with-outline_svg__a)"></path></svg></div></button></div></div></div><div class="l"><div class="ev ew ex ey ez l"><div class="ab ca"><div class="ch bg fa fb fc fd"></div></div><article class="meteredContent"><div class="l"><div class="l"><span class="l"></span><section><div><div class="ep fj fk fl fm fn"></div><div><div class="speechify-ignore l"><div class="fo fp fq fr fs l"></div><div class="ab ca"><div class="ch bg fa fb fc fd"><div class="ck l"><div class="fv ab"><div class="bl" aria-hidden="false"><button class="l ax ao am" aria-label="Member-only story"><div class="h k j i d"><div><div class="bl" aria-hidden="false"><svg width="16" height="16" viewBox="0 0 64 64" fill="none"><path d="M39.64 40.83L33.87 56.7a1.99 1.99 0 0 1-3.74 0l-5.77-15.87a2.02 2.02 0 0 0-1.2-1.2L7.3 33.88a1.99 1.99 0 0 1 0-3.74l15.87-5.77a2.02 2.02 0 0 0 1.2-1.2L30.12 7.3a1.99 1.99 0 0 1 3.74 0l5.77 15.87a2.02 2.02 0 0 0 1.2 1.2l15.86 5.76a1.99 1.99 0 0 1 0 3.74l-15.87 5.77a2.02 2.02 0 0 0-1.2 1.2z" fill="#FFC017"></path></svg></div></div></div><div class="s u w ec ed q"><svg width="16" height="16" viewBox="0 0 64 64" fill="none" class="ft fu"><path d="M39.64 40.83L33.87 56.7a1.99 1.99 0 0 1-3.74 0l-5.77-15.87a2.02 2.02 0 0 0-1.2-1.2L7.3 33.88a1.99 1.99 0 0 1 0-3.74l15.87-5.77a2.02 2.02 0 0 0 1.2-1.2L30.12 7.3a1.99 1.99 0 0 1 3.74 0l5.77 15.87a2.02 2.02 0 0 0 1.2 1.2l15.86 5.76a1.99 1.99 0 0 1 0 3.74l-15.87 5.77a2.02 2.02 0 0 0-1.2 1.2z" fill="#FFC017"></path></svg><p class="be b bf z dw">Member-only story</p></div></button></div></div></div></div></div></div></div><div class="fw fx fy fz ga"><div class="ab ca"><div class="ch bg fa fb fc fd"><div><h1 id="9b6e" class="pw-post-title gb gc gd be ge gf gg gh gi gj gk gl gm gn go gp gq gr gs gt gu gv bj" data-testid="storyTitle">AutoHyDE: Making HyDE Better for Advanced LLM RAG</h1></div><div><h2 id="08c4" class="pw-subtitle-paragraph gw gc gd be b gx gy gz ha hb hc hd he hf hg hh hi hj hk hl cp dw">🔎 A deep-dive into HyDE for Advanced LLM RAG + 💡 Introducing AutoHyDE, a semi-supervised framework to improve the effectiveness, coverage and applicability of HyDE</h2><div class="hm hn ho hp hq"><div class="speechify-ignore ab co"><div class="speechify-ignore bg l"><div class="hr hs ht hu hv ab"><div><div class="ab hw"><a href="https://ianhojy.medium.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><div><div class="bl" aria-hidden="false"><div class="l hx hy bx hz ia"><div class="l ef"><img alt="Ian Ho" class="l er bx dc dd cw" src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*x4qNNXqMnKmaw-9P" width="44" height="44" loading="lazy" data-testid="authorPhoto"/><div class="ib bx l dc dd ep n ic eq"></div></div></div></div></div></a><a href="https://towardsdatascience.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><div class="id ab ef"><div><div class="bl" aria-hidden="false"><div class="l ie if bx hz ig"><div class="l ef"><img alt="Towards Data Science" class="l er bx bq ih cw" src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"/><div class="ib bx l bq ih ep n ic eq"></div></div></div></div></div></div></a></div></div><div class="bm bg l"><div class="ab"><div style="flex:1"><span class="be b bf z bj"><div class="ii ab q"><div class="ab q ij"><div class="ab q"><div><div class="bl" aria-hidden="false"><p class="be b ik il bj"><a class="af ag ah ai aj ak al am an ao ap aq ar im" data-testid="authorName" href="https://ianhojy.medium.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow">Ian Ho</a></p></div></div></div><span class="in io" aria-hidden="true"><span class="be b bf z dw">·</span></span><p class="be b ik il dw"><button class="ip iq ah ai aj ak al am an ao ap aq ar ir is it" disabled="">Follow</button></p></div></div></span></div></div><div class="l iu"><span class="be b bf z dw"><div class="ab cm iv iw ix"><div class="iy iz ab"><div class="be b bf z dw ab ja"><span class="ft l iu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar im ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b bf z jb jc jd je jf jg jh ji bj">Towards Data Science</p></a></div></div></div><div class="h k"><span class="in io" aria-hidden="true"><span class="be b bf z dw">·</span></span></div></div><span class="be b bf z dw"><div class="ab ae"><span data-testid="storyReadTime">19 min read</span><div class="jj jk l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="be b bf z dw">·</span></span></div><span data-testid="storyPublishDate">Apr 4, 2024</span></div></span></div></span></div></div></div><div class="ab co jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka"><div class="h k w ec ed q"><div class="kq l"><div class="ab q kr ks"><div class="pw-multi-vote-icon ef ft kt ku kv"><div class=""><div class="kw kx ky kz la lb lc am ld le lf kv"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l lg lh li lj lk ll lm"><p class="be b dx z dw"><span class="kx">--</span></p></div></div></div><div><div class="bl" aria-hidden="false"><button class="ao kw lo lp ab q eg lq lr" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" class="gu"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count ln gu">6</span></p></button></div></div></div><div class="ab q kb kc kd ke kf kg kh ki kj kk kl km kn ko kp"><div class="eu k j i d"></div><div class="h k"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af eg ah ai aj ak al ls an ao ap ir lt lu lv" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="lw"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div></div><div class="er lx cm"><div class="l ae"><div class="ab ca"><div class="ly lz ma mb mc md ch bg"><div class="ab"><div class="bl bg" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af eg ah ai aj ak al ls an ao ap ir me mf lr mg mh mi mj mk s ml mm mn mo mp mq mr u ms mt mu"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0zm9-10a10 10 0 1 0 0 20 10 10 0 0 0 0-20zm3.38 10.42l-4.6 3.06a.5.5 0 0 1-.78-.41V8.93c0-.4.45-.63.78-.41l4.6 3.06c.3.2.3.64 0 .84z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af eg ah ai aj ak al ls an ao ap ir me mf lr mg mh mi mj mk s ml mm mn mo mp mq mr u ms mt mu"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">Share</p></div></button></div></div></div><div class="bl" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af eg ah ai aj ak al ls an ao ap ir me mf lr mg mh mi mj mk s ml mm mn mo mp mq mr u ms mt mu"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ef ng bg nh"><div class="mv mw mx"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BoEb2eI8BFOectBr5y0zjA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*BoEb2eI8BFOectBr5y0zjA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*BoEb2eI8BFOectBr5y0zjA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*BoEb2eI8BFOectBr5y0zjA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*BoEb2eI8BFOectBr5y0zjA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BoEb2eI8BFOectBr5y0zjA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BoEb2eI8BFOectBr5y0zjA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*BoEb2eI8BFOectBr5y0zjA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*BoEb2eI8BFOectBr5y0zjA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*BoEb2eI8BFOectBr5y0zjA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*BoEb2eI8BFOectBr5y0zjA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*BoEb2eI8BFOectBr5y0zjA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*BoEb2eI8BFOectBr5y0zjA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*BoEb2eI8BFOectBr5y0zjA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg md ni c" width="700" height="498" loading="eager" role="presentation"/></picture></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">Image by Author with the help of DALL-E</figcaption></figure><h1 id="8b4d" class="no np gd be nq nr ns gz nt nu nv hc nw nx ny nz oa ob oc od oe of og oh oi oj bj"><strong class="al">Introduction</strong></h1><p id="745d" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">In the field of Retrieval Augmented Generation (RAG),<strong class="om ge"> Hypothetical Document Embeddings </strong>(HyDE) have proven to be a powerful form of query rewriting to improve the relevance of retrieved documents.</p><p id="9ef5" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">For the uninitiated, whilst traditional retrieval simply uses the original input to create embedding vectors for retrieval, HyDE is a methodology to generate embedding vectors that are more relevant to the embedding space of the indexed documents to be retrieved.</p><p id="09d2" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The super high-level summary is: (1) Create hypothetical documents from user input, (2) Convert hypothetical documents to embeddings, (3) Use embeddings to retrieve similar documents</p><p id="fea6" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">I’ve been using RAG and basic HyDE in some of my work and personal projects, and after some time, I’ve realized that the existing implementation of HyDE does not always work well out of the box and it is not as flexible as I hoped it would be. So after doing my research on the methodology and digging through the papers and source codes, I wanted to share my thoughts on the current approach, and to suggest an <strong class="om ge">enhanced version of HyDE (I call it AutoHyDE) that has the potential to be more effective and adaptable across a variety of use-cases.</strong></p><p id="e2d0" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">In this article, <strong class="om ge">Section 1</strong> will be a deep dive into the original HyDE paper and we will go through how it has been translated into the LangChain implementation. If you are already familiar with HyDE, feel free to skip ahead.</p><p id="76c1" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">In <strong class="om ge">Section 2</strong>, I will discuss what I believe are the major limitations of the current HyDE approach.</p><p id="8d44" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Finally, I will introduce <strong class="om ge">AutoHyDE in Section 3</strong>, an attempt by me to create what I believe can be a better version of HyDE. Here is the tldr:</p><blockquote class="pl pm pn"><p id="6ba5" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">AutoHyDE is a framework to automatically discover underlying relevance patterns in your indexed documents and to generate hypothetical documents that directly represent these relevance patterns.</strong></p></blockquote><p id="a06c" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">I have also directly tweaked the LangChain <code class="cw pp pq pr ps b">HypotheticalDocumentEmbedder</code> class to realize AutoHyDE, so that it can still be chained with other parts of LangChain.</p><p id="295a" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Check out the <a class="af pt" href="https://github.com/ianhojy/auto-hyde/tree/main" rel="noopener ugc nofollow" target="_blank">repo</a> for more details, <a class="af pt" href="https://github.com/ianhojy/auto-hyde/blob/main/auto-hyde-demo.ipynb" rel="noopener ugc nofollow" target="_blank">demo</a> &amp; <a class="af pt" href="https://github.com/ianhojy/auto-hyde/blob/main/src/auto_hyde.py" rel="noopener ugc nofollow" target="_blank">source code</a>.</p><blockquote class="pl pm pn"><p id="9335" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Give a 👍🏻, leave a 💬 and 🤝🏻 with me on <a class="af pt" href="https://www.linkedin.com/in/ianhojy/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>!</p></blockquote><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ef ng bg nh"><div class="mv mw pu"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg md ni c" width="700" height="498" loading="lazy" role="presentation"/></picture></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">Image by Author: Code Snippet for AutoHyDE</figcaption></figure><p id="2fc9" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><em class="po">For a higher-level overview of RAG and Query Rewriting, this is a good </em><a class="af pt" href="https://medium.com/@florian_algo/advanced-rag-06-exploring-query-rewriting-23997297f2d1" rel="noopener"><em class="po">article</em></a><em class="po"> written by Florian June. This </em><a class="af pt" href="https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="po">repo</em></a><em class="po"> also contains some good practical implementations of RAG, HyDE and other enhancements.</em></p></div></div></div><div class="ab ca pv pw px py" role="separator"><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb"></span></div><div class="fw fx fy fz ga"><div class="ab ca"><div class="ch bg fa fb fc fd"><h1 id="7cf8" class="no np gd be nq nr qd gz nt nu qe hc nw nx qf nz oa ob qg od oe of qh oh oi oj bj"><strong class="al">Section 1: What is HyDE?</strong></h1><h2 id="4ac8" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj">HyDE: The Original Paper</h2><p id="4b56" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">Hypothetical Document Embeddings (HyDE) were first introduced in the 2022 paper by Gao et al. titled <a class="af pt" href="https://arxiv.org/pdf/2212.10496.pdf" rel="noopener ugc nofollow" target="_blank">Precise Zero-Shot Dense Retrieval without Relevance Labels</a>.</p><p id="cda4" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The paper set out to find a way to improve zero-shot dense retrieval (i.e. using semantic embedding similarities). To that end, they came up with a two-step methodology called HyDE.</p><p id="e645" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">Step 1 </strong>involved instruction prompting a language model (in the paper they use GPT-3) to generate a hypothetical document given the original query (specifically limited to a question in the paper).</p><p id="3524" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">Step 2</strong> involved using a <em class="po">Contriever</em>, or an <em class="po">“unsupervised contrastive encoder”,</em> to turn this hypothetical document into an embedding vector, which is then used for downstream similarity search and retrieval.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ef ng bg nh"><div class="mv mw qz"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*UFRWRzsuBxwRypjA_y9tJQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*UFRWRzsuBxwRypjA_y9tJQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*UFRWRzsuBxwRypjA_y9tJQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*UFRWRzsuBxwRypjA_y9tJQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*UFRWRzsuBxwRypjA_y9tJQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*UFRWRzsuBxwRypjA_y9tJQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UFRWRzsuBxwRypjA_y9tJQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*UFRWRzsuBxwRypjA_y9tJQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*UFRWRzsuBxwRypjA_y9tJQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*UFRWRzsuBxwRypjA_y9tJQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*UFRWRzsuBxwRypjA_y9tJQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*UFRWRzsuBxwRypjA_y9tJQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*UFRWRzsuBxwRypjA_y9tJQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*UFRWRzsuBxwRypjA_y9tJQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg md ni c" width="700" height="199" loading="lazy" role="presentation"/></picture></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">Illustration of HyDE methodology from the <a class="af pt" href="https://arxiv.org/pdf/2212.10496.pdf" rel="noopener ugc nofollow" target="_blank">original paper</a></figcaption></figure><h2 id="5521" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj"><strong class="al">[<em class="ra">Optional</em>] An aside on the <em class="ra">Contriever</em></strong></h2><p id="c72c" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">The <em class="po">Contriever</em> used in the original HyDE paper was derived from an earlier paper in August 2022 by Izacard et al. called <a class="af pt" href="https://arxiv.org/pdf/2112.09118.pdf" rel="noopener ugc nofollow" target="_blank">Unsupervised Dense Information Retrieval with Contrastive Learning</a>. In this paper, the authors claim that while neural networks had emerged as good alternatives to term-frequency methods for retrieval, they required large amounts of data and did not always transfer well to new areas of application. Therefore, they devised a way to train the embedding network in an unsupervised manner through<strong class="om ge"> contrastive learning</strong>.</p><p id="363f" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">For those interested in Contrastive Learning, you can check out Section 3 of the paper for information, but here’s a high-level summary:</p><p id="b36a" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">First, they define a loss function (contrastive InfoNCE loss) that rewards positive pairs of documents and penalizes negative pairs. Next, they build positive and negative pairs of document representations. Positive pairs are constructed using a data augmentation method called the Inverse Cloze Task (ICT), and through Independent Cropping. Negative pairs are constructed using in-batch negative sampling, and across-batch negative sampling (also called MoCo). Finally, they train this over the BERT base uncased architecture.</p><p id="f412" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The results of the paper showed that this method was able to match unsupervised term-frequency methods like BM25, and also performed well on cross-lingual retrieval, an option which is not possible for term matching methods.</p><p id="ffb0" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">For the purposes of this article, I will not be going further into the details of training and data augmentation, but do check out the paper if you are interested. The contriever can also be found on HuggingFace at this <a class="af pt" href="https://huggingface.co/facebook/contriever" rel="noopener ugc nofollow" target="_blank">link</a>, and the research repo can be found <a class="af pt" href="https://github.com/facebookresearch/contriever" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="388f" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Anyway, It is this <em class="po">Contriever</em> that the HyDE paper uses to embed generated documents. They also explore <em class="po">ContrieverFT</em>, which is in-domain fine-tuned in a supervised manner. Now, back to the main focus of the article, which is the HyDE Paper.</p><h2 id="96f3" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj"><strong class="al">Back to the HyDE Paper</strong></h2><p id="b4ef" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">Previously, I provided an overview of the HyDE methodology and the origins of the underlying <em class="po">contriever. </em>Now, let’s take a closer look at what’s going on in HyDE by reviewing some key excerpts from the paper.</p><blockquote class="pl pm pn"><p id="3a4f" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">…In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder’s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings.</p></blockquote><p id="c9d7" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">More formally, the hypothetical document embedding vector is defined as:</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ef ng bg nh"><div class="mv mw rb"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*UBvSsoJUagB0lh7r4j661A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*UBvSsoJUagB0lh7r4j661A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*UBvSsoJUagB0lh7r4j661A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*UBvSsoJUagB0lh7r4j661A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*UBvSsoJUagB0lh7r4j661A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*UBvSsoJUagB0lh7r4j661A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBvSsoJUagB0lh7r4j661A.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*UBvSsoJUagB0lh7r4j661A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*UBvSsoJUagB0lh7r4j661A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*UBvSsoJUagB0lh7r4j661A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*UBvSsoJUagB0lh7r4j661A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*UBvSsoJUagB0lh7r4j661A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*UBvSsoJUagB0lh7r4j661A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*UBvSsoJUagB0lh7r4j661A.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg md ni c" width="700" height="88" loading="lazy" role="presentation"/></picture></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">Formula for encoding the generated document in the <a class="af pt" href="https://arxiv.org/pdf/2212.10496.pdf" rel="noopener ugc nofollow" target="_blank">HyDE Paper</a></figcaption></figure><ul class=""><li id="d2b4" class="ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf rc rd re bj">where g is the <em class="po">InstructLM(query, INST)</em> function in the first step of hypothetical document generation; and</li><li id="d525" class="ok ol gd om b gx rf oo op ha rg or os ot rh ov ow ox ri oz pa pb rj pd pe pf rc rd re bj">where f is the document encoder (<em class="po">contriever</em>)</li></ul><p id="0eb9" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">“Formally, g defines a probability distribution based on the chain rule”, and V is estimated through the following equation:</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ef ng bg nh"><div class="mv mw rk"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*y9lHVX24zORuojIz8GMiMA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*y9lHVX24zORuojIz8GMiMA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*y9lHVX24zORuojIz8GMiMA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*y9lHVX24zORuojIz8GMiMA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*y9lHVX24zORuojIz8GMiMA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*y9lHVX24zORuojIz8GMiMA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9lHVX24zORuojIz8GMiMA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*y9lHVX24zORuojIz8GMiMA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*y9lHVX24zORuojIz8GMiMA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*y9lHVX24zORuojIz8GMiMA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*y9lHVX24zORuojIz8GMiMA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*y9lHVX24zORuojIz8GMiMA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*y9lHVX24zORuojIz8GMiMA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*y9lHVX24zORuojIz8GMiMA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg md ni c" width="700" height="362" loading="lazy" role="presentation"/></picture></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">Formula for estimating the embedding vector in the <a class="af pt" href="https://arxiv.org/pdf/2212.10496.pdf" rel="noopener ugc nofollow" target="_blank">HyDE Paper</a></figcaption></figure><p id="a4a5" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">In the original paper, you can also find the python implementation at this <a class="af pt" href="https://github.com/texttron/hyde/blob/74101c5157e04f7b57559e7da8ef4a4e5b6da82b/src/hyde/hyde.py" rel="noopener ugc nofollow" target="_blank">repo</a>, where:</p><p id="f227" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><em class="po">f is the generative function as we see in the repo:</em></p><figure class="my mz na nb nc nd"><div class="rl jb l ef"><div class="rm rn l"></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">generate function from <a class="af pt" href="https://github.com/texttron/hyde/blob/74101c5157e04f7b57559e7da8ef4a4e5b6da82b/src/hyde/hyde.py" rel="noopener ugc nofollow" target="_blank">texttron/hyde</a></figcaption></figure><p id="a2a6" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><em class="po">g is the encoding function as we see in the repo:</em></p><figure class="my mz na nb nc nd"><div class="rl jb l ef"><div class="rm rn l"></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">encode function from <a class="af pt" href="https://github.com/texttron/hyde/blob/74101c5157e04f7b57559e7da8ef4a4e5b6da82b/src/hyde/hyde.py" rel="noopener ugc nofollow" target="_blank">texttron/hyde</a></figcaption></figure><p id="2139" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Just as in the estimation equation, we see that a simple averaging is used to estimate the embedding vector across the various hypothetical documents. This mean-aggregated embedding is then used to do similarity search:</p><figure class="my mz na nb nc nd"><div class="rl jb l ef"><div class="rm rn l"></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">search function from <a class="af pt" href="https://github.com/texttron/hyde/blob/74101c5157e04f7b57559e7da8ef4a4e5b6da82b/src/hyde/hyde.py" rel="noopener ugc nofollow" target="_blank">texttron/hyde</a></figcaption></figure><p id="20e0" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Overall, we see that HyDE is really not a complicated implementation. <strong class="om ge">Generate, embed, average, retrieve</strong>.</p><p id="b3bf" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Now, let’s take a look at popular implementations of HyDE that have surfaced recently.</p><h2 id="252e" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj"><strong class="al">LangChain Implementation</strong></h2><p id="b66b" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">In both LangChain &amp; LlamaIndex, there are implementations of HyDE that are similar to what we’ve seen in the paper and repo above. For most of us who are playing around with LLM capabilities, these are the most likely ways that we’ll be looking to use HyDE. We’ll also have a closer look at the prompts being used under the hood for these HyDE classes. Let’s focus on the LangChain implementation. The repo for reference is <a class="af pt" href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/hyde/base.py" rel="noopener ugc nofollow" target="_blank">here</a>.</p><figure class="my mz na nb nc nd"><div class="rl jb l ef"><div class="rm rn l"></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">from_llm constructor for HypotheticalDocumentEmbedder in <a class="af pt" href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/hyde/base.py" rel="noopener ugc nofollow" target="_blank">langchain/hyde</a></figcaption></figure><p id="5087" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The first step is to create the <code class="cw pp pq pr ps b">HypotheticalDocumentEmbedder</code> using <code class="cw pp pq pr ps b">from_llm</code> — you can choose one of the existing prompt templates such as the one you see below for writing a news passage, or define your own custom prompt.</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="09de" class="rr np gd ps b bf rs rt l ru rv">trec_news_template = &quot;&quot;&quot;Please write a news passage about the topic.<br/>Topic: {TOPIC}<br/>Passage:&quot;&quot;&quot;</span></pre><p id="1185" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Then, you can execute <code class="cw pp pq pr ps b">embed_query</code> , which first generates the hypothetical document(s) based on the prompt template, and then translates these documents into embeddings and combines them. The <code class="cw pp pq pr ps b">embed_documents</code> function simply uses the defined <code class="cw pp pq pr ps b">base_embeddings</code> from the <code class="cw pp pq pr ps b">from_llm</code> definition, while the <code class="cw pp pq pr ps b">combine_embeddings</code> is a mean aggregation.</p><figure class="my mz na nb nc nd"><div class="rl jb l ef"><div class="rm rn l"></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">embed_query function for <a class="af pt" href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/hyde/base.py" rel="noopener ugc nofollow" target="_blank">langchain/hyde</a></figcaption></figure><p id="cfe7" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">You will also notice that documents is of <code class="cw pp pq pr ps b">List[str]</code> type because you can generate multiple documents. All you have to do is define the LLM accordingly in the <code class="cw pp pq pr ps b">from_llm</code> step. Here’s an example:</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="3464" class="rr np gd ps b bf rs rt l ru rv">multi_llm = OpenAI(n=4, best_of=4)<br/>embeddings = HypotheticalDocumentEmbedder.from_llm(<br/>    multi_llm, base_embeddings, &quot;web_search&quot;<br/>)<br/>result = embeddings.embed_query(&quot;Where is the Taj Mahal?&quot;)</span></pre><p id="f364" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">For a simple walkthrough of how to use HyDE, you can refer to the <a class="af pt" href="https://github.com/ianhojy/auto-hyde/blob/main/langchain-hyde-demo.ipynb" rel="noopener ugc nofollow" target="_blank">notebook</a>. It’s nothing too complicated and LangChain has made it really easy to use by abstracting away most of the underlying functionality.</p></div></div></div><div class="ab ca pv pw px py" role="separator"><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb"></span></div><div class="fw fx fy fz ga"><div class="ab ca"><div class="ch bg fa fb fc fd"><h1 id="2fe3" class="no np gd be nq nr qd gz nt nu qe hc nw nx qf nz oa ob qg od oe of qh oh oi oj bj"><strong class="al">Section 2: Limitations of HyDE</strong></h1><p id="381b" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">Now that we have a good idea of how HyDE was originally designed and how it was eventually implemented in the popular LangChain &amp; LlamaIndex libraries, I want to talk about their limitations.</p><p id="c51b" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The first thing I noticed is that whilst the HyDE paper does experiment with different instruction LLMs, it does not spend too much time talking about further optimizing this aspect of the HyDE process. From table 4 in the paper, we see that using different models results in rather huge result variance:</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ef ng bg nh"><div class="mv mw rw"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*u9xHr0QqHbCMaPw-C-hvkQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*u9xHr0QqHbCMaPw-C-hvkQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*u9xHr0QqHbCMaPw-C-hvkQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*u9xHr0QqHbCMaPw-C-hvkQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*u9xHr0QqHbCMaPw-C-hvkQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*u9xHr0QqHbCMaPw-C-hvkQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u9xHr0QqHbCMaPw-C-hvkQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*u9xHr0QqHbCMaPw-C-hvkQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*u9xHr0QqHbCMaPw-C-hvkQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*u9xHr0QqHbCMaPw-C-hvkQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*u9xHr0QqHbCMaPw-C-hvkQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*u9xHr0QqHbCMaPw-C-hvkQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*u9xHr0QqHbCMaPw-C-hvkQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*u9xHr0QqHbCMaPw-C-hvkQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg md ni c" width="700" height="582" loading="eager" role="presentation"/></picture></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">HyDE results from using different LLMs in the <a class="af pt" href="https://arxiv.org/pdf/2212.10496.pdf" rel="noopener ugc nofollow" target="_blank">HyDE Paper</a></figcaption></figure><p id="92bc" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The key implication here is not to use the biggest models with the most parameters. Instead, the fact that the underlying LLMs bring about such huge differences for the overall tasks of retrieval relevance convinces me that apart from using different LLMs, <strong class="om ge">the generative task is one area worth further optimizing</strong>.</p><p id="b957" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">In fact, the researchers themselves do state that:</p><blockquote class="pl pm pn"><p id="2730" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">InstructGPT model able to further bring up the performance… this suggests that there may still exist certain factors not captured by the fine-tuned encoder but only by the generative model.</p></blockquote><p id="7f84" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">With that possibility in mind, we have also seen from the LangChain deep-dive in Section 1 that the prompt templates are pre-defined to be generic in instructing hypothetical documents. For example, <em class="po">“Please write a scientific paper passage to support/refute the claim”</em>. This only works when your query/input nicely fits these pre-defined templates. Otherwise, you can also provide a custom template to generate hypothetical documents, but this still has its limitations.</p><p id="8f71" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Firstly, the existing implementations are largely limited to a Q&amp;A framework. In the real world, not all retrieval use-cases are for answering questions, and thus asking the LLM to hypothetically answer your question does not make make sense. So in all these other situations, what do we ask the LLM to generate hypothetically then? Yes, custom prompting is available, but there is no guarantee that the prompt you write is actually relevant to the existing documents. Unless the underlying documents are very uniform in terms of relevance patterns, writing a prompt to capture these patterns would be a very manual and imperfect process.</p><p id="af16" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">More importantly, when you chunk and index your documents, the <strong class="om ge">chunks are not always homogenous</strong> in terms of style, tone, structure etc. Consequently, it may be <strong class="om ge">insufficient to use a single generalized prompt</strong> to generate hypothetical documents given your input, and it may be intractable to write multiple prompts to cater to different kinds of chunks. For instance, imagine you are trying to generate hypothetical documents for blog posts. With different authors, tones, writing styles, how do you create a generalized prompt to capture this variety? Thus, the current implementation of HyDE appears to be rather rigid and suboptimal in terms of coverage.</p><p id="8bfa" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">On a more technical level, we also note that regarding the final vector embedding, the paper says that:</p><blockquote class="pl pm pn"><p id="d779" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">We simply consider the expectation value, assuming the distribution of v_qij is uni-modal, i.e. the query is not ambiguous.</p></blockquote><p id="c205" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">But if you think about it, most humans approach search problems with ambiguity.</p><p id="9134" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">This is an important caveat. In real use-cases, I can imagine that the distribution of v is not in fact nicely uni-modal, especially when the query is as ambiguous as just one or two keywords. What this means is that the same underlying query can be mapped to more than one mode of vector embedding representations.</p><p id="56aa" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">How might we begin to improve HyDE?</strong></p><blockquote class="pl pm pn"><p id="8504" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">HyDE appears unsupervised. No model is trained in HyDE: both the generative model and the contrastive encoder remain intact.</p></blockquote><p id="4894" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The unsupervised nature of HyDE is important because it does away with the need to have huge datasets or generous amounts of compute for any supervised variant of methodology to improve dense retrieval.</p><p id="3ed3" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">However, by remaining strictly unsupervised, we lose some benefits. Specifically, there is no way to dynamically define multiple generative functions <em class="po">g(q, INST)</em> that are more aligned with the document embeddings.</p><p id="6bd8" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Instead, it might be worth considering <strong class="om ge">a semi-supervised approach</strong> that is more adaptive and generalizable. This approach does not require one to forcefully fit hypothetical generation of documents into a single pre-defined/custom prompt. It does not even require one to know what custom prompts to write, unlike the LangChain/LlamaIndex implementations.</p><p id="29ba" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Instead, an enhanced version of HyDE will automatically <strong class="om ge">learn a greater variety of relevance patterns</strong> above and beyond a baseline retrieval, and generate hypothetical documents that are aligned with different clusters of indexed chunks. I call this approach <strong class="om ge">AutoHyDE</strong>.</p><p id="5a22" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Before talking more about AutoHyDE, I have one final point to bring up regarding with the current HyDE implementation. I will not be addressing it in AutoHyDE, but I think it deserves some attention.</p><p id="97cb" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Recall again the averaging equation across embeddings, which manifests as the following line of code:</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="4206" class="rr np gd ps b bf rs rt l ru rv">list(np.array(embeddings).mean(axis=0))</span></pre><p id="20f0" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">It’s a simple aggregation but does not make much sense to me, especially when you consider the multi-modal distribution of the embeddings. Let’s say you manage to construct the chained encoding function <em class="po">f(g(q, INST))</em> that nicely captures the diversity of the relevance patterns across all your document chunks. When you take averages across these multi-modal distributed encodings, you end up with an average that may not be relevant at all (think Jensen’s inequality kind of averaging). Of course, this all works well when you have a uni-modal distribution, but this is rarely a realistic assumption, and there is no way to verify it universally.</p><p id="826e" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Anyway, on to <strong class="om ge">AutoHyDE</strong>.</p></div></div></div><div class="ab ca pv pw px py" role="separator"><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb"></span></div><div class="fw fx fy fz ga"><div class="ab ca"><div class="ch bg fa fb fc fd"><h1 id="8099" class="no np gd be nq nr qd gz nt nu qe hc nw nx qf nz oa ob qg od oe of qh oh oi oj bj"><strong class="al">Section 3: AutoHyDE</strong></h1><h2 id="9e95" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj">Overview</h2><p id="4d61" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">To recap, the main point of AutoHyDE is to <strong class="om ge">automatically discover the variety of relevance patterns</strong> in your vector database and <strong class="om ge">generate a variety of documents </strong>to improve coverage of these patterns.</p><p id="8dd7" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Technically, I took the existing implementation of<strong class="om ge"> class HypotheticalDocumentEmbedder </strong>in LangChain and created new functions to achieve AutoHyDE, so that it works immediately as part of any RAG chain that you might have. This is the updated <code class="cw pp pq pr ps b">embed_query</code> function that I wrote, and will explore each of the sub-methods in the following walkthrough.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ef ng bg nh"><div class="mv mw pu"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*oenoqdYqCyvlHYGQ9GEn1A.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg md ni c" width="700" height="498" loading="lazy" role="presentation"/></picture></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">Image by Author: Src Code Snippet from AutoHyDE Repo</figcaption></figure><p id="eec7" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">🧑🏻‍💻 Check out the implementation here if you are interested to try it out:<strong class="om ge"> </strong><a class="af pt" href="https://github.com/ianhojy/auto-hyde/tree/main" rel="noopener ugc nofollow" target="_blank"><strong class="om ge">https://github.com/ianhojy/auto-hyde/tree/main</strong></a></p><p id="b457" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Here’s a quick demo of how to use it:</p><figure class="my mz na nb nc nd"><div class="rl jb l ef"><div class="rm rn l"></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">Demo code for AutoHyDE</figcaption></figure><p id="c16d" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">To illustrate how AutoHyDE works and, I’m going to be using J. S. Mill’s <a class="af pt" href="https://sacred-texts.com/phi/mill/util.txt" rel="noopener ugc nofollow" target="_blank">Utilitarianism</a>. I’m going to chunk it, index it and try to receive the relevant chunks based on a query.</p><p id="7bc6" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">I chose this text not just because I spent countless hours struggling with it as an undergraduate back in the good old days of Philosophy 101, but also because it’s a good example of a text where it’s really difficult to write a custom prompt to generate hypothetical documents, even if you hold a PhD in Moral Philosophy with endless knowledge of Mill. Let’s just say that Mill’s writing is not the most straightforward and consistent, he’s really an expert at run-on sentences (so am I).</p><p id="525f" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">As I walk through my implementation of AutoHyDE below, I’ll have both high-level explanations and more technical descriptions for those interested in the <a class="af pt" href="https://github.com/ianhojy/auto-hyde/tree/main" rel="noopener ugc nofollow" target="_blank">repo</a>.</p><h2 id="5aaf" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj"><strong class="al">Step 1: Extract Key Words from Query</strong></h2><p id="720c" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">Imagine you received this essay assignment:</p><blockquote class="pl pm pn"><p id="16fa" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">What is the relationship between justice and happiness?</p></blockquote><p id="c8ef" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">At this step 1, the LLM will be prompted to extract the key words, in this case:</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="a352" class="rr np gd ps b bf rs rt l ru rv">&gt;&gt;&gt; Extracting Keywords from your Query…<br/>&gt;&gt;&gt; …Keywords Extracted: [&#x27;relationship&#x27;, &#x27;justice&#x27;, &#x27;happiness&#x27;]</span></pre><p id="c761" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The key words are important in this case because they will be utilized in Step 3, in order to quickly (and dirtily) identify documents that you would have left out in a normal retrieval.</p><p id="2511" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">Technical Implementation</strong></p><p id="e9b2" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">This is the function I wrote to extract the keywords:</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="a8ec" class="rr np gd ps b bf rs rt l ru rv">@retry(tries=5)<br/>def extract_keywords(<br/>    self, <br/>    text: str, <br/>    hypo_params: dict<br/>    ) -&gt; List[str]:<br/><br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(f&quot;\n&gt;&gt;&gt; Extracting Keywords from your Query...&quot;)<br/>    <br/>    KEYWORD_EXTRACTION_PROMPT = &quot;&quot;&quot;<br/>    Your goal is to extract a list of keywords from an input phrase, sentence, or several sentences.<br/><br/>    - You can only generate 1 to 5 keywords.<br/>    - Keywords should be nouns, issues, concepts<br/>    - Keywords should not include verbs, prepositions, pronouns<br/>    - Each keyword can only be one word long.<br/>    - If the input is just a single word, return that word as the only keyword.<br/><br/>    {format_instructions}<br/><br/>    The input is:<br/>    {input}<br/>    &quot;&quot;&quot;<br/><br/>    class KeywordListSchema(BaseModel):<br/>        keywordList: list[str] = Field(description=&quot;list of one-word keywords based on a given phrase&quot;)<br/><br/>    parser = JsonOutputParser(pydantic_object=KeywordListSchema)<br/><br/>    prompt = ChatPromptTemplate.from_template(<br/>        template=KEYWORD_EXTRACTION_PROMPT,<br/>        intput_variables = [&quot;input&quot;],<br/>        partial_variables = {<br/>            &#x27;format_instructions&#x27;: parser.get_format_instructions()<br/>        }<br/>    )<br/><br/>    keyword_extraction_chain = (<br/>        {&#x27;input&#x27;: RunnablePassthrough()}<br/>        | prompt<br/>        | self.llm_chain<br/>        | parser<br/>    )<br/>    <br/>    keywords = keyword_extraction_chain.invoke(text)[&#x27;keywordList&#x27;]<br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(f&quot;&gt;&gt;&gt; ...Keywords Extracted: {keywords}\n&quot;)<br/>    <br/>    return keywords</span></pre><p id="eaa0" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Nothing fancy here, just using the JsonOutputParser to ensure that I get the exact output which is a list of keywords.</p><h2 id="f548" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj"><strong class="al">Step 2: Do initial retrieval</strong></h2><p id="6206" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">Apart from extracting the keywords, we’ll also use the original query to do an initial retrieval. You’ll have to ask yourself, how many chunks do you want to retrieve as part of your workflow?</p><p id="44d9" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Assuming that you initially intended to retrieve 20 documents to use as context for answering your question in RAG, what this step 2 will then do is to retrieve more than 20 documents so that we can explore potential documents that you would have <em class="po">neglected</em> (for lack of a better word) due to your cut off of 20. This can be toggled using <code class="cw pp pq pr ps b">exploration_multiplier</code>. With a <code class="cw pp pq pr ps b"><em class="po">baseline_k</em></code> of 20 and <code class="cw pp pq pr ps b">exploration_multiplier</code> of 5, we will be exploring 20 x 5 = 100 top documents based on cosine similarity.</p><p id="4759" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">Technical Implementation</strong></p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="6ec0" class="rr np gd ps b bf rs rt l ru rv">def do_init_retrieval(<br/>    self,<br/>    db: VectorStore, <br/>    text: str, <br/>    hypo_params: dict<br/>    ) -&gt; List[Tuple[Document, float]]:<br/>    <br/>    k = hypo_params[&#x27;baseline_k&#x27;] * hypo_params[&#x27;exploration_multiplier&#x27;]<br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(f&quot;\n&gt;&gt;&gt; Performing Initial Retrieval of {k} documents...\n&quot;)<br/>    docs = db.similarity_search_with_score(<br/>        text, <br/>        k=k<br/>    )<br/>    return docs</span></pre><h2 id="3b37" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj"><strong class="al">Step 3: Get neglected documents that contain the keywords</strong></h2><p id="01a2" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">So above and beyond the 20 documents that you would have retrieved from a straightforward retrieval, there are 80 documents that are now up for consideration that you would have <em class="po">neglected</em>.</p><p id="2463" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Out of these 80 documents, we want to find the subset of documents that would have been directly relevant to your original query. How do we do this efficiently? One simple way would just be to select the documents that contain any of the keywords extracted from step 1. There are ways to make this better, but I think keyword matching is already a quick and rather effective way to identify the first cut of neglected documents. Maybe we’ll explore better ways to do this identification, but this will do for now.</p><p id="35a6" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">This is what you will see as output from the code:</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="bede" class="rr np gd ps b bf rs rt l ru rv">&gt;&gt;&gt; Checking 80 Docs ranked after 20 for presence of keyword…<br/>&gt;&gt;&gt; …69 neglected Docs identified</span></pre><p id="3601" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">In the output above, we see that out of the 80 Documents, 69 contained one or more of the keywords [‘relationship’, ‘justice’, ‘happiness’].</p><p id="6f84" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Intuitively, we can understand these as potentially relevant documents that would have been neglected in a normal retrieval of the top 20 documents.</p><p id="600b" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">Technical Implementation</strong></p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="1f87" class="rr np gd ps b bf rs rt l ru rv">def get_remaining_docs_with_keywords(<br/>    self, <br/>    text: str, <br/>    init_docs: List[Tuple[Document, float]], <br/>    keywords: List[str], <br/>    hypo_params: dict<br/>    ) -&gt; List[Document]:<br/><br/>    remaining_docs_with_keywords = list()<br/>    <br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(f&quot;&quot;&quot;\n&gt;&gt;&gt; Checking {len(init_docs[hypo_params[&#x27;baseline_k&#x27;]:])} <br/>                Docs ranked after {hypo_params[&#x27;baseline_k&#x27;]} for presence of keyword...&quot;&quot;&quot;)<br/><br/>    for r in init_docs[hypo_params[&#x27;baseline_k&#x27;]:]:<br/>        page_content = r[0].page_content.lower()<br/>        for keyword in keywords:<br/>            if keyword.lower() in page_content:<br/>                remaining_docs_with_keywords.append(r)<br/>                continue<br/>                <br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(f&quot;&gt;&gt;&gt; ...{len(remaining_docs_with_keywords)} neglected Docs identified\n&quot;)<br/>    return remaining_docs_with_keywords</span></pre><p id="f8b9" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">For each document, if any of the keywords are found in it, I add it to my list of neglected docs (<code class="cw pp pq pr ps b">remaining_docs_with_keywords</code>).</p><h2 id="f9ac" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj"><strong class="al">Step 4: Cluster Neglected Documents</strong></h2><p id="1b26" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">This is an important step. We might be tempted to ask the LLM to reference each of these 69 neglected documents and write a hypothetical document for each of them based on the original user query.</p><p id="6229" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">However, this is computationally expensive and not the most efficient way to discover the main relevance patterns that exist across these documents.</p><p id="af09" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Instead, using the embeddings, we will cluster the 69 documents. This allows us to discover the key relevant patterns that exist across the datasets. In the output below, we see that the 69 documents were subsequently clustered into 6 different groups.</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="6014" class="rr np gd ps b bf rs rt l ru rv">&gt;&gt;&gt; Clustering neglected Docs...<br/>&gt;&gt;&gt; ...6 Clusters identifiedTechnical Implementation</span></pre><p id="6be7" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">Technical Implementation</strong></p><p id="03b7" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">How do we choose the number of clusters? Here I use <a class="af pt" href="https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14" rel="noopener ugc nofollow" target="_blank">HDBSCAN</a>, which is a semi-supervised hierarchical clustering version of DBSCAN. This is how HDBSCAN is described in the docs:</p><blockquote class="pl pm pn"><p id="e813" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">The algorithm starts off much the same as DBSCAN: we transform the space according to density, exactly as DBSCAN does, and perform single linkage clustering on the transformed space. Instead of taking an epsilon value as a cut level for the dendrogram however, a different approach is taken: the dendrogram is condensed by viewing splits that result in a small number of points splitting off as points ‘falling out of a cluster’. This results in a smaller tree with fewer clusters that ‘lose points’. That tree can then be used to select the most stable or persistent clusters.</p></blockquote><p id="366f" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Intuitively, <code class="cw pp pq pr ps b">min_samples</code> is a parameter that controls the minimum number of neighbours to a core point. The greater <code class="cw pp pq pr ps b">min_samples</code> is, more points will be labelled as noise from the clustering. For now, I will keep it as 1 and leave the experimentation to later, but in general <em class="po">&quot;HDBSCAN is not that sensitive to it and we can choose some sensible defaults, but this remains the biggest weakness of the algorithm.&quot;</em> <code class="cw pp pq pr ps b">min_cluster_size</code> is used to identify points ‘falling out of a cluster’ or splitting to form two new clusters.</p><p id="cd40" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">After labelling the documents, we drop those labelled as -1 (no clusters), and then create a dictionary with keys being the labelled group number, and the values being the page contents for each document.</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="2a3a" class="rr np gd ps b bf rs rt l ru rv">def cluster_docs(<br/>    self, <br/>    remaining_docs_with_keywords: List[Document], <br/>    hypo_params: dict<br/>    ) -&gt; Dict[int, List[str]]:<br/>    <br/>    from hdbscan import HDBSCAN<br/>    <br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(f&quot;\n&gt;&gt;&gt; Clustering neglected Docs...&quot;)<br/>    <br/>    embeddings = self.embed_documents([<br/>        r[0].page_content <br/>        for r in remaining_docs_with_keywords], <br/>        {&#x27;verbose&#x27;: False})<br/>    hdb = HDBSCAN(min_samples=1, min_cluster_size=3).fit(embeddings)<br/>    remaining_docs_with_cat = filter(lambda x: x[1] != -1, zip([r[0].page_content for r in remaining_docs_with_keywords], hdb.labels_))<br/>    <br/>    cat_dict = {}<br/><br/>    for page_content, cat in remaining_docs_with_cat:<br/>        if cat not in cat_dict:<br/>            cat_dict[cat] = [page_content]<br/>        else:<br/>            cat_dict[cat].append(page_content)<br/>            <br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(f&quot;&gt;&gt;&gt; ...{len(cat_dict)} Clusters identified\n&quot;)<br/>            <br/>    return cat_dict</span></pre><h2 id="b1f8" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj"><strong class="al">Step 5: Generate Hypothetical Documents</strong></h2><p id="6ac3" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">Now, for each of our 6 clusters that represent groups of relevance patterns, we will ask our LLM to reference the documents belonging to the cluster to create a new hypothetical document that is similar to those documents. This is an important step that differentiates the AutoHyDE approach from existing HyDE approaches. In AutoHyDE, the relevance patterns are automatically learned and then used to generate the hypothetical documents. No customized prompt writing is required at all.</p><p id="2cea" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Here’s an example for the first cluster in the code below. You can see that the LLM referenced every neglected in the first neglected document cluster, and created a hypothetical document based on the original query. This is done for all the clusters. A quick look at the text below, and you can see that it really does emulate the convluted style of Mill quite well, and you did not have to write a custom prompt to try and capture this style. All it took was few-shot prompting under the hood.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ef ng bg nh"><div class="mv mw rx"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*wyhHIToAespRTk63DHagjA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*wyhHIToAespRTk63DHagjA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*wyhHIToAespRTk63DHagjA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*wyhHIToAespRTk63DHagjA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*wyhHIToAespRTk63DHagjA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*wyhHIToAespRTk63DHagjA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wyhHIToAespRTk63DHagjA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*wyhHIToAespRTk63DHagjA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*wyhHIToAespRTk63DHagjA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*wyhHIToAespRTk63DHagjA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*wyhHIToAespRTk63DHagjA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*wyhHIToAespRTk63DHagjA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*wyhHIToAespRTk63DHagjA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*wyhHIToAespRTk63DHagjA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg md ni c" width="700" height="573" loading="lazy" role="presentation"/></picture></div></div><figcaption class="nj nk nl mv mw nm nn be b bf z dw">Image by Author: Hypothetical Document generated from AutoHyDE</figcaption></figure><p id="d2cf" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">Technical Implementation</strong></p><p id="b85b" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">What the function below does is to generate hypothetical documents for each identified cluster of documents. Similar to before, we use the JsonOutputParser to ensure that we get the desired output. I found that a few-shot approach generally works as you see in <code class="cw pp pq pr ps b">HYPOTHETICAL_DOCUMENT_PROMPT</code> below:</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="364f" class="rr np gd ps b bf rs rt l ru rv">@retry(tries=5)<br/>def generate_hypo_docs(<br/>    self, <br/>    text: str, <br/>    cat_dict: Dict[int, List[str]], <br/>    hypo_params: dict<br/>    ) -&gt; List[str]:<br/>    <br/>    hypo_docs = list()<br/>    <br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(f&quot;\n&gt;&gt;&gt; Generating Hypothetical Documents for each Doc Cluster...\n&quot;)<br/><br/>    HYPOTHETICAL_DOCUMENT_PROMPT = &quot;&quot;&quot;<br/>    Your instruction is to generate a single hypothetical document from an input.<br/>    - This hypothetical document must be similar in style, tone and voice as examples you are provided with.<br/>    - This hypothetical document must appear like it was written by the same author as the examples you are provided with.<br/>    - This hypothetical document must also be similar in length with the examples you are provided with.<br/><br/>    {format_instructions}<br/><br/>    ### EXAMPLES ###<br/>    Below are some examples of hypothetical documents, all written by the same author, in pairs of &lt;Input&gt; and &lt;Hypothetical Document&gt;:<br/><br/>    {ref_documents}<br/><br/>    ### INSTRUCTION ###<br/>    Now generate a new hypothetical document. <br/><br/>    &lt;Input&gt;<br/>    {input}<br/>    &lt;Hypothetical Document&gt;<br/><br/>    &quot;&quot;&quot;<br/><br/>    class HypotheticalDocumentSchema(BaseModel):<br/>        hypotheticalDocument: str = Field(description=&quot;a hypothetical document given an input word, phrase or question&quot;)<br/><br/>    parser = JsonOutputParser(pydantic_object=HypotheticalDocumentSchema)<br/><br/>    prompt = ChatPromptTemplate.from_template(<br/>        template=HYPOTHETICAL_DOCUMENT_PROMPT,<br/>        intput_variables = [&quot;input&quot;, &quot;ref_documents&quot;],<br/>        partial_variables = {<br/>            &#x27;format_instructions&#x27;: parser.get_format_instructions()<br/>        }<br/>    )<br/><br/>    hypothetical_document_chain = (<br/>        {&#x27;input&#x27;: RunnablePassthrough(), &#x27;ref_documents&#x27;: RunnablePassthrough()}<br/>        | prompt<br/>        | self.llm_chain<br/>        | parser<br/>    )<br/><br/>    cat_ii = 1<br/>    for cat in cat_dict.keys():<br/><br/>        ref_doc_string = &quot;&quot;<br/>        doc_ii = 1<br/>        for doc in cat_dict[cat]:<br/>            ref_doc_string += f&quot;\n\n&lt;Input&gt;&quot;<br/>            ref_doc_string += text<br/>            ref_doc_string += f&quot;\n\n&lt;Hypothetical Document&gt;\n&quot;<br/>            ref_doc_string += f&#x27;{{&quot;hypotheticalDocument&quot;: &quot;{doc}&quot;}}&#x27;<br/>            doc_ii += 1<br/><br/>        hypo_doc = hypothetical_document_chain.invoke(<br/>            {&#x27;input&#x27;: text, &#x27;ref_documents&#x27;: ref_doc_string}<br/>        )[&#x27;hypotheticalDocument&#x27;]<br/><br/>        if hypo_params[&#x27;verbose&#x27;]:<br/>            print(f&quot;\n### Hypo Doc {cat_ii} ###&quot;)<br/>            print(hypo_doc+&#x27;\n&#x27;)<br/>        <br/>        hypo_docs.append(hypo_doc)<br/>        <br/>        cat_ii += 1<br/>        <br/>    return hypo_docs</span></pre><h2 id="7037" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj">Step 6+7: Embed &amp; Combine</h2><p id="561f" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">Finally, with the list of hypothetical documents, one for each cluster, we embed the hypothetical documents (step 6) and combine them into a single embedding vector (step 7).</p><p id="929e" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">This step is similar to the original implementation, where hypothetical documents are embedded and then averaged to get a final vector.</p><p id="bc1a" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">⚠️ As I mentioned earlier, I don’t think this step really makes sense to capture the broader heterogeneity of documents, especially when we’re trying to discover and generate documents with different relevance patterns. But to keep it simpler and maintain compatibility with the existing LangChain implementation (the point being that you can use this immediately with other component in LangChain RAG), we’ll keep this for now to output a single embedding vector, and perhaps deal with the aggregation limitation later. If you’re convinced that simple averaging is not a good idea, you can always just stop at Step 6.</p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="75d7" class="rr np gd ps b bf rs rt l ru rv">&gt;&gt;&gt; Generating embeddings for hypothetical documents...<br/>&gt;&gt;&gt; Combining embeddings for hypothetical documents...</span></pre><pre class="ry ro ps rp bo rq ba bj"><span id="4eb6" class="rr np gd ps b bf rs rt l ru rv">hyde_embedding[:10]<br/>&gt;&gt;&gt; [<br/>   0.015876703333653572,<br/>  -0.013635452586265312,<br/>   0.021941788843565697,<br/>  -0.02570370381768275,<br/>  -0.015861831315729033,<br/>  -0.0003427757204382006,<br/>   0.0027591148428962454,<br/>  -0.00883308151544041,<br/>  -0.014893267477206646,<br/>  -0.020748802766928837<br/>]</span></pre><p id="0b10" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><strong class="om ge">Technical Implementation</strong></p><pre class="my mz na nb nc ro ps rp bo rq ba bj"><span id="05ef" class="rr np gd ps b bf rs rt l ru rv">def embed_documents(self, texts: List[str], hypo_params) -&gt; List[List[float]]:<br/>    &quot;&quot;&quot;Call the base embeddings.&quot;&quot;&quot;<br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(&quot;\n&gt;&gt;&gt; Generating embeddings for hypothetical documents...\n&quot;)<br/>    return self.base_embeddings.embed_documents(texts)<br/><br/>def combine_embeddings(self, embeddings: List[List[float]], hypo_params) -&gt; List[float]:<br/>    &quot;&quot;&quot;Combine embeddings into final embeddings.&quot;&quot;&quot;<br/>    if hypo_params[&#x27;verbose&#x27;]:<br/>        print(&quot;\n&gt;&gt;&gt; Combining embeddings for hypothetical documents...\n&quot;)<br/>    return list(np.array(embeddings).mean(axis=0))</span></pre><p id="06df" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj"><em class="po">As before, the repo where I implemented this enhanced version of HyDE can be found here at this </em><a class="af pt" href="https://github.com/ianhojy/auto-hyde/tree/main" rel="noopener ugc nofollow" target="_blank"><em class="po">repo</em></a><em class="po">.</em></p></div></div></div><div class="ab ca pv pw px py" role="separator"><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb"></span></div><div class="fw fx fy fz ga"><div class="ab ca"><div class="ch bg fa fb fc fd"><h1 id="c6a3" class="no np gd be nq nr qd gz nt nu qe hc nw nx qf nz oa ob qg od oe of qh oh oi oj bj"><strong class="al">Section 4: Conclusion</strong></h1><p id="ecb9" class="pw-post-body-paragraph ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf fw bj">And that’s the end of the <strong class="om ge">AutoHyDE</strong> demo!</p><p id="00f7" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">To recap, the main improvement being made in AutoHyde is the way the hypothetical documents are being generated. Instead of using a fixed prompt to generate these documents (whether it be pre-defined in LangChain or customised by the user), I have devised a framework to automatically discover the underlying relevance patterns across all the documents that may be <em class="po">neglected </em>by a baseline retrieval, and to generate hypothetical documents for each of these patterns.</p><p id="fd00" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">In this manner, we are able to adapt HyDE for a wider variety of tasks and contexts, as well as to accommodate retrieval over an index which is heterogeneous in relevance pattern.</p><p id="64f4" class="pw-post-body-paragraph ok ol gd om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">If you’ve made it to the end, I’m always happy to hear your thoughts on this approach, and feel free to make suggestions to my implementation as well!</p><blockquote class="pl pm pn"><p id="9639" class="ok ol po om b gx pg oo op ha ph or os ot pi ov ow ox pj oz pa pb pk pd pe pf fw bj">Give a 👍🏻, leave a 💬 and 🤝🏻 with me on <a class="af pt" href="https://www.linkedin.com/in/ianhojy/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>!</p></blockquote></div></div></div><div class="ab ca pv pw px py" role="separator"><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb qc"></span><span class="pz bx bl qa qb"></span></div><div class="fw fx fy fz ga"><div class="ab ca"><div class="ch bg fa fb fc fd"><h2 id="bec6" class="qi np gd be nq qj qk ql nt qm qn qo nw ot qp qq qr ox qs qt qu pb qv qw qx qy bj">References</h2><ul class=""><li id="885c" class="ok ol gd om b gx on oo op ha oq or os ot ou ov ow ox oy oz pa pb pc pd pe pf rc rd re bj">Unsupervised Dense Information Retrieval with Contrastive Learning (<a class="af pt" href="https://arxiv.org/pdf/2112.09118.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2112.09118.pdf</a>)</li><li id="18b5" class="ok ol gd om b gx rf oo op ha rg or os ot rh ov ow ox ri oz pa pb rj pd pe pf rc rd re bj">Precise Zero-Shot Dense Retrieval without Relevance Labels (<a class="af pt" href="https://arxiv.org/pdf/2212.10496.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2212.10496.pdf</a>)</li><li id="a917" class="ok ol gd om b gx rf oo op ha rg or os ot rh ov ow ox ri oz pa pb rj pd pe pf rc rd re bj">LangChain HyDE Repo (<a class="af pt" href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/hyde/base.py" rel="noopener ugc nofollow" target="_blank">https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/hyde</a>)</li><li id="2a45" class="ok ol gd om b gx rf oo op ha rg or os ot rh ov ow ox ri oz pa pb rj pd pe pf rc rd re bj">Original HyDE Implementation (<a class="af pt" href="https://github.com/texttron/hyde/blob/74101c5157e04f7b57559e7da8ef4a4e5b6da82b/src/hyde/hyde.py" rel="noopener ugc nofollow" target="_blank">https://github.com/texttron/hyde/blob/74101c5157e04f7b57559e7da8ef4a4e5b6da82b/src/hyde/hyde.py</a>)</li></ul></div></div></div></div></section></div></div></article><div class="ab ca"><div class="ch bg fa fb fc fd"></div></div></div><div class="ab ca"><div class="ch bg fa fb fc fd"><div class="rz sa ab ix"><div class="sb ab"><a class="sc ax am ao" href="https://medium.com/tag/llm?source=post_page-----619e58cdbd8e---------------llm-----------------" rel="noopener follow"><div class="sd ef cw se ff sf sg be b bf z bj sh">Llm</div></a></div><div class="sb ab"><a class="sc ax am ao" href="https://medium.com/tag/ai?source=post_page-----619e58cdbd8e---------------ai-----------------" rel="noopener follow"><div class="sd ef cw se ff sf sg be b bf z bj sh">AI</div></a></div><div class="sb ab"><a class="sc ax am ao" href="https://medium.com/tag/langchain?source=post_page-----619e58cdbd8e---------------langchain-----------------" rel="noopener follow"><div class="sd ef cw se ff sf sg be b bf z bj sh">Langchain</div></a></div><div class="sb ab"><a class="sc ax am ao" href="https://medium.com/tag/data-science?source=post_page-----619e58cdbd8e---------------data_science-----------------" rel="noopener follow"><div class="sd ef cw se ff sf sg be b bf z bj sh">Data Science</div></a></div><div class="sb ab"><a class="sc ax am ao" href="https://medium.com/tag/hands-on-tutorials?source=post_page-----619e58cdbd8e---------------hands_on_tutorials-----------------" rel="noopener follow"><div class="sd ef cw se ff sf sg be b bf z bj sh">Hands On Tutorials</div></a></div></div></div></div><div class="l"></div><footer class="si sj sk sl sm sn so sp fv ab q sq ig c"><div class="l ae"><div class="ab ca"><div class="ch bg fa fb fc fd"><div class="ab co sr"><div class="ab q kr"><div class="ss l"><span class="l st su sv e d"><div class="ab q kr ks"><div class="pw-multi-vote-icon ef ft kt ku kv"><div class=""><div class="kw kx ky kz la lb lc am ld le lf kv"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l lg lh li lj lk ll lm"><p class="be b dx z dw"><span class="kx">--</span></p></div></div></span><span class="l h g f sw sx"><div class="ab q kr ks"><div class="pw-multi-vote-icon ef ft kt ku kv"><div class=""><div class="kw kx ky kz la lb lc am ld le lf kv"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l lg lh li lj lk ll lm"><p class="be b dx z dw"><span class="kx">--</span></p></div></div></span></div><div class="bp ab"><div><div class="bl" aria-hidden="false"><button class="ao kw lo lp ab q eg lq lr" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" class="gu"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b bf z dw"><span class="pw-responses-count ln gu">6</span></p></button></div></div></div></div><div class="ab q"><div class="qc l iu"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="footerBookmarkButton" class="af eg ah ai aj ak al ls an ao ap ir lt lu lv" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="lw"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div></div><div class="qc l iu"><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af eg ah ai aj ak al ls an ao ap ir me mf lr mg"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg></button></div></div></div></div><div class="bl" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="More options" data-testid="footerStoryOptionsButton" class="af eg ah ai aj ak al ls an ao ap ir me mf lr mg"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div></div></footer><div class="sy sz ta tb tc l bw"><div class="ab ca"><div class="ch bg fa fb fc fd"><div class="ck ab td co"><div class="ab hw"><a href="https://ianhojy.medium.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><div class="l te tf bx tg ia"><div class="l ef"><img alt="Ian Ho" class="l er bx th ti cw" src="https://miro.medium.com/v2/da:true/resize:fill:144:144/0*x4qNNXqMnKmaw-9P" width="72" height="72" loading="lazy"/><div class="ib bx l th ti ep n ic eq"></div></div></div></a><a href="https://towardsdatascience.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><div class="tj ab ef"><div><div class="bl" aria-hidden="false"><div class="l tk tl bx tg ig"><div class="l ef"><img alt="Towards Data Science" class="l er bx by bz cw" src="https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg" width="32" height="32" loading="lazy"/><div class="ib bx l by bz ep n ic eq"></div></div></div></div></div></div></a></div><div class="j i d"><div class="ab"><button class="be b bf z tm sd tn to tp tq tr ts tt tu tv ir tw tx ty tz ua ub er bl uc nk">Follow</button><div class="dv l"><div><div><div class="bl" aria-hidden="false"><div class="l"><button class="be b bf z tm am tn to tp tq tr ts tt tu tv ir tw tx ty ua ub er bl uc nk" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="ud tl tk"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></div></div></div></div></div></div></div></div><div class="ab cm co"><div class="l"><div class="ab q"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab q" href="https://ianhojy.medium.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><h2 class="pw-author-name be ue uf ug uh bj"><span class="fw">Written by <!-- -->Ian Ho</span></h2></a></div><div class="sb ab"><div class="l iu"><span class="pw-follower-count be b bf z bj"><a class="af ag ah ai aj ak al am an ao ap aq ar im" href="https://ianhojy.medium.com/followers?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow">549 Followers</a></span></div><div class="be b bf z jb jc jd ab jf jg jh ji dw ja"><span class="in l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span class="l iu">Writer for </span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar im ab q" href="https://towardsdatascience.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b bf z jb jc jd je jf jg jh ji bj">Towards Data Science</p></a></div></div></div></div><div class="ry l"><p class="be b bf z bj"><span class="fw">Data Scientist @ GovTech</span></p></div></div><div class="h k"><div class="ab"><button class="be b bf z tm sd tn to tp tq tr ts tt tu tv ir tw tx ty tz ua ub er bl uc nk">Follow</button><div class="dv l"><div><div><div class="bl" aria-hidden="false"><div class="l"><button class="be b bf z tm am tn to tp tq tr ts tt tu tv ir tw tx ty ua ub er bl uc nk" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="ud tl tk"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></div></div></div></div></div></div></div></div><div class="ui bg uj fo fp fq fr fs"></div></div></div><div class="h k j"><div class="ui bg uj uk"></div><div class="ab ca"><div class="ch bg fa fb fc fd"><div class="ul ab kr ix"><div class="um un l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">Help</p></a></div><div class="um un l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">Status</p></a></div><div class="um un l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/about?autoplay=1&amp;source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">About</p></a></div><div class="um un l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">Careers</p></a></div><div class="um un l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">Blog</p></a></div><div class="um un l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">Privacy</p></a></div><div class="um un l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">Terms</p></a></div><div class="um un l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">Text to speech</p></a></div><div class="um l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/business?source=post_page-----619e58cdbd8e--------------------------------" rel="noopener follow"><p class="be b dx z dw">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20240426-213717-501a5f7ae6"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"User is logged in","group":"disabled","tags":["group-edgeCachePosts","post-619e58cdbd8e","user-ca061af8c7b7","collection-7f60cf5620c9"],"serverVariantState":"","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":false,"vary":[],"loHomepageEnabled":false,"updatedPostPreviewsEnabled":false,"customMocPreviewWeightThreshold":"control","recommendedTagsQueryEnabled":false,"enableLohpWithSearch":"control","enableLohpFocused":"control","enableMobileLohpShortHero":"control","enableSpamBuster":"control"},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true},"viewerIsBot":false},"debug":{"requestId":"f8cd27d7-c62b-472b-af14-b19fd6b71d62","hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"31759738003c5db8","ot-tracer-traceid":"220c1812b24fe62a","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fautohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"config":{"nodeEnv":"production","version":"main-20240426-213717-501a5f7ae6","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20240426-213717-501a5f7ae6","commit":"501a5f7ae6ad1c8201b23123c9376836f6af3ec3"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":"b0726029b979"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"android_two_hour_refresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier_badge","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_switch_plan_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_easy_resubscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_legacy_feed_in_iceland","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_google_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_dynamic_programming_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_partner_program_enrollment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_auto_follow_on_subscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_premium_plan","valueType":{"__typename":"VariantFlagString","value":"12a660186432"}},{"__typename":"VariantFlag","name":"enable_recaptcha_enterprise","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_autorefresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_programming","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_simplified_digest_v2_b","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expired_membership_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_in_app_free_trial","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"onboarding_tags_from_top_views","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_widget","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_aspiriational","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_eventstats_event_processing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_diversification_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sprig","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_moc_preview_weight_threshold_li","valueType":{"__typename":"VariantFlagString","value":"group_2"}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_entities_to_follow_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lohp_focused","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"ios_enable_verified_book_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_send_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_dashboard_referred_earnings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reengagement_notification_duration","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"enable_android_dynamic_aspirational_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_deprecate_legacy_providers_v3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recirc_model","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_syntax_highlight","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"crm_send_contact_to_sendgrid","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound-source-serif-pro"}},{"__typename":"VariantFlag","name":"enable_seamless_social_sharing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_play_purchase_on_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_validate_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automod","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pill_based_home_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_spam_buster","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"can_receive_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_new_push_notification_endpoint","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lohp_with_search","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_pre_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_display_paywall_after_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_image_sharer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tag_recs","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefined_top_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members_username_selection","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_moc_preview_weight_threshold","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"available_annual_premium_plan","valueType":{"__typename":"VariantFlagString","value":"4a442ace1476"}},{"__typename":"VariantFlag","name":"enable_newsletter_lo_flow_custom_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cache_less_following_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_response_markup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reader_fair_distribution_non_qp","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"textshots_userid","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_pp_writer_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_iceland_forced_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_verifications_service","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_topic_portals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_group_gifting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_miro_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_maim_the_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_updated_new_user_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mobile_lohp_short_hero","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_rex_aggregator_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"price_smoke_test_yearly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_avatar_upload","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_reading_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"web_enable_syntax_highlighting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_fs_cache_user_vals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_verified_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_members_only_audio","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_rating_prompt_stories_read_threshold","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_create_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_lists_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"enable_creator_welcome_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_archive_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_c","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_remove_twitter_onboarding_step","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_social_share_sheet","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"price_smoke_test_monthly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"rex_generator_max_candidates","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_user_onboarding_emails_flow","valueType":{"__typename":"VariantFlagString","value":"group_1"}},{"__typename":"VariantFlag","name":"android_enable_editor_new_publishing_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"isLoggedIn":true,"viewer":{"__ref":"User:2ae680787402"},"collectionByDomainOrSlug({\"domainOrSlug\":\"towardsdatascience.com\"})":{"__ref":"Collection:7f60cf5620c9"},"postResult({\"id\":\"619e58cdbd8e\"})":{"__ref":"Post:619e58cdbd8e"}},"Membership:9a4bea77cdac":{"__typename":"Membership","id":"9a4bea77cdac","tier":"MEMBER","memberSince":1713637217000,"friendSince":null},"UserViewerEdge:userId:2ae680787402-viewerId:2ae680787402":{"__typename":"UserViewerEdge","id":"userId:2ae680787402-viewerId:2ae680787402","createdAt":1568800581191,"membership":{"__ref":"MembershipStatus:{}"}},"User:2ae680787402":{"__typename":"User","id":"2ae680787402","allowEmailAddressSharingEditorWriter":false,"atsQualifiedAt":0,"dismissableFlags":["TOOLTIP_ABOUT_EDITOR"],"emailObfuscated":"ak••••••••@gmail.com","geolocation":{"__typename":"Geolocation","country":"IN"},"hasGroupGiftingEnabled":false,"hasPastMemberships":true,"hasSubdomain":false,"imageId":"0*kMyeT04xC4BXpBc6.jpg","isEligibleToImportEmails":false,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"membership":{"__ref":"Membership:9a4bea77cdac"},"name":"Akash Mishra","partnerProgramEnrollment":null,"postSubscribeMembershipUpsellShownAt":0,"styleEditorOnboardingVersionSeen":0,"twitterScreenName":"","unverifiedEmail":"","username":"akashm1219","viewerEdge":{"__ref":"UserViewerEdge:userId:2ae680787402-viewerId:2ae680787402"}},"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png":{"__typename":"ImageMetadata","id":"1*VzTUkfeGymHP4Bvav-T-lA.png"},"Collection:7f60cf5620c9":{"__typename":"Collection","id":"7f60cf5620c9","favicon":{"__ref":"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png"},"googleAnalyticsId":null,"editors":[{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:7e12c71dfa81"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:e6ad8abedec9"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:895063a310f4"}}],"name":"Towards Data Science","avatar":{"__ref":"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg"},"domain":"towardsdatascience.com","slug":"towards-data-science","description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","subscriberCount":688958,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:2ae680787402"},"twitterUsername":"TDataScience","facebookPageId":null,"logo":{"__ref":"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png"},"customStyleSheet":null,"colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}}},"User:7e12c71dfa81":{"__typename":"User","id":"7e12c71dfa81"},"User:e6ad8abedec9":{"__typename":"User","id":"e6ad8abedec9"},"User:895063a310f4":{"__typename":"User","id":"895063a310f4"},"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg":{"__typename":"ImageMetadata","id":"1*CJe3891yB1A1mzMdqemkdg.jpeg"},"LinkedAccounts:ca061af8c7b7":{"__typename":"LinkedAccounts","mastodon":null,"id":"ca061af8c7b7"},"UserViewerEdge:userId:ca061af8c7b7-viewerId:2ae680787402":{"__typename":"UserViewerEdge","id":"userId:ca061af8c7b7-viewerId:2ae680787402","isFollowing":false,"isUser":false,"isMuting":false},"NewsletterV3:b1dfe19cf495":{"__typename":"NewsletterV3","id":"b1dfe19cf495","type":"NEWSLETTER_TYPE_AUTHOR","slug":"ca061af8c7b7","name":"ca061af8c7b7","collection":null,"user":{"__ref":"User:ca061af8c7b7"}},"User:ca061af8c7b7":{"__typename":"User","id":"ca061af8c7b7","name":"Ian Ho","username":"ianhojy","newsletterV3":{"__ref":"NewsletterV3:b1dfe19cf495"},"linkedAccounts":{"__ref":"LinkedAccounts:ca061af8c7b7"},"isSuspended":false,"imageId":"0*x4qNNXqMnKmaw-9P","mediumMemberAt":1691566395000,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":549},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"ianhojy.medium.com"}},"hasSubdomain":true,"bio":"Data Scientist @ GovTech","isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:ca061af8c7b7-viewerId:2ae680787402"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"allowNotes":true,"membership":{"__ref":"Membership:34d3b27be11e"},"twitterScreenName":""},"Topic:1eca0103fff3":{"__typename":"Topic","slug":"machine-learning","id":"1eca0103fff3","name":"Machine Learning"},"Paragraph:f02f576c9bc6_0":{"__typename":"Paragraph","id":"f02f576c9bc6_0","name":"9b6e","type":"H3","href":null,"layout":null,"metadata":null,"text":"AutoHyDE: Making HyDE Better for Advanced LLM RAG","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_1":{"__typename":"Paragraph","id":"f02f576c9bc6_1","name":"08c4","type":"H4","href":null,"layout":null,"metadata":null,"text":"🔎 A deep-dive into HyDE for Advanced LLM RAG + 💡 Introducing AutoHyDE, a semi-supervised framework to improve the effectiveness, coverage and applicability of HyDE","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BoEb2eI8BFOectBr5y0zjA.png":{"__typename":"ImageMetadata","id":"1*BoEb2eI8BFOectBr5y0zjA.png","originalHeight":874,"originalWidth":1230,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_2":{"__typename":"Paragraph","id":"f02f576c9bc6_2","name":"ac60","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*BoEb2eI8BFOectBr5y0zjA.png"},"text":"Image by Author with the help of DALL-E","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_3":{"__typename":"Paragraph","id":"f02f576c9bc6_3","name":"8b4d","type":"H3","href":null,"layout":null,"metadata":null,"text":"Introduction","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_4":{"__typename":"Paragraph","id":"f02f576c9bc6_4","name":"745d","type":"P","href":null,"layout":null,"metadata":null,"text":"In the field of Retrieval Augmented Generation (RAG), Hypothetical Document Embeddings (HyDE) have proven to be a powerful form of query rewriting to improve the relevance of retrieved documents.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":53,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_5":{"__typename":"Paragraph","id":"f02f576c9bc6_5","name":"9ef5","type":"P","href":null,"layout":null,"metadata":null,"text":"For the uninitiated, whilst traditional retrieval simply uses the original input to create embedding vectors for retrieval, HyDE is a methodology to generate embedding vectors that are more relevant to the embedding space of the indexed documents to be retrieved.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_6":{"__typename":"Paragraph","id":"f02f576c9bc6_6","name":"09d2","type":"P","href":null,"layout":null,"metadata":null,"text":"The super high-level summary is: (1) Create hypothetical documents from user input, (2) Convert hypothetical documents to embeddings, (3) Use embeddings to retrieve similar documents","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_7":{"__typename":"Paragraph","id":"f02f576c9bc6_7","name":"fea6","type":"P","href":null,"layout":null,"metadata":null,"text":"I’ve been using RAG and basic HyDE in some of my work and personal projects, and after some time, I’ve realized that the existing implementation of HyDE does not always work well out of the box and it is not as flexible as I hoped it would be. So after doing my research on the methodology and digging through the papers and source codes, I wanted to share my thoughts on the current approach, and to suggest an enhanced version of HyDE (I call it AutoHyDE) that has the potential to be more effective and adaptable across a variety of use-cases.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":412,"end":546,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_8":{"__typename":"Paragraph","id":"f02f576c9bc6_8","name":"e2d0","type":"P","href":null,"layout":null,"metadata":null,"text":"In this article, Section 1 will be a deep dive into the original HyDE paper and we will go through how it has been translated into the LangChain implementation. If you are already familiar with HyDE, feel free to skip ahead.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":17,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_9":{"__typename":"Paragraph","id":"f02f576c9bc6_9","name":"76c1","type":"P","href":null,"layout":null,"metadata":null,"text":"In Section 2, I will discuss what I believe are the major limitations of the current HyDE approach.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":3,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_10":{"__typename":"Paragraph","id":"f02f576c9bc6_10","name":"8d44","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, I will introduce AutoHyDE in Section 3, an attempt by me to create what I believe can be a better version of HyDE. Here is the tldr:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":26,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_11":{"__typename":"Paragraph","id":"f02f576c9bc6_11","name":"6ba5","type":"BQ","href":null,"layout":null,"metadata":null,"text":"AutoHyDE is a framework to automatically discover underlying relevance patterns in your indexed documents and to generate hypothetical documents that directly represent these relevance patterns.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":194,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_12":{"__typename":"Paragraph","id":"f02f576c9bc6_12","name":"a06c","type":"P","href":null,"layout":null,"metadata":null,"text":"I have also directly tweaked the LangChain HypotheticalDocumentEmbedder class to realize AutoHyDE, so that it can still be chained with other parts of LangChain.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":43,"end":71,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_13":{"__typename":"Paragraph","id":"f02f576c9bc6_13","name":"295a","type":"P","href":null,"layout":null,"metadata":null,"text":"Check out the repo for more details, demo & source code.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":14,"end":18,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":37,"end":41,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Fblob\u002Fmain\u002Fauto-hyde-demo.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":44,"end":55,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Fblob\u002Fmain\u002Fsrc\u002Fauto_hyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_14":{"__typename":"Paragraph","id":"f02f576c9bc6_14","name":"9335","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Give a 👍🏻, leave a 💬 and 🤝🏻 with me on LinkedIn!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":44,"end":52,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fianhojy\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg":{"__typename":"ImageMetadata","id":"1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg","originalHeight":1206,"originalWidth":1696,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_15":{"__typename":"Paragraph","id":"f02f576c9bc6_15","name":"2e6e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg"},"text":"Image by Author: Code Snippet for AutoHyDE","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_16":{"__typename":"Paragraph","id":"f02f576c9bc6_16","name":"2fc9","type":"P","href":null,"layout":null,"metadata":null,"text":"For a higher-level overview of RAG and Query Rewriting, this is a good article written by Florian June. This repo also contains some good practical implementations of RAG, HyDE and other enhancements.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":71,"end":78,"href":"https:\u002F\u002Fmedium.com\u002F@florian_algo\u002Fadvanced-rag-06-exploring-query-rewriting-23997297f2d1","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":109,"end":113,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Frag-from-scratch\u002Fblob\u002Fmain\u002Frag_from_scratch_5_to_9.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":200,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_17":{"__typename":"Paragraph","id":"f02f576c9bc6_17","name":"7cf8","type":"H3","href":null,"layout":null,"metadata":null,"text":"Section 1: What is HyDE?","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_18":{"__typename":"Paragraph","id":"f02f576c9bc6_18","name":"4ac8","type":"H4","href":null,"layout":null,"metadata":null,"text":"HyDE: The Original Paper","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_19":{"__typename":"Paragraph","id":"f02f576c9bc6_19","name":"4b56","type":"P","href":null,"layout":null,"metadata":null,"text":"Hypothetical Document Embeddings (HyDE) were first introduced in the 2022 paper by Gao et al. titled Precise Zero-Shot Dense Retrieval without Relevance Labels.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":101,"end":159,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_20":{"__typename":"Paragraph","id":"f02f576c9bc6_20","name":"cda4","type":"P","href":null,"layout":null,"metadata":null,"text":"The paper set out to find a way to improve zero-shot dense retrieval (i.e. using semantic embedding similarities). To that end, they came up with a two-step methodology called HyDE.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_21":{"__typename":"Paragraph","id":"f02f576c9bc6_21","name":"e645","type":"P","href":null,"layout":null,"metadata":null,"text":"Step 1 involved instruction prompting a language model (in the paper they use GPT-3) to generate a hypothetical document given the original query (specifically limited to a question in the paper).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_22":{"__typename":"Paragraph","id":"f02f576c9bc6_22","name":"3524","type":"P","href":null,"layout":null,"metadata":null,"text":"Step 2 involved using a Contriever, or an “unsupervised contrastive encoder”, to turn this hypothetical document into an embedding vector, which is then used for downstream similarity search and retrieval.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":24,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":42,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UFRWRzsuBxwRypjA_y9tJQ.png":{"__typename":"ImageMetadata","id":"1*UFRWRzsuBxwRypjA_y9tJQ.png","originalHeight":734,"originalWidth":2592,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_23":{"__typename":"Paragraph","id":"f02f576c9bc6_23","name":"1928","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*UFRWRzsuBxwRypjA_y9tJQ.png"},"text":"Illustration of HyDE methodology from the original paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":42,"end":56,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_24":{"__typename":"Paragraph","id":"f02f576c9bc6_24","name":"5521","type":"H4","href":null,"layout":null,"metadata":null,"text":"[Optional] An aside on the Contriever","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":1,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":27,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_25":{"__typename":"Paragraph","id":"f02f576c9bc6_25","name":"c72c","type":"P","href":null,"layout":null,"metadata":null,"text":"The Contriever used in the original HyDE paper was derived from an earlier paper in August 2022 by Izacard et al. called Unsupervised Dense Information Retrieval with Contrastive Learning. In this paper, the authors claim that while neural networks had emerged as good alternatives to term-frequency methods for retrieval, they required large amounts of data and did not always transfer well to new areas of application. Therefore, they devised a way to train the embedding network in an unsupervised manner through contrastive learning.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":121,"end":187,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2112.09118.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":515,"end":536,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":4,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_26":{"__typename":"Paragraph","id":"f02f576c9bc6_26","name":"363f","type":"P","href":null,"layout":null,"metadata":null,"text":"For those interested in Contrastive Learning, you can check out Section 3 of the paper for information, but here’s a high-level summary:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_27":{"__typename":"Paragraph","id":"f02f576c9bc6_27","name":"b36a","type":"P","href":null,"layout":null,"metadata":null,"text":"First, they define a loss function (contrastive InfoNCE loss) that rewards positive pairs of documents and penalizes negative pairs. Next, they build positive and negative pairs of document representations. Positive pairs are constructed using a data augmentation method called the Inverse Cloze Task (ICT), and through Independent Cropping. Negative pairs are constructed using in-batch negative sampling, and across-batch negative sampling (also called MoCo). Finally, they train this over the BERT base uncased architecture.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_28":{"__typename":"Paragraph","id":"f02f576c9bc6_28","name":"f412","type":"P","href":null,"layout":null,"metadata":null,"text":"The results of the paper showed that this method was able to match unsupervised term-frequency methods like BM25, and also performed well on cross-lingual retrieval, an option which is not possible for term matching methods.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_29":{"__typename":"Paragraph","id":"f02f576c9bc6_29","name":"ffb0","type":"P","href":null,"layout":null,"metadata":null,"text":"For the purposes of this article, I will not be going further into the details of training and data augmentation, but do check out the paper if you are interested. The contriever can also be found on HuggingFace at this link, and the research repo can be found here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":220,"end":224,"href":"https:\u002F\u002Fhuggingface.co\u002Ffacebook\u002Fcontriever","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":261,"end":265,"href":"https:\u002F\u002Fgithub.com\u002Ffacebookresearch\u002Fcontriever","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_30":{"__typename":"Paragraph","id":"f02f576c9bc6_30","name":"388f","type":"P","href":null,"layout":null,"metadata":null,"text":"Anyway, It is this Contriever that the HyDE paper uses to embed generated documents. They also explore ContrieverFT, which is in-domain fine-tuned in a supervised manner. Now, back to the main focus of the article, which is the HyDE Paper.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":19,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":103,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_31":{"__typename":"Paragraph","id":"f02f576c9bc6_31","name":"96f3","type":"H4","href":null,"layout":null,"metadata":null,"text":"Back to the HyDE Paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_32":{"__typename":"Paragraph","id":"f02f576c9bc6_32","name":"b4ef","type":"P","href":null,"layout":null,"metadata":null,"text":"Previously, I provided an overview of the HyDE methodology and the origins of the underlying contriever. Now, let’s take a closer look at what’s going on in HyDE by reviewing some key excerpts from the paper.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":93,"end":105,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_33":{"__typename":"Paragraph","id":"f02f576c9bc6_33","name":"3a4f","type":"BQ","href":null,"layout":null,"metadata":null,"text":"…In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder’s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_34":{"__typename":"Paragraph","id":"f02f576c9bc6_34","name":"c9d7","type":"P","href":null,"layout":null,"metadata":null,"text":"More formally, the hypothetical document embedding vector is defined as:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UBvSsoJUagB0lh7r4j661A.png":{"__typename":"ImageMetadata","id":"1*UBvSsoJUagB0lh7r4j661A.png","originalHeight":126,"originalWidth":1012,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_35":{"__typename":"Paragraph","id":"f02f576c9bc6_35","name":"9ead","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*UBvSsoJUagB0lh7r4j661A.png"},"text":"Formula for encoding the generated document in the HyDE Paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":51,"end":61,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_36":{"__typename":"Paragraph","id":"f02f576c9bc6_36","name":"d2b4","type":"ULI","href":null,"layout":null,"metadata":null,"text":"where g is the InstructLM(query, INST) function in the first step of hypothetical document generation; and","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":15,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_37":{"__typename":"Paragraph","id":"f02f576c9bc6_37","name":"d525","type":"ULI","href":null,"layout":null,"metadata":null,"text":"where f is the document encoder (contriever)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":33,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_38":{"__typename":"Paragraph","id":"f02f576c9bc6_38","name":"0eb9","type":"P","href":null,"layout":null,"metadata":null,"text":"“Formally, g defines a probability distribution based on the chain rule”, and V is estimated through the following equation:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*y9lHVX24zORuojIz8GMiMA.png":{"__typename":"ImageMetadata","id":"1*y9lHVX24zORuojIz8GMiMA.png","originalHeight":454,"originalWidth":878,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_39":{"__typename":"Paragraph","id":"f02f576c9bc6_39","name":"e666","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*y9lHVX24zORuojIz8GMiMA.png"},"text":"Formula for estimating the embedding vector in the HyDE Paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":51,"end":61,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_40":{"__typename":"Paragraph","id":"f02f576c9bc6_40","name":"a4a5","type":"P","href":null,"layout":null,"metadata":null,"text":"In the original paper, you can also find the python implementation at this repo, where:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":75,"end":79,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_41":{"__typename":"Paragraph","id":"f02f576c9bc6_41","name":"f227","type":"P","href":null,"layout":null,"metadata":null,"text":"f is the generative function as we see in the repo:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:01ab5468a67531ae11e216e18ce1ba16":{"__typename":"MediaResource","id":"01ab5468a67531ae11e216e18ce1ba16","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"hyde-generate.py"},"Paragraph:f02f576c9bc6_42":{"__typename":"Paragraph","id":"f02f576c9bc6_42","name":"881e","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"generate function from texttron\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":23,"end":36,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:01ab5468a67531ae11e216e18ce1ba16"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_43":{"__typename":"Paragraph","id":"f02f576c9bc6_43","name":"a2a6","type":"P","href":null,"layout":null,"metadata":null,"text":"g is the encoding function as we see in the repo:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:3c8f50e3614243b14f4eab2837713b63":{"__typename":"MediaResource","id":"3c8f50e3614243b14f4eab2837713b63","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"hyde-encode.py"},"Paragraph:f02f576c9bc6_44":{"__typename":"Paragraph","id":"f02f576c9bc6_44","name":"c833","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"encode function from texttron\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":21,"end":34,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:3c8f50e3614243b14f4eab2837713b63"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_45":{"__typename":"Paragraph","id":"f02f576c9bc6_45","name":"2139","type":"P","href":null,"layout":null,"metadata":null,"text":"Just as in the estimation equation, we see that a simple averaging is used to estimate the embedding vector across the various hypothetical documents. This mean-aggregated embedding is then used to do similarity search:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:84f3be025edaa08ac709e2943e3e8f2e":{"__typename":"MediaResource","id":"84f3be025edaa08ac709e2943e3e8f2e","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"hyde-search.py"},"Paragraph:f02f576c9bc6_46":{"__typename":"Paragraph","id":"f02f576c9bc6_46","name":"c5b7","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"search function from texttron\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":21,"end":34,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:84f3be025edaa08ac709e2943e3e8f2e"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_47":{"__typename":"Paragraph","id":"f02f576c9bc6_47","name":"20e0","type":"P","href":null,"layout":null,"metadata":null,"text":"Overall, we see that HyDE is really not a complicated implementation. Generate, embed, average, retrieve.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":70,"end":104,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_48":{"__typename":"Paragraph","id":"f02f576c9bc6_48","name":"b3bf","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, let’s take a look at popular implementations of HyDE that have surfaced recently.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_49":{"__typename":"Paragraph","id":"f02f576c9bc6_49","name":"252e","type":"H4","href":null,"layout":null,"metadata":null,"text":"LangChain Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_50":{"__typename":"Paragraph","id":"f02f576c9bc6_50","name":"b66b","type":"P","href":null,"layout":null,"metadata":null,"text":"In both LangChain & LlamaIndex, there are implementations of HyDE that are similar to what we’ve seen in the paper and repo above. For most of us who are playing around with LLM capabilities, these are the most likely ways that we’ll be looking to use HyDE. We’ll also have a closer look at the prompts being used under the hood for these HyDE classes. Let’s focus on the LangChain implementation. The repo for reference is here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":424,"end":428,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde\u002Fbase.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:4693b29b6b93f92975dcb520235bea23":{"__typename":"MediaResource","id":"4693b29b6b93f92975dcb520235bea23","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"langchain-hyde-from-llm.py"},"Paragraph:f02f576c9bc6_51":{"__typename":"Paragraph","id":"f02f576c9bc6_51","name":"760e","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"from_llm constructor for HypotheticalDocumentEmbedder in langchain\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":57,"end":71,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde\u002Fbase.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:4693b29b6b93f92975dcb520235bea23"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_52":{"__typename":"Paragraph","id":"f02f576c9bc6_52","name":"5087","type":"P","href":null,"layout":null,"metadata":null,"text":"The first step is to create the HypotheticalDocumentEmbedder using from_llm — you can choose one of the existing prompt templates such as the one you see below for writing a news passage, or define your own custom prompt.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":32,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":67,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_53":{"__typename":"Paragraph","id":"f02f576c9bc6_53","name":"09de","type":"PRE","href":null,"layout":null,"metadata":null,"text":"trec_news_template = \"\"\"Please write a news passage about the topic.\nTopic: {TOPIC}\nPassage:\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_54":{"__typename":"Paragraph","id":"f02f576c9bc6_54","name":"1185","type":"P","href":null,"layout":null,"metadata":null,"text":"Then, you can execute embed_query , which first generates the hypothetical document(s) based on the prompt template, and then translates these documents into embeddings and combines them. The embed_documents function simply uses the defined base_embeddings from the from_llm definition, while the combine_embeddings is a mean aggregation.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":22,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":192,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":241,"end":256,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":266,"end":274,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":297,"end":315,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:66bcb6e766294aa26a85adc1de110f9b":{"__typename":"MediaResource","id":"66bcb6e766294aa26a85adc1de110f9b","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"langchain-hyde-embed-query.py"},"Paragraph:f02f576c9bc6_55":{"__typename":"Paragraph","id":"f02f576c9bc6_55","name":"4e31","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"embed_query function for langchain\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":25,"end":39,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde\u002Fbase.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:66bcb6e766294aa26a85adc1de110f9b"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_56":{"__typename":"Paragraph","id":"f02f576c9bc6_56","name":"cfe7","type":"P","href":null,"layout":null,"metadata":null,"text":"You will also notice that documents is of List[str] type because you can generate multiple documents. All you have to do is define the LLM accordingly in the from_llm step. Here’s an example:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":42,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":158,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_57":{"__typename":"Paragraph","id":"f02f576c9bc6_57","name":"3464","type":"PRE","href":null,"layout":null,"metadata":null,"text":"multi_llm = OpenAI(n=4, best_of=4)\nembeddings = HypotheticalDocumentEmbedder.from_llm(\n    multi_llm, base_embeddings, \"web_search\"\n)\nresult = embeddings.embed_query(\"Where is the Taj Mahal?\")","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_58":{"__typename":"Paragraph","id":"f02f576c9bc6_58","name":"f364","type":"P","href":null,"layout":null,"metadata":null,"text":"For a simple walkthrough of how to use HyDE, you can refer to the notebook. It’s nothing too complicated and LangChain has made it really easy to use by abstracting away most of the underlying functionality.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":66,"end":74,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Fblob\u002Fmain\u002Flangchain-hyde-demo.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_59":{"__typename":"Paragraph","id":"f02f576c9bc6_59","name":"2fe3","type":"H3","href":null,"layout":null,"metadata":null,"text":"Section 2: Limitations of HyDE","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_60":{"__typename":"Paragraph","id":"f02f576c9bc6_60","name":"381b","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we have a good idea of how HyDE was originally designed and how it was eventually implemented in the popular LangChain & LlamaIndex libraries, I want to talk about their limitations.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_61":{"__typename":"Paragraph","id":"f02f576c9bc6_61","name":"c51b","type":"P","href":null,"layout":null,"metadata":null,"text":"The first thing I noticed is that whilst the HyDE paper does experiment with different instruction LLMs, it does not spend too much time talking about further optimizing this aspect of the HyDE process. From table 4 in the paper, we see that using different models results in rather huge result variance:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*u9xHr0QqHbCMaPw-C-hvkQ.png":{"__typename":"ImageMetadata","id":"1*u9xHr0QqHbCMaPw-C-hvkQ.png","originalHeight":648,"originalWidth":780,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_62":{"__typename":"Paragraph","id":"f02f576c9bc6_62","name":"16ad","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*u9xHr0QqHbCMaPw-C-hvkQ.png"},"text":"HyDE results from using different LLMs in the HyDE Paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":46,"end":56,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_63":{"__typename":"Paragraph","id":"f02f576c9bc6_63","name":"92bc","type":"P","href":null,"layout":null,"metadata":null,"text":"The key implication here is not to use the biggest models with the most parameters. Instead, the fact that the underlying LLMs bring about such huge differences for the overall tasks of retrieval relevance convinces me that apart from using different LLMs, the generative task is one area worth further optimizing.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":257,"end":313,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_64":{"__typename":"Paragraph","id":"f02f576c9bc6_64","name":"b957","type":"P","href":null,"layout":null,"metadata":null,"text":"In fact, the researchers themselves do state that:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_65":{"__typename":"Paragraph","id":"f02f576c9bc6_65","name":"2730","type":"BQ","href":null,"layout":null,"metadata":null,"text":"InstructGPT model able to further bring up the performance… this suggests that there may still exist certain factors not captured by the fine-tuned encoder but only by the generative model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_66":{"__typename":"Paragraph","id":"f02f576c9bc6_66","name":"7f84","type":"P","href":null,"layout":null,"metadata":null,"text":"With that possibility in mind, we have also seen from the LangChain deep-dive in Section 1 that the prompt templates are pre-defined to be generic in instructing hypothetical documents. For example, “Please write a scientific paper passage to support\u002Frefute the claim”. This only works when your query\u002Finput nicely fits these pre-defined templates. Otherwise, you can also provide a custom template to generate hypothetical documents, but this still has its limitations.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":199,"end":268,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_67":{"__typename":"Paragraph","id":"f02f576c9bc6_67","name":"8f71","type":"P","href":null,"layout":null,"metadata":null,"text":"Firstly, the existing implementations are largely limited to a Q&A framework. In the real world, not all retrieval use-cases are for answering questions, and thus asking the LLM to hypothetically answer your question does not make make sense. So in all these other situations, what do we ask the LLM to generate hypothetically then? Yes, custom prompting is available, but there is no guarantee that the prompt you write is actually relevant to the existing documents. Unless the underlying documents are very uniform in terms of relevance patterns, writing a prompt to capture these patterns would be a very manual and imperfect process.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_68":{"__typename":"Paragraph","id":"f02f576c9bc6_68","name":"af16","type":"P","href":null,"layout":null,"metadata":null,"text":"More importantly, when you chunk and index your documents, the chunks are not always homogenous in terms of style, tone, structure etc. Consequently, it may be insufficient to use a single generalized prompt to generate hypothetical documents given your input, and it may be intractable to write multiple prompts to cater to different kinds of chunks. For instance, imagine you are trying to generate hypothetical documents for blog posts. With different authors, tones, writing styles, how do you create a generalized prompt to capture this variety? Thus, the current implementation of HyDE appears to be rather rigid and suboptimal in terms of coverage.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":63,"end":95,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":160,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_69":{"__typename":"Paragraph","id":"f02f576c9bc6_69","name":"8bfa","type":"P","href":null,"layout":null,"metadata":null,"text":"On a more technical level, we also note that regarding the final vector embedding, the paper says that:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_70":{"__typename":"Paragraph","id":"f02f576c9bc6_70","name":"d779","type":"BQ","href":null,"layout":null,"metadata":null,"text":"We simply consider the expectation value, assuming the distribution of v_qij is uni-modal, i.e. the query is not ambiguous.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_71":{"__typename":"Paragraph","id":"f02f576c9bc6_71","name":"c205","type":"P","href":null,"layout":null,"metadata":null,"text":"But if you think about it, most humans approach search problems with ambiguity.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_72":{"__typename":"Paragraph","id":"f02f576c9bc6_72","name":"9134","type":"P","href":null,"layout":null,"metadata":null,"text":"This is an important caveat. In real use-cases, I can imagine that the distribution of v is not in fact nicely uni-modal, especially when the query is as ambiguous as just one or two keywords. What this means is that the same underlying query can be mapped to more than one mode of vector embedding representations.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_73":{"__typename":"Paragraph","id":"f02f576c9bc6_73","name":"56aa","type":"P","href":null,"layout":null,"metadata":null,"text":"How might we begin to improve HyDE?","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_74":{"__typename":"Paragraph","id":"f02f576c9bc6_74","name":"8504","type":"BQ","href":null,"layout":null,"metadata":null,"text":"HyDE appears unsupervised. No model is trained in HyDE: both the generative model and the contrastive encoder remain intact.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_75":{"__typename":"Paragraph","id":"f02f576c9bc6_75","name":"4894","type":"P","href":null,"layout":null,"metadata":null,"text":"The unsupervised nature of HyDE is important because it does away with the need to have huge datasets or generous amounts of compute for any supervised variant of methodology to improve dense retrieval.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_76":{"__typename":"Paragraph","id":"f02f576c9bc6_76","name":"3ed3","type":"P","href":null,"layout":null,"metadata":null,"text":"However, by remaining strictly unsupervised, we lose some benefits. Specifically, there is no way to dynamically define multiple generative functions g(q, INST) that are more aligned with the document embeddings.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":150,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_77":{"__typename":"Paragraph","id":"f02f576c9bc6_77","name":"6bd8","type":"P","href":null,"layout":null,"metadata":null,"text":"Instead, it might be worth considering a semi-supervised approach that is more adaptive and generalizable. This approach does not require one to forcefully fit hypothetical generation of documents into a single pre-defined\u002Fcustom prompt. It does not even require one to know what custom prompts to write, unlike the LangChain\u002FLlamaIndex implementations.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":39,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_78":{"__typename":"Paragraph","id":"f02f576c9bc6_78","name":"29ba","type":"P","href":null,"layout":null,"metadata":null,"text":"Instead, an enhanced version of HyDE will automatically learn a greater variety of relevance patterns above and beyond a baseline retrieval, and generate hypothetical documents that are aligned with different clusters of indexed chunks. I call this approach AutoHyDE.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":56,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":258,"end":266,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_79":{"__typename":"Paragraph","id":"f02f576c9bc6_79","name":"5a22","type":"P","href":null,"layout":null,"metadata":null,"text":"Before talking more about AutoHyDE, I have one final point to bring up regarding with the current HyDE implementation. I will not be addressing it in AutoHyDE, but I think it deserves some attention.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_80":{"__typename":"Paragraph","id":"f02f576c9bc6_80","name":"97cb","type":"P","href":null,"layout":null,"metadata":null,"text":"Recall again the averaging equation across embeddings, which manifests as the following line of code:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_81":{"__typename":"Paragraph","id":"f02f576c9bc6_81","name":"4206","type":"PRE","href":null,"layout":null,"metadata":null,"text":"list(np.array(embeddings).mean(axis=0))","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_82":{"__typename":"Paragraph","id":"f02f576c9bc6_82","name":"20f0","type":"P","href":null,"layout":null,"metadata":null,"text":"It’s a simple aggregation but does not make much sense to me, especially when you consider the multi-modal distribution of the embeddings. Let’s say you manage to construct the chained encoding function f(g(q, INST)) that nicely captures the diversity of the relevance patterns across all your document chunks. When you take averages across these multi-modal distributed encodings, you end up with an average that may not be relevant at all (think Jensen’s inequality kind of averaging). Of course, this all works well when you have a uni-modal distribution, but this is rarely a realistic assumption, and there is no way to verify it universally.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":203,"end":216,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_83":{"__typename":"Paragraph","id":"f02f576c9bc6_83","name":"826e","type":"P","href":null,"layout":null,"metadata":null,"text":"Anyway, on to AutoHyDE.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":14,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_84":{"__typename":"Paragraph","id":"f02f576c9bc6_84","name":"8099","type":"H3","href":null,"layout":null,"metadata":null,"text":"Section 3: AutoHyDE","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_85":{"__typename":"Paragraph","id":"f02f576c9bc6_85","name":"9e95","type":"H4","href":null,"layout":null,"metadata":null,"text":"Overview","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_86":{"__typename":"Paragraph","id":"f02f576c9bc6_86","name":"4d61","type":"P","href":null,"layout":null,"metadata":null,"text":"To recap, the main point of AutoHyDE is to automatically discover the variety of relevance patterns in your vector database and generate a variety of documents to improve coverage of these patterns.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":43,"end":99,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":128,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_87":{"__typename":"Paragraph","id":"f02f576c9bc6_87","name":"8dd7","type":"P","href":null,"layout":null,"metadata":null,"text":"Technically, I took the existing implementation of class HypotheticalDocumentEmbedder in LangChain and created new functions to achieve AutoHyDE, so that it works immediately as part of any RAG chain that you might have. This is the updated embed_query function that I wrote, and will explore each of the sub-methods in the following walkthrough.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":241,"end":252,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":50,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*oenoqdYqCyvlHYGQ9GEn1A.jpeg":{"__typename":"ImageMetadata","id":"1*oenoqdYqCyvlHYGQ9GEn1A.jpeg","originalHeight":1206,"originalWidth":1696,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_88":{"__typename":"Paragraph","id":"f02f576c9bc6_88","name":"e9e9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*oenoqdYqCyvlHYGQ9GEn1A.jpeg"},"text":"Image by Author: Src Code Snippet from AutoHyDE Repo","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_89":{"__typename":"Paragraph","id":"f02f576c9bc6_89","name":"eec7","type":"P","href":null,"layout":null,"metadata":null,"text":"🧑🏻‍💻 Check out the implementation here if you are interested to try it out: https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":79,"end":125,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":78,"end":125,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_90":{"__typename":"Paragraph","id":"f02f576c9bc6_90","name":"b457","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s a quick demo of how to use it:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:0b8e21808f3ee4ec622424dfecaf7ed5":{"__typename":"MediaResource","id":"0b8e21808f3ee4ec622424dfecaf7ed5","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"auto-hyde-demo.py"},"Paragraph:f02f576c9bc6_91":{"__typename":"Paragraph","id":"f02f576c9bc6_91","name":"0e1c","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"Demo code for AutoHyDE","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:0b8e21808f3ee4ec622424dfecaf7ed5"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_92":{"__typename":"Paragraph","id":"f02f576c9bc6_92","name":"c16d","type":"P","href":null,"layout":null,"metadata":null,"text":"To illustrate how AutoHyDE works and, I’m going to be using J. S. Mill’s Utilitarianism. I’m going to chunk it, index it and try to receive the relevant chunks based on a query.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":73,"end":87,"href":"https:\u002F\u002Fsacred-texts.com\u002Fphi\u002Fmill\u002Futil.txt","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_93":{"__typename":"Paragraph","id":"f02f576c9bc6_93","name":"7bc6","type":"P","href":null,"layout":null,"metadata":null,"text":"I chose this text not just because I spent countless hours struggling with it as an undergraduate back in the good old days of Philosophy 101, but also because it’s a good example of a text where it’s really difficult to write a custom prompt to generate hypothetical documents, even if you hold a PhD in Moral Philosophy with endless knowledge of Mill. Let’s just say that Mill’s writing is not the most straightforward and consistent, he’s really an expert at run-on sentences (so am I).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_94":{"__typename":"Paragraph","id":"f02f576c9bc6_94","name":"525f","type":"P","href":null,"layout":null,"metadata":null,"text":"As I walk through my implementation of AutoHyDE below, I’ll have both high-level explanations and more technical descriptions for those interested in the repo.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":154,"end":158,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_95":{"__typename":"Paragraph","id":"f02f576c9bc6_95","name":"5aaf","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 1: Extract Key Words from Query","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_96":{"__typename":"Paragraph","id":"f02f576c9bc6_96","name":"720c","type":"P","href":null,"layout":null,"metadata":null,"text":"Imagine you received this essay assignment:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_97":{"__typename":"Paragraph","id":"f02f576c9bc6_97","name":"16fa","type":"BQ","href":null,"layout":null,"metadata":null,"text":"What is the relationship between justice and happiness?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_98":{"__typename":"Paragraph","id":"f02f576c9bc6_98","name":"c8ef","type":"P","href":null,"layout":null,"metadata":null,"text":"At this step 1, the LLM will be prompted to extract the key words, in this case:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_99":{"__typename":"Paragraph","id":"f02f576c9bc6_99","name":"a352","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003E\u003E\u003E Extracting Keywords from your Query…\n\u003E\u003E\u003E …Keywords Extracted: ['relationship', 'justice', 'happiness']","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"plaintext"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_100":{"__typename":"Paragraph","id":"f02f576c9bc6_100","name":"c761","type":"P","href":null,"layout":null,"metadata":null,"text":"The key words are important in this case because they will be utilized in Step 3, in order to quickly (and dirtily) identify documents that you would have left out in a normal retrieval.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_101":{"__typename":"Paragraph","id":"f02f576c9bc6_101","name":"2511","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_102":{"__typename":"Paragraph","id":"f02f576c9bc6_102","name":"e9b2","type":"P","href":null,"layout":null,"metadata":null,"text":"This is the function I wrote to extract the keywords:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_103":{"__typename":"Paragraph","id":"f02f576c9bc6_103","name":"a8ec","type":"PRE","href":null,"layout":null,"metadata":null,"text":"@retry(tries=5)\ndef extract_keywords(\n    self, \n    text: str, \n    hypo_params: dict\n    ) -\u003E List[str]:\n\n    if hypo_params['verbose']:\n        print(f\"\\n\u003E\u003E\u003E Extracting Keywords from your Query...\")\n    \n    KEYWORD_EXTRACTION_PROMPT = \"\"\"\n    Your goal is to extract a list of keywords from an input phrase, sentence, or several sentences.\n\n    - You can only generate 1 to 5 keywords.\n    - Keywords should be nouns, issues, concepts\n    - Keywords should not include verbs, prepositions, pronouns\n    - Each keyword can only be one word long.\n    - If the input is just a single word, return that word as the only keyword.\n\n    {format_instructions}\n\n    The input is:\n    {input}\n    \"\"\"\n\n    class KeywordListSchema(BaseModel):\n        keywordList: list[str] = Field(description=\"list of one-word keywords based on a given phrase\")\n\n    parser = JsonOutputParser(pydantic_object=KeywordListSchema)\n\n    prompt = ChatPromptTemplate.from_template(\n        template=KEYWORD_EXTRACTION_PROMPT,\n        intput_variables = [\"input\"],\n        partial_variables = {\n            'format_instructions': parser.get_format_instructions()\n        }\n    )\n\n    keyword_extraction_chain = (\n        {'input': RunnablePassthrough()}\n        | prompt\n        | self.llm_chain\n        | parser\n    )\n    \n    keywords = keyword_extraction_chain.invoke(text)['keywordList']\n    if hypo_params['verbose']:\n        print(f\"\u003E\u003E\u003E ...Keywords Extracted: {keywords}\\n\")\n    \n    return keywords","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_104":{"__typename":"Paragraph","id":"f02f576c9bc6_104","name":"eaa0","type":"P","href":null,"layout":null,"metadata":null,"text":"Nothing fancy here, just using the JsonOutputParser to ensure that I get the exact output which is a list of keywords.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_105":{"__typename":"Paragraph","id":"f02f576c9bc6_105","name":"f548","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 2: Do initial retrieval","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_106":{"__typename":"Paragraph","id":"f02f576c9bc6_106","name":"6206","type":"P","href":null,"layout":null,"metadata":null,"text":"Apart from extracting the keywords, we’ll also use the original query to do an initial retrieval. You’ll have to ask yourself, how many chunks do you want to retrieve as part of your workflow?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_107":{"__typename":"Paragraph","id":"f02f576c9bc6_107","name":"44d9","type":"P","href":null,"layout":null,"metadata":null,"text":"Assuming that you initially intended to retrieve 20 documents to use as context for answering your question in RAG, what this step 2 will then do is to retrieve more than 20 documents so that we can explore potential documents that you would have neglected (for lack of a better word) due to your cut off of 20. This can be toggled using exploration_multiplier. With a baseline_k of 20 and exploration_multiplier of 5, we will be exploring 20 x 5 = 100 top documents based on cosine similarity.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":338,"end":360,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":369,"end":379,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":390,"end":412,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":247,"end":256,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":369,"end":379,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_108":{"__typename":"Paragraph","id":"f02f576c9bc6_108","name":"4759","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_109":{"__typename":"Paragraph","id":"f02f576c9bc6_109","name":"6ec0","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def do_init_retrieval(\n    self,\n    db: VectorStore, \n    text: str, \n    hypo_params: dict\n    ) -\u003E List[Tuple[Document, float]]:\n    \n    k = hypo_params['baseline_k'] * hypo_params['exploration_multiplier']\n    if hypo_params['verbose']:\n        print(f\"\\n\u003E\u003E\u003E Performing Initial Retrieval of {k} documents...\\n\")\n    docs = db.similarity_search_with_score(\n        text, \n        k=k\n    )\n    return docs","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_110":{"__typename":"Paragraph","id":"f02f576c9bc6_110","name":"3b37","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 3: Get neglected documents that contain the keywords","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_111":{"__typename":"Paragraph","id":"f02f576c9bc6_111","name":"01a2","type":"P","href":null,"layout":null,"metadata":null,"text":"So above and beyond the 20 documents that you would have retrieved from a straightforward retrieval, there are 80 documents that are now up for consideration that you would have neglected.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":178,"end":187,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_112":{"__typename":"Paragraph","id":"f02f576c9bc6_112","name":"2463","type":"P","href":null,"layout":null,"metadata":null,"text":"Out of these 80 documents, we want to find the subset of documents that would have been directly relevant to your original query. How do we do this efficiently? One simple way would just be to select the documents that contain any of the keywords extracted from step 1. There are ways to make this better, but I think keyword matching is already a quick and rather effective way to identify the first cut of neglected documents. Maybe we’ll explore better ways to do this identification, but this will do for now.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_113":{"__typename":"Paragraph","id":"f02f576c9bc6_113","name":"35a6","type":"P","href":null,"layout":null,"metadata":null,"text":"This is what you will see as output from the code:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_114":{"__typename":"Paragraph","id":"f02f576c9bc6_114","name":"bede","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003E\u003E\u003E Checking 80 Docs ranked after 20 for presence of keyword…\n\u003E\u003E\u003E …69 neglected Docs identified","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"plaintext"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_115":{"__typename":"Paragraph","id":"f02f576c9bc6_115","name":"3601","type":"P","href":null,"layout":null,"metadata":null,"text":"In the output above, we see that out of the 80 Documents, 69 contained one or more of the keywords [‘relationship’, ‘justice’, ‘happiness’].","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_116":{"__typename":"Paragraph","id":"f02f576c9bc6_116","name":"6f84","type":"P","href":null,"layout":null,"metadata":null,"text":"Intuitively, we can understand these as potentially relevant documents that would have been neglected in a normal retrieval of the top 20 documents.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_117":{"__typename":"Paragraph","id":"f02f576c9bc6_117","name":"600b","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_118":{"__typename":"Paragraph","id":"f02f576c9bc6_118","name":"1f87","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def get_remaining_docs_with_keywords(\n    self, \n    text: str, \n    init_docs: List[Tuple[Document, float]], \n    keywords: List[str], \n    hypo_params: dict\n    ) -\u003E List[Document]:\n\n    remaining_docs_with_keywords = list()\n    \n    if hypo_params['verbose']:\n        print(f\"\"\"\\n\u003E\u003E\u003E Checking {len(init_docs[hypo_params['baseline_k']:])} \n                Docs ranked after {hypo_params['baseline_k']} for presence of keyword...\"\"\")\n\n    for r in init_docs[hypo_params['baseline_k']:]:\n        page_content = r[0].page_content.lower()\n        for keyword in keywords:\n            if keyword.lower() in page_content:\n                remaining_docs_with_keywords.append(r)\n                continue\n                \n    if hypo_params['verbose']:\n        print(f\"\u003E\u003E\u003E ...{len(remaining_docs_with_keywords)} neglected Docs identified\\n\")\n    return remaining_docs_with_keywords","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_119":{"__typename":"Paragraph","id":"f02f576c9bc6_119","name":"f8b9","type":"P","href":null,"layout":null,"metadata":null,"text":"For each document, if any of the keywords are found in it, I add it to my list of neglected docs (remaining_docs_with_keywords).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":98,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_120":{"__typename":"Paragraph","id":"f02f576c9bc6_120","name":"f9ac","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 4: Cluster Neglected Documents","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_121":{"__typename":"Paragraph","id":"f02f576c9bc6_121","name":"1b26","type":"P","href":null,"layout":null,"metadata":null,"text":"This is an important step. We might be tempted to ask the LLM to reference each of these 69 neglected documents and write a hypothetical document for each of them based on the original user query.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_122":{"__typename":"Paragraph","id":"f02f576c9bc6_122","name":"6229","type":"P","href":null,"layout":null,"metadata":null,"text":"However, this is computationally expensive and not the most efficient way to discover the main relevance patterns that exist across these documents.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_123":{"__typename":"Paragraph","id":"f02f576c9bc6_123","name":"af09","type":"P","href":null,"layout":null,"metadata":null,"text":"Instead, using the embeddings, we will cluster the 69 documents. This allows us to discover the key relevant patterns that exist across the datasets. In the output below, we see that the 69 documents were subsequently clustered into 6 different groups.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_124":{"__typename":"Paragraph","id":"f02f576c9bc6_124","name":"6014","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003E\u003E\u003E Clustering neglected Docs...\n\u003E\u003E\u003E ...6 Clusters identifiedTechnical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"plaintext"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_125":{"__typename":"Paragraph","id":"f02f576c9bc6_125","name":"6be7","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_126":{"__typename":"Paragraph","id":"f02f576c9bc6_126","name":"03b7","type":"P","href":null,"layout":null,"metadata":null,"text":"How do we choose the number of clusters? Here I use HDBSCAN, which is a semi-supervised hierarchical clustering version of DBSCAN. This is how HDBSCAN is described in the docs:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":52,"end":59,"href":"https:\u002F\u002Flink.springer.com\u002Fchapter\u002F10.1007\u002F978-3-642-37456-2_14","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_127":{"__typename":"Paragraph","id":"f02f576c9bc6_127","name":"e813","type":"BQ","href":null,"layout":null,"metadata":null,"text":"The algorithm starts off much the same as DBSCAN: we transform the space according to density, exactly as DBSCAN does, and perform single linkage clustering on the transformed space. Instead of taking an epsilon value as a cut level for the dendrogram however, a different approach is taken: the dendrogram is condensed by viewing splits that result in a small number of points splitting off as points ‘falling out of a cluster’. This results in a smaller tree with fewer clusters that ‘lose points’. That tree can then be used to select the most stable or persistent clusters.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_128":{"__typename":"Paragraph","id":"f02f576c9bc6_128","name":"366f","type":"P","href":null,"layout":null,"metadata":null,"text":"Intuitively, min_samples is a parameter that controls the minimum number of neighbours to a core point. The greater min_samples is, more points will be labelled as noise from the clustering. For now, I will keep it as 1 and leave the experimentation to later, but in general \"HDBSCAN is not that sensitive to it and we can choose some sensible defaults, but this remains the biggest weakness of the algorithm.\" min_cluster_size is used to identify points ‘falling out of a cluster’ or splitting to form two new clusters.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":13,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":116,"end":127,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":411,"end":427,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":275,"end":410,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_129":{"__typename":"Paragraph","id":"f02f576c9bc6_129","name":"cd40","type":"P","href":null,"layout":null,"metadata":null,"text":"After labelling the documents, we drop those labelled as -1 (no clusters), and then create a dictionary with keys being the labelled group number, and the values being the page contents for each document.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_130":{"__typename":"Paragraph","id":"f02f576c9bc6_130","name":"2a3a","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def cluster_docs(\n    self, \n    remaining_docs_with_keywords: List[Document], \n    hypo_params: dict\n    ) -\u003E Dict[int, List[str]]:\n    \n    from hdbscan import HDBSCAN\n    \n    if hypo_params['verbose']:\n        print(f\"\\n\u003E\u003E\u003E Clustering neglected Docs...\")\n    \n    embeddings = self.embed_documents([\n        r[0].page_content \n        for r in remaining_docs_with_keywords], \n        {'verbose': False})\n    hdb = HDBSCAN(min_samples=1, min_cluster_size=3).fit(embeddings)\n    remaining_docs_with_cat = filter(lambda x: x[1] != -1, zip([r[0].page_content for r in remaining_docs_with_keywords], hdb.labels_))\n    \n    cat_dict = {}\n\n    for page_content, cat in remaining_docs_with_cat:\n        if cat not in cat_dict:\n            cat_dict[cat] = [page_content]\n        else:\n            cat_dict[cat].append(page_content)\n            \n    if hypo_params['verbose']:\n        print(f\"\u003E\u003E\u003E ...{len(cat_dict)} Clusters identified\\n\")\n            \n    return cat_dict","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_131":{"__typename":"Paragraph","id":"f02f576c9bc6_131","name":"b1f8","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 5: Generate Hypothetical Documents","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_132":{"__typename":"Paragraph","id":"f02f576c9bc6_132","name":"6ac3","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, for each of our 6 clusters that represent groups of relevance patterns, we will ask our LLM to reference the documents belonging to the cluster to create a new hypothetical document that is similar to those documents. This is an important step that differentiates the AutoHyDE approach from existing HyDE approaches. In AutoHyDE, the relevance patterns are automatically learned and then used to generate the hypothetical documents. No customized prompt writing is required at all.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_133":{"__typename":"Paragraph","id":"f02f576c9bc6_133","name":"2cea","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s an example for the first cluster in the code below. You can see that the LLM referenced every neglected in the first neglected document cluster, and created a hypothetical document based on the original query. This is done for all the clusters. A quick look at the text below, and you can see that it really does emulate the convluted style of Mill quite well, and you did not have to write a custom prompt to try and capture this style. All it took was few-shot prompting under the hood.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*wyhHIToAespRTk63DHagjA.png":{"__typename":"ImageMetadata","id":"1*wyhHIToAespRTk63DHagjA.png","originalHeight":874,"originalWidth":1068,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_134":{"__typename":"Paragraph","id":"f02f576c9bc6_134","name":"7a8d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*wyhHIToAespRTk63DHagjA.png"},"text":"Image by Author: Hypothetical Document generated from AutoHyDE","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_135":{"__typename":"Paragraph","id":"f02f576c9bc6_135","name":"d2cf","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_136":{"__typename":"Paragraph","id":"f02f576c9bc6_136","name":"b85b","type":"P","href":null,"layout":null,"metadata":null,"text":"What the function below does is to generate hypothetical documents for each identified cluster of documents. Similar to before, we use the JsonOutputParser to ensure that we get the desired output. I found that a few-shot approach generally works as you see in HYPOTHETICAL_DOCUMENT_PROMPT below:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":261,"end":289,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_137":{"__typename":"Paragraph","id":"f02f576c9bc6_137","name":"364f","type":"PRE","href":null,"layout":null,"metadata":null,"text":"@retry(tries=5)\ndef generate_hypo_docs(\n    self, \n    text: str, \n    cat_dict: Dict[int, List[str]], \n    hypo_params: dict\n    ) -\u003E List[str]:\n    \n    hypo_docs = list()\n    \n    if hypo_params['verbose']:\n        print(f\"\\n\u003E\u003E\u003E Generating Hypothetical Documents for each Doc Cluster...\\n\")\n\n    HYPOTHETICAL_DOCUMENT_PROMPT = \"\"\"\n    Your instruction is to generate a single hypothetical document from an input.\n    - This hypothetical document must be similar in style, tone and voice as examples you are provided with.\n    - This hypothetical document must appear like it was written by the same author as the examples you are provided with.\n    - This hypothetical document must also be similar in length with the examples you are provided with.\n\n    {format_instructions}\n\n    ### EXAMPLES ###\n    Below are some examples of hypothetical documents, all written by the same author, in pairs of \u003CInput\u003E and \u003CHypothetical Document\u003E:\n\n    {ref_documents}\n\n    ### INSTRUCTION ###\n    Now generate a new hypothetical document. \n\n    \u003CInput\u003E\n    {input}\n    \u003CHypothetical Document\u003E\n\n    \"\"\"\n\n    class HypotheticalDocumentSchema(BaseModel):\n        hypotheticalDocument: str = Field(description=\"a hypothetical document given an input word, phrase or question\")\n\n    parser = JsonOutputParser(pydantic_object=HypotheticalDocumentSchema)\n\n    prompt = ChatPromptTemplate.from_template(\n        template=HYPOTHETICAL_DOCUMENT_PROMPT,\n        intput_variables = [\"input\", \"ref_documents\"],\n        partial_variables = {\n            'format_instructions': parser.get_format_instructions()\n        }\n    )\n\n    hypothetical_document_chain = (\n        {'input': RunnablePassthrough(), 'ref_documents': RunnablePassthrough()}\n        | prompt\n        | self.llm_chain\n        | parser\n    )\n\n    cat_ii = 1\n    for cat in cat_dict.keys():\n\n        ref_doc_string = \"\"\n        doc_ii = 1\n        for doc in cat_dict[cat]:\n            ref_doc_string += f\"\\n\\n\u003CInput\u003E\"\n            ref_doc_string += text\n            ref_doc_string += f\"\\n\\n\u003CHypothetical Document\u003E\\n\"\n            ref_doc_string += f'{{\"hypotheticalDocument\": \"{doc}\"}}'\n            doc_ii += 1\n\n        hypo_doc = hypothetical_document_chain.invoke(\n            {'input': text, 'ref_documents': ref_doc_string}\n        )['hypotheticalDocument']\n\n        if hypo_params['verbose']:\n            print(f\"\\n### Hypo Doc {cat_ii} ###\")\n            print(hypo_doc+'\\n')\n        \n        hypo_docs.append(hypo_doc)\n        \n        cat_ii += 1\n        \n    return hypo_docs","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_138":{"__typename":"Paragraph","id":"f02f576c9bc6_138","name":"7037","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 6+7: Embed & Combine","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_139":{"__typename":"Paragraph","id":"f02f576c9bc6_139","name":"561f","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, with the list of hypothetical documents, one for each cluster, we embed the hypothetical documents (step 6) and combine them into a single embedding vector (step 7).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_140":{"__typename":"Paragraph","id":"f02f576c9bc6_140","name":"929e","type":"P","href":null,"layout":null,"metadata":null,"text":"This step is similar to the original implementation, where hypothetical documents are embedded and then averaged to get a final vector.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_141":{"__typename":"Paragraph","id":"f02f576c9bc6_141","name":"bc1a","type":"P","href":null,"layout":null,"metadata":null,"text":"⚠️ As I mentioned earlier, I don’t think this step really makes sense to capture the broader heterogeneity of documents, especially when we’re trying to discover and generate documents with different relevance patterns. But to keep it simpler and maintain compatibility with the existing LangChain implementation (the point being that you can use this immediately with other component in LangChain RAG), we’ll keep this for now to output a single embedding vector, and perhaps deal with the aggregation limitation later. If you’re convinced that simple averaging is not a good idea, you can always just stop at Step 6.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_142":{"__typename":"Paragraph","id":"f02f576c9bc6_142","name":"75d7","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003E\u003E\u003E Generating embeddings for hypothetical documents...\n\u003E\u003E\u003E Combining embeddings for hypothetical documents...","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"plaintext"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_143":{"__typename":"Paragraph","id":"f02f576c9bc6_143","name":"4eb6","type":"PRE","href":null,"layout":null,"metadata":null,"text":"hyde_embedding[:10]\n\u003E\u003E\u003E [\n   0.015876703333653572,\n  -0.013635452586265312,\n   0.021941788843565697,\n  -0.02570370381768275,\n  -0.015861831315729033,\n  -0.0003427757204382006,\n   0.0027591148428962454,\n  -0.00883308151544041,\n  -0.014893267477206646,\n  -0.020748802766928837\n]","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_144":{"__typename":"Paragraph","id":"f02f576c9bc6_144","name":"0b10","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_145":{"__typename":"Paragraph","id":"f02f576c9bc6_145","name":"05ef","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def embed_documents(self, texts: List[str], hypo_params) -\u003E List[List[float]]:\n    \"\"\"Call the base embeddings.\"\"\"\n    if hypo_params['verbose']:\n        print(\"\\n\u003E\u003E\u003E Generating embeddings for hypothetical documents...\\n\")\n    return self.base_embeddings.embed_documents(texts)\n\ndef combine_embeddings(self, embeddings: List[List[float]], hypo_params) -\u003E List[float]:\n    \"\"\"Combine embeddings into final embeddings.\"\"\"\n    if hypo_params['verbose']:\n        print(\"\\n\u003E\u003E\u003E Combining embeddings for hypothetical documents...\\n\")\n    return list(np.array(embeddings).mean(axis=0))","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_146":{"__typename":"Paragraph","id":"f02f576c9bc6_146","name":"06df","type":"P","href":null,"layout":null,"metadata":null,"text":"As before, the repo where I implemented this enhanced version of HyDE can be found here at this repo.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":96,"end":100,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_147":{"__typename":"Paragraph","id":"f02f576c9bc6_147","name":"c6a3","type":"H3","href":null,"layout":null,"metadata":null,"text":"Section 4: Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_148":{"__typename":"Paragraph","id":"f02f576c9bc6_148","name":"ecb9","type":"P","href":null,"layout":null,"metadata":null,"text":"And that’s the end of the AutoHyDE demo!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":26,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_149":{"__typename":"Paragraph","id":"f02f576c9bc6_149","name":"00f7","type":"P","href":null,"layout":null,"metadata":null,"text":"To recap, the main improvement being made in AutoHyde is the way the hypothetical documents are being generated. Instead of using a fixed prompt to generate these documents (whether it be pre-defined in LangChain or customised by the user), I have devised a framework to automatically discover the underlying relevance patterns across all the documents that may be neglected by a baseline retrieval, and to generate hypothetical documents for each of these patterns.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":365,"end":375,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_150":{"__typename":"Paragraph","id":"f02f576c9bc6_150","name":"fd00","type":"P","href":null,"layout":null,"metadata":null,"text":"In this manner, we are able to adapt HyDE for a wider variety of tasks and contexts, as well as to accommodate retrieval over an index which is heterogeneous in relevance pattern.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_151":{"__typename":"Paragraph","id":"f02f576c9bc6_151","name":"64f4","type":"P","href":null,"layout":null,"metadata":null,"text":"If you’ve made it to the end, I’m always happy to hear your thoughts on this approach, and feel free to make suggestions to my implementation as well!","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_152":{"__typename":"Paragraph","id":"f02f576c9bc6_152","name":"9639","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Give a 👍🏻, leave a 💬 and 🤝🏻 with me on LinkedIn!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":44,"end":52,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fianhojy\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_153":{"__typename":"Paragraph","id":"f02f576c9bc6_153","name":"bec6","type":"H4","href":null,"layout":null,"metadata":null,"text":"References","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_154":{"__typename":"Paragraph","id":"f02f576c9bc6_154","name":"885c","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Unsupervised Dense Information Retrieval with Contrastive Learning (https:\u002F\u002Farxiv.org\u002Fpdf\u002F2112.09118.pdf)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":68,"end":104,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2112.09118.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_155":{"__typename":"Paragraph","id":"f02f576c9bc6_155","name":"18b5","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Precise Zero-Shot Dense Retrieval without Relevance Labels (https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":60,"end":96,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_156":{"__typename":"Paragraph","id":"f02f576c9bc6_156","name":"a917","type":"ULI","href":null,"layout":null,"metadata":null,"text":"LangChain HyDE Repo (https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":21,"end":111,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde\u002Fbase.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_157":{"__typename":"Paragraph","id":"f02f576c9bc6_157","name":"2a45","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Original HyDE Implementation (https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":30,"end":125,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:2ae680787402":{"__typename":"CollectionViewerEdge","id":"collectionId:7f60cf5620c9-viewerId:2ae680787402","isEditor":false,"isMuting":false},"Membership:34d3b27be11e":{"__typename":"Membership","tier":"MEMBER","id":"34d3b27be11e"},"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png":{"__typename":"ImageMetadata","id":"1*cFFKn8rFH4ZndmaYeAs6iQ.png","originalWidth":2381,"originalHeight":743},"Tag:llm":{"__typename":"Tag","id":"llm","displayTitle":"Llm","normalizedTagSlug":"llm"},"Tag:ai":{"__typename":"Tag","id":"ai","displayTitle":"AI","normalizedTagSlug":"ai"},"Tag:langchain":{"__typename":"Tag","id":"langchain","displayTitle":"Langchain","normalizedTagSlug":"langchain"},"Tag:data-science":{"__typename":"Tag","id":"data-science","displayTitle":"Data Science","normalizedTagSlug":"data-science"},"Tag:hands-on-tutorials":{"__typename":"Tag","id":"hands-on-tutorials","displayTitle":"Hands On Tutorials","normalizedTagSlug":"hands-on-tutorials"},"Post:619e58cdbd8e":{"__typename":"Post","id":"619e58cdbd8e","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"8add","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"fed5","startIndex":17,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"d447","startIndex":59,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"35ec","startIndex":84,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"52bd","startIndex":147,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"6cf4","startIndex":153,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:f02f576c9bc6_0"},{"__ref":"Paragraph:f02f576c9bc6_1"},{"__ref":"Paragraph:f02f576c9bc6_2"},{"__ref":"Paragraph:f02f576c9bc6_3"},{"__ref":"Paragraph:f02f576c9bc6_4"},{"__ref":"Paragraph:f02f576c9bc6_5"},{"__ref":"Paragraph:f02f576c9bc6_6"},{"__ref":"Paragraph:f02f576c9bc6_7"},{"__ref":"Paragraph:f02f576c9bc6_8"},{"__ref":"Paragraph:f02f576c9bc6_9"},{"__ref":"Paragraph:f02f576c9bc6_10"},{"__ref":"Paragraph:f02f576c9bc6_11"},{"__ref":"Paragraph:f02f576c9bc6_12"},{"__ref":"Paragraph:f02f576c9bc6_13"},{"__ref":"Paragraph:f02f576c9bc6_14"},{"__ref":"Paragraph:f02f576c9bc6_15"},{"__ref":"Paragraph:f02f576c9bc6_16"},{"__ref":"Paragraph:f02f576c9bc6_17"},{"__ref":"Paragraph:f02f576c9bc6_18"},{"__ref":"Paragraph:f02f576c9bc6_19"},{"__ref":"Paragraph:f02f576c9bc6_20"},{"__ref":"Paragraph:f02f576c9bc6_21"},{"__ref":"Paragraph:f02f576c9bc6_22"},{"__ref":"Paragraph:f02f576c9bc6_23"},{"__ref":"Paragraph:f02f576c9bc6_24"},{"__ref":"Paragraph:f02f576c9bc6_25"},{"__ref":"Paragraph:f02f576c9bc6_26"},{"__ref":"Paragraph:f02f576c9bc6_27"},{"__ref":"Paragraph:f02f576c9bc6_28"},{"__ref":"Paragraph:f02f576c9bc6_29"},{"__ref":"Paragraph:f02f576c9bc6_30"},{"__ref":"Paragraph:f02f576c9bc6_31"},{"__ref":"Paragraph:f02f576c9bc6_32"},{"__ref":"Paragraph:f02f576c9bc6_33"},{"__ref":"Paragraph:f02f576c9bc6_34"},{"__ref":"Paragraph:f02f576c9bc6_35"},{"__ref":"Paragraph:f02f576c9bc6_36"},{"__ref":"Paragraph:f02f576c9bc6_37"},{"__ref":"Paragraph:f02f576c9bc6_38"},{"__ref":"Paragraph:f02f576c9bc6_39"},{"__ref":"Paragraph:f02f576c9bc6_40"},{"__ref":"Paragraph:f02f576c9bc6_41"},{"__ref":"Paragraph:f02f576c9bc6_42"},{"__ref":"Paragraph:f02f576c9bc6_43"},{"__ref":"Paragraph:f02f576c9bc6_44"},{"__ref":"Paragraph:f02f576c9bc6_45"},{"__ref":"Paragraph:f02f576c9bc6_46"},{"__ref":"Paragraph:f02f576c9bc6_47"},{"__ref":"Paragraph:f02f576c9bc6_48"},{"__ref":"Paragraph:f02f576c9bc6_49"},{"__ref":"Paragraph:f02f576c9bc6_50"},{"__ref":"Paragraph:f02f576c9bc6_51"},{"__ref":"Paragraph:f02f576c9bc6_52"},{"__ref":"Paragraph:f02f576c9bc6_53"},{"__ref":"Paragraph:f02f576c9bc6_54"},{"__ref":"Paragraph:f02f576c9bc6_55"},{"__ref":"Paragraph:f02f576c9bc6_56"},{"__ref":"Paragraph:f02f576c9bc6_57"},{"__ref":"Paragraph:f02f576c9bc6_58"},{"__ref":"Paragraph:f02f576c9bc6_59"},{"__ref":"Paragraph:f02f576c9bc6_60"},{"__ref":"Paragraph:f02f576c9bc6_61"},{"__ref":"Paragraph:f02f576c9bc6_62"},{"__ref":"Paragraph:f02f576c9bc6_63"},{"__ref":"Paragraph:f02f576c9bc6_64"},{"__ref":"Paragraph:f02f576c9bc6_65"},{"__ref":"Paragraph:f02f576c9bc6_66"},{"__ref":"Paragraph:f02f576c9bc6_67"},{"__ref":"Paragraph:f02f576c9bc6_68"},{"__ref":"Paragraph:f02f576c9bc6_69"},{"__ref":"Paragraph:f02f576c9bc6_70"},{"__ref":"Paragraph:f02f576c9bc6_71"},{"__ref":"Paragraph:f02f576c9bc6_72"},{"__ref":"Paragraph:f02f576c9bc6_73"},{"__ref":"Paragraph:f02f576c9bc6_74"},{"__ref":"Paragraph:f02f576c9bc6_75"},{"__ref":"Paragraph:f02f576c9bc6_76"},{"__ref":"Paragraph:f02f576c9bc6_77"},{"__ref":"Paragraph:f02f576c9bc6_78"},{"__ref":"Paragraph:f02f576c9bc6_79"},{"__ref":"Paragraph:f02f576c9bc6_80"},{"__ref":"Paragraph:f02f576c9bc6_81"},{"__ref":"Paragraph:f02f576c9bc6_82"},{"__ref":"Paragraph:f02f576c9bc6_83"},{"__ref":"Paragraph:f02f576c9bc6_84"},{"__ref":"Paragraph:f02f576c9bc6_85"},{"__ref":"Paragraph:f02f576c9bc6_86"},{"__ref":"Paragraph:f02f576c9bc6_87"},{"__ref":"Paragraph:f02f576c9bc6_88"},{"__ref":"Paragraph:f02f576c9bc6_89"},{"__ref":"Paragraph:f02f576c9bc6_90"},{"__ref":"Paragraph:f02f576c9bc6_91"},{"__ref":"Paragraph:f02f576c9bc6_92"},{"__ref":"Paragraph:f02f576c9bc6_93"},{"__ref":"Paragraph:f02f576c9bc6_94"},{"__ref":"Paragraph:f02f576c9bc6_95"},{"__ref":"Paragraph:f02f576c9bc6_96"},{"__ref":"Paragraph:f02f576c9bc6_97"},{"__ref":"Paragraph:f02f576c9bc6_98"},{"__ref":"Paragraph:f02f576c9bc6_99"},{"__ref":"Paragraph:f02f576c9bc6_100"},{"__ref":"Paragraph:f02f576c9bc6_101"},{"__ref":"Paragraph:f02f576c9bc6_102"},{"__ref":"Paragraph:f02f576c9bc6_103"},{"__ref":"Paragraph:f02f576c9bc6_104"},{"__ref":"Paragraph:f02f576c9bc6_105"},{"__ref":"Paragraph:f02f576c9bc6_106"},{"__ref":"Paragraph:f02f576c9bc6_107"},{"__ref":"Paragraph:f02f576c9bc6_108"},{"__ref":"Paragraph:f02f576c9bc6_109"},{"__ref":"Paragraph:f02f576c9bc6_110"},{"__ref":"Paragraph:f02f576c9bc6_111"},{"__ref":"Paragraph:f02f576c9bc6_112"},{"__ref":"Paragraph:f02f576c9bc6_113"},{"__ref":"Paragraph:f02f576c9bc6_114"},{"__ref":"Paragraph:f02f576c9bc6_115"},{"__ref":"Paragraph:f02f576c9bc6_116"},{"__ref":"Paragraph:f02f576c9bc6_117"},{"__ref":"Paragraph:f02f576c9bc6_118"},{"__ref":"Paragraph:f02f576c9bc6_119"},{"__ref":"Paragraph:f02f576c9bc6_120"},{"__ref":"Paragraph:f02f576c9bc6_121"},{"__ref":"Paragraph:f02f576c9bc6_122"},{"__ref":"Paragraph:f02f576c9bc6_123"},{"__ref":"Paragraph:f02f576c9bc6_124"},{"__ref":"Paragraph:f02f576c9bc6_125"},{"__ref":"Paragraph:f02f576c9bc6_126"},{"__ref":"Paragraph:f02f576c9bc6_127"},{"__ref":"Paragraph:f02f576c9bc6_128"},{"__ref":"Paragraph:f02f576c9bc6_129"},{"__ref":"Paragraph:f02f576c9bc6_130"},{"__ref":"Paragraph:f02f576c9bc6_131"},{"__ref":"Paragraph:f02f576c9bc6_132"},{"__ref":"Paragraph:f02f576c9bc6_133"},{"__ref":"Paragraph:f02f576c9bc6_134"},{"__ref":"Paragraph:f02f576c9bc6_135"},{"__ref":"Paragraph:f02f576c9bc6_136"},{"__ref":"Paragraph:f02f576c9bc6_137"},{"__ref":"Paragraph:f02f576c9bc6_138"},{"__ref":"Paragraph:f02f576c9bc6_139"},{"__ref":"Paragraph:f02f576c9bc6_140"},{"__ref":"Paragraph:f02f576c9bc6_141"},{"__ref":"Paragraph:f02f576c9bc6_142"},{"__ref":"Paragraph:f02f576c9bc6_143"},{"__ref":"Paragraph:f02f576c9bc6_144"},{"__ref":"Paragraph:f02f576c9bc6_145"},{"__ref":"Paragraph:f02f576c9bc6_146"},{"__ref":"Paragraph:f02f576c9bc6_147"},{"__ref":"Paragraph:f02f576c9bc6_148"},{"__ref":"Paragraph:f02f576c9bc6_149"},{"__ref":"Paragraph:f02f576c9bc6_150"},{"__ref":"Paragraph:f02f576c9bc6_151"},{"__ref":"Paragraph:f02f576c9bc6_152"},{"__ref":"Paragraph:f02f576c9bc6_153"},{"__ref":"Paragraph:f02f576c9bc6_154"},{"__ref":"Paragraph:f02f576c9bc6_155"},{"__ref":"Paragraph:f02f576c9bc6_156"},{"__ref":"Paragraph:f02f576c9bc6_157"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:ca061af8c7b7"},"inResponseToEntityType":null,"isLocked":true,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_UGC","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fautohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e","primaryTopic":{"__ref":"Topic:1eca0103fff3"},"topics":[{"__typename":"Topic","slug":"artificial-intelligence"},{"__typename":"Topic","slug":"machine-learning"},{"__typename":"Topic","slug":"data-science"},{"__typename":"Topic","slug":"programming"}],"isPublished":true,"latestPublishedVersion":"f02f576c9bc6","visibility":"LOCKED","postResponses":{"__typename":"PostResponses","count":6},"createdAt":1711300283807,"firstPublishedAt":1712250360920,"latestPublishedAt":1712250360920,"clapCount":529,"allowResponses":true,"isLimitedState":false,"title":"AutoHyDE: Making HyDE Better for Advanced LLM RAG","isSeries":false,"sequence":null,"uniqueSlug":"autohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e","socialTitle":"","socialDek":"","noIndex":null,"canonicalUrl":"","metaDescription":"","readingTime":18.423899371069183,"previewContent":{"__typename":"PreviewContent","subtitle":"Introducing AutoHyDE, a framework for improving the effectiveness, coverage and adaptability of HyDE for Advanced LLM RAG Applications"},"previewImage":{"__ref":"ImageMetadata:1*BoEb2eI8BFOectBr5y0zjA.png"},"isShortform":false,"seoTitle":"","updatedAt":1713750504768,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:llm"},{"__ref":"Tag:ai"},{"__ref":"Tag:langchain"},{"__ref":"Tag:data-science"},{"__ref":"Tag:hands-on-tutorials"}],"isNewsletter":false,"statusForCollection":"APPROVED","pendingCollection":null,"detectedLanguage":"en","wordCount":4582,"layerCake":1},"CreditCard:{}":{"__typename":"CreditCard","expirationMonth":5,"expirationYear":2028},"MembershipStatus:{}":{"__typename":"MembershipStatus","isCancelled":false,"paymentProvider":"BRAINTREE","currentPeriodEndsAt":1716163200000,"hasRecurringPaymentFailedMoreThanOnce":false,"hasCreditCardExpired":false,"paymentFailedAt":null,"paymentMethod":{"__ref":"CreditCard:{}"}}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.dc42215b.js"></script><script src="https://cdn-client.medium.com/lite/static/js/3057.5e22bbb0.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.bedd0e2c.js"></script><script src="https://cdn-client.medium.com/lite/static/js/instrumentation.5e7f2981.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/reporting.2021fe63.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/6068.e9093f2e.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/4398.db4d4378.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/7883.0e445e04.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/9281.e9be8bce.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/7111.1421aaa2.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/6481.e3e8b67f.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/8695.17d1af21.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/6358.a78f5809.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/4341.e697d2a1.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/5971.fd9e1c6f.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/5203.e7a22052.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/5514.6018be2b.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/7098.7bbb418a.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/5700.24361217.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/8491.7fa46461.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/8597.d6e15950.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/1711.b70f1a35.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/9174.244cf6ba.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/8883.c8b03d13.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/705.da9267d6.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/5781.39279ff8.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/8580.feeb2549.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/6046.f9be485b.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/6605.84e81b15.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/500.596a9584.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/9408.9f41b422.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/6637.8a411a8c.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/8565.ce53d42b.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/4667.993ebc2b.chunk.js"></script>
    <script src="https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.12f09e15.chunk.js"></script><script>window.main();</script><script defer src="https://static.cloudflareinsights.com/beacon.min.js/v55bfa2fee65d44688e90c00735ed189a1713218998793" integrity="sha512-FIKRFRxgD20moAo96hkZQy/5QojZDAbyx0mQ17jEGHCJc/vi0G2HXLtofwD7Q3NmivvP9at5EVgbRqOaOQb+Rg==" data-cf-beacon='{"rayId":"87beaa564b44937f","version":"2024.4.1","token":"0b5f665943484354a59c39c6833f7078"}' crossorigin="anonymous"></script>
    </body></html>