<body><div><div><div></div><script>document.domain = document.domain;</script><div><div><div><a>Open in app<svg><path></path></svg></a></div><div><div><a><svg><g><g><path></path></g></g></svg></a><div><div><div></div><div><svg><path></path></svg></div><input/></div></div></div><div><div><a><div><svg><path></path><path></path></svg><div>Write</div></div></a></div></div><div><div><a><div><svg><path></path></svg></div></a></div></div><div><a><div><svg><path></path><path></path></svg></div></a></div><div><button><div><div><img/><div></div></div><svg><mask><path></path><path></path></mask><path></path><path></path></svg></div></button></div></div></div><div><div><div><div></div></div><article><div><div><span></span><section><div><div></div><div><div><div></div><div><div><div><div><div><button><div><div><div><svg><path></path></svg></div></div></div><div><svg><path></path></svg><p>Member-only story</p></div></button></div></div></div></div></div></div></div><div><div><div><div><h1>AutoHyDE: Making HyDE Better for Advanced LLM RAG</h1></div><div><h2>üîé A deep-dive into HyDE for Advanced LLM RAG + üí° Introducing AutoHyDE, a semi-supervised framework to improve the effectiveness, coverage and applicability of HyDE</h2><div><div><div><div><div><div><a><div><div><div><div><img/><div></div></div></div></div></div></a><a><div><div><div><div><div><img/><div></div></div></div></div></div></div></a></div></div><div><div><div><span><div><div><div><div><div><p><a>Ian Ho</a></p></div></div></div><span><span>¬∑</span></span><p><button>Follow</button></p></div></div></span></div></div><div><span><div><div><div><span>Published in</span><div><div><a><p>Towards Data Science</p></a></div></div></div><div><span><span>¬∑</span></span></div></div><span><div><span>19 min read</span><div><span><span>¬∑</span></span></div><span>Apr 4, 2024</span></div></span></div></span></div></div></div><div><div><div><div><div><div><div><svg><path></path><path></path></svg></div></div></div><div><p><span>--</span></p></div></div></div><div><div><button><svg><path></path></svg><p><span>6</span></p></button></div></div></div><div><div></div><div><div><div><button><svg><path></path></svg></button></div></div></div><div><div><div><div><div><div><div><div><button><svg><path></path></svg><div><p>Listen</p></div></button></div></div></div></div></div></div></div></div><div><div><div><button><svg><path></path></svg><div><p>Share</p></div></button></div></div></div><div><div><div><button><svg><path></path></svg><div><p>More</p></div></button></div></div></div></div></div></div></div></div></div><figure><div><div><picture><source/><source/><img/></picture></div></div><figcaption>Image by Author with the help of DALL-E</figcaption></figure><h1><strong>Introduction</strong></h1><p>In the field of Retrieval Augmented Generation (RAG),<strong> Hypothetical Document Embeddings </strong>(HyDE) have proven to be a powerful form of query rewriting to improve the relevance of retrieved documents.</p><p>For the uninitiated, whilst traditional retrieval simply uses the original input to create embedding vectors for retrieval, HyDE is a methodology to generate embedding vectors that are more relevant to the embedding space of the indexed documents to be retrieved.</p><p>The super high-level summary is: (1) Create hypothetical documents from user input, (2) Convert hypothetical documents to embeddings, (3) Use embeddings to retrieve similar documents</p><p>I‚Äôve been using RAG and basic HyDE in some of my work and personal projects, and after some time, I‚Äôve realized that the existing implementation of HyDE does not always work well out of the box and it is not as flexible as I hoped it would be. So after doing my research on the methodology and digging through the papers and source codes, I wanted to share my thoughts on the current approach, and to suggest an <strong>enhanced version of HyDE (I call it AutoHyDE) that has the potential to be more effective and adaptable across a variety of use-cases.</strong></p><p>In this article, <strong>Section 1</strong> will be a deep dive into the original HyDE paper and we will go through how it has been translated into the LangChain implementation. If you are already familiar with HyDE, feel free to skip ahead.</p><p>In <strong>Section 2</strong>, I will discuss what I believe are the major limitations of the current HyDE approach.</p><p>Finally, I will introduce <strong>AutoHyDE in Section 3</strong>, an attempt by me to create what I believe can be a better version of HyDE. Here is the tldr:</p><blockquote><p><strong>AutoHyDE is a framework to automatically discover underlying relevance patterns in your indexed documents and to generate hypothetical documents that directly represent these relevance patterns.</strong></p></blockquote><p>I have also directly tweaked the LangChain <code>HypotheticalDocumentEmbedder</code> class to realize AutoHyDE, so that it can still be chained with other parts of LangChain.</p><p>Check out the <a>repo</a> for more details, <a>demo</a> &amp; <a>source code</a>.</p><blockquote><p>Give a üëçüèª, leave a üí¨ and ü§ùüèª with me on <a>LinkedIn</a>!</p></blockquote><figure><div><div><picture><source/><source/><img/></picture></div></div><figcaption>Image by Author: Code Snippet for AutoHyDE</figcaption></figure><p><em>For a higher-level overview of RAG and Query Rewriting, this is a good </em><a><em>article</em></a><em> written by Florian June. This </em><a><em>repo</em></a><em> also contains some good practical implementations of RAG, HyDE and other enhancements.</em></p></div></div></div><div><span></span><span></span><span></span></div><div><div><div><h1><strong>Section 1: What is HyDE?</strong></h1><h2>HyDE: The Original Paper</h2><p>Hypothetical Document Embeddings (HyDE) were first introduced in the 2022 paper by Gao et al. titled <a>Precise Zero-Shot Dense Retrieval without Relevance Labels</a>.</p><p>The paper set out to find a way to improve zero-shot dense retrieval (i.e. using semantic embedding similarities). To that end, they came up with a two-step methodology called HyDE.</p><p><strong>Step 1 </strong>involved instruction prompting a language model (in the paper they use GPT-3) to generate a hypothetical document given the original query (specifically limited to a question in the paper).</p><p><strong>Step 2</strong> involved using a <em>Contriever</em>, or an <em>‚Äúunsupervised contrastive encoder‚Äù,</em> to turn this hypothetical document into an embedding vector, which is then used for downstream similarity search and retrieval.</p><figure><div><div><picture><source/><source/><img/></picture></div></div><figcaption>Illustration of HyDE methodology from the <a>original paper</a></figcaption></figure><h2><strong>[<em>Optional</em>] An aside on the <em>Contriever</em></strong></h2><p>The <em>Contriever</em> used in the original HyDE paper was derived from an earlier paper in August 2022 by Izacard et al. called <a>Unsupervised Dense Information Retrieval with Contrastive Learning</a>. In this paper, the authors claim that while neural networks had emerged as good alternatives to term-frequency methods for retrieval, they required large amounts of data and did not always transfer well to new areas of application. Therefore, they devised a way to train the embedding network in an unsupervised manner through<strong> contrastive learning</strong>.</p><p>For those interested in Contrastive Learning, you can check out Section 3 of the paper for information, but here‚Äôs a high-level summary:</p><p>First, they define a loss function (contrastive InfoNCE loss) that rewards positive pairs of documents and penalizes negative pairs. Next, they build positive and negative pairs of document representations. Positive pairs are constructed using a data augmentation method called the Inverse Cloze Task (ICT), and through Independent Cropping. Negative pairs are constructed using in-batch negative sampling, and across-batch negative sampling (also called MoCo). Finally, they train this over the BERT base uncased architecture.</p><p>The results of the paper showed that this method was able to match unsupervised term-frequency methods like BM25, and also performed well on cross-lingual retrieval, an option which is not possible for term matching methods.</p><p>For the purposes of this article, I will not be going further into the details of training and data augmentation, but do check out the paper if you are interested. The contriever can also be found on HuggingFace at this <a>link</a>, and the research repo can be found <a>here</a>.</p><p>Anyway, It is this <em>Contriever</em> that the HyDE paper uses to embed generated documents. They also explore <em>ContrieverFT</em>, which is in-domain fine-tuned in a supervised manner. Now, back to the main focus of the article, which is the HyDE Paper.</p><h2><strong>Back to the HyDE Paper</strong></h2><p>Previously, I provided an overview of the HyDE methodology and the origins of the underlying <em>contriever. </em>Now, let‚Äôs take a closer look at what‚Äôs going on in HyDE by reviewing some key excerpts from the paper.</p><blockquote><p>‚Ä¶In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder‚Äôs dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings.</p></blockquote><p>More formally, the hypothetical document embedding vector is defined as:</p><figure><div><div><picture><source/><source/><img/></picture></div></div><figcaption>Formula for encoding the generated document in the <a>HyDE Paper</a></figcaption></figure><ul><li>where g is the <em>InstructLM(query, INST)</em> function in the first step of hypothetical document generation; and</li><li>where f is the document encoder (<em>contriever</em>)</li></ul><p>‚ÄúFormally, g defines a probability distribution based on the chain rule‚Äù, and V is estimated through the following equation:</p><figure><div><div><picture><source/><source/><img/></picture></div></div><figcaption>Formula for estimating the embedding vector in the <a>HyDE Paper</a></figcaption></figure><p>In the original paper, you can also find the python implementation at this <a>repo</a>, where:</p><p><em>f is the generative function as we see in the repo:</em></p><figure><div><div></div></div><figcaption>generate function from <a>texttron/hyde</a></figcaption></figure><p><em>g is the encoding function as we see in the repo:</em></p><figure><div><div></div></div><figcaption>encode function from <a>texttron/hyde</a></figcaption></figure><p>Just as in the estimation equation, we see that a simple averaging is used to estimate the embedding vector across the various hypothetical documents. This mean-aggregated embedding is then used to do similarity search:</p><figure><div><div></div></div><figcaption>search function from <a>texttron/hyde</a></figcaption></figure><p>Overall, we see that HyDE is really not a complicated implementation. <strong>Generate, embed, average, retrieve</strong>.</p><p>Now, let‚Äôs take a look at popular implementations of HyDE that have surfaced recently.</p><h2><strong>LangChain Implementation</strong></h2><p>In both LangChain &amp; LlamaIndex, there are implementations of HyDE that are similar to what we‚Äôve seen in the paper and repo above. For most of us who are playing around with LLM capabilities, these are the most likely ways that we‚Äôll be looking to use HyDE. We‚Äôll also have a closer look at the prompts being used under the hood for these HyDE classes. Let‚Äôs focus on the LangChain implementation. The repo for reference is <a>here</a>.</p><figure><div><div></div></div><figcaption>from_llm constructor for HypotheticalDocumentEmbedder in <a>langchain/hyde</a></figcaption></figure><p>The first step is to create the <code>HypotheticalDocumentEmbedder</code> using <code>from_llm</code> ‚Äî you can choose one of the existing prompt templates such as the one you see below for writing a news passage, or define your own custom prompt.</p><pre><span>trec_news_template = """Please write a news passage about the topic.<br/>Topic: {TOPIC}<br/>Passage:"""</span></pre><p>Then, you can execute <code>embed_query</code> , which first generates the hypothetical document(s) based on the prompt template, and then translates these documents into embeddings and combines them. The <code>embed_documents</code> function simply uses the defined <code>base_embeddings</code> from the <code>from_llm</code> definition, while the <code>combine_embeddings</code> is a mean aggregation.</p><figure><div><div></div></div><figcaption>embed_query function for <a>langchain/hyde</a></figcaption></figure><p>You will also notice that documents is of <code>List[str]</code> type because you can generate multiple documents. All you have to do is define the LLM accordingly in the <code>from_llm</code> step. Here‚Äôs an example:</p><pre><span>multi_llm = OpenAI(n=4, best_of=4)<br/>embeddings = HypotheticalDocumentEmbedder.from_llm(<br/>    multi_llm, base_embeddings, "web_search"<br/>)<br/>result = embeddings.embed_query("Where is the Taj Mahal?")</span></pre><p>For a simple walkthrough of how to use HyDE, you can refer to the <a>notebook</a>. It‚Äôs nothing too complicated and LangChain has made it really easy to use by abstracting away most of the underlying functionality.</p></div></div></div><div><span></span><span></span><span></span></div><div><div><div><h1><strong>Section 2: Limitations of HyDE</strong></h1><p>Now that we have a good idea of how HyDE was originally designed and how it was eventually implemented in the popular LangChain &amp; LlamaIndex libraries, I want to talk about their limitations.</p><p>The first thing I noticed is that whilst the HyDE paper does experiment with different instruction LLMs, it does not spend too much time talking about further optimizing this aspect of the HyDE process. From table 4 in the paper, we see that using different models results in rather huge result variance:</p><figure><div><div><picture><source/><source/><img/></picture></div></div><figcaption>HyDE results from using different LLMs in the <a>HyDE Paper</a></figcaption></figure><p>The key implication here is not to use the biggest models with the most parameters. Instead, the fact that the underlying LLMs bring about such huge differences for the overall tasks of retrieval relevance convinces me that apart from using different LLMs, <strong>the generative task is one area worth further optimizing</strong>.</p><p>In fact, the researchers themselves do state that:</p><blockquote><p>InstructGPT model able to further bring up the performance‚Ä¶ this suggests that there may still exist certain factors not captured by the fine-tuned encoder but only by the generative model.</p></blockquote><p>With that possibility in mind, we have also seen from the LangChain deep-dive in Section 1 that the prompt templates are pre-defined to be generic in instructing hypothetical documents. For example, <em>‚ÄúPlease write a scientific paper passage to support/refute the claim‚Äù</em>. This only works when your query/input nicely fits these pre-defined templates. Otherwise, you can also provide a custom template to generate hypothetical documents, but this still has its limitations.</p><p>Firstly, the existing implementations are largely limited to a Q&amp;A framework. In the real world, not all retrieval use-cases are for answering questions, and thus asking the LLM to hypothetically answer your question does not make make sense. So in all these other situations, what do we ask the LLM to generate hypothetically then? Yes, custom prompting is available, but there is no guarantee that the prompt you write is actually relevant to the existing documents. Unless the underlying documents are very uniform in terms of relevance patterns, writing a prompt to capture these patterns would be a very manual and imperfect process.</p><p>More importantly, when you chunk and index your documents, the <strong>chunks are not always homogenous</strong> in terms of style, tone, structure etc. Consequently, it may be <strong>insufficient to use a single generalized prompt</strong> to generate hypothetical documents given your input, and it may be intractable to write multiple prompts to cater to different kinds of chunks. For instance, imagine you are trying to generate hypothetical documents for blog posts. With different authors, tones, writing styles, how do you create a generalized prompt to capture this variety? Thus, the current implementation of HyDE appears to be rather rigid and suboptimal in terms of coverage.</p><p>On a more technical level, we also note that regarding the final vector embedding, the paper says that:</p><blockquote><p>We simply consider the expectation value, assuming the distribution of v_qij is uni-modal, i.e. the query is not ambiguous.</p></blockquote><p>But if you think about it, most humans approach search problems with ambiguity.</p><p>This is an important caveat. In real use-cases, I can imagine that the distribution of v is not in fact nicely uni-modal, especially when the query is as ambiguous as just one or two keywords. What this means is that the same underlying query can be mapped to more than one mode of vector embedding representations.</p><p><strong>How might we begin to improve HyDE?</strong></p><blockquote><p>HyDE appears unsupervised. No model is trained in HyDE: both the generative model and the contrastive encoder remain intact.</p></blockquote><p>The unsupervised nature of HyDE is important because it does away with the need to have huge datasets or generous amounts of compute for any supervised variant of methodology to improve dense retrieval.</p><p>However, by remaining strictly unsupervised, we lose some benefits. Specifically, there is no way to dynamically define multiple generative functions <em>g(q, INST)</em> that are more aligned with the document embeddings.</p><p>Instead, it might be worth considering <strong>a semi-supervised approach</strong> that is more adaptive and generalizable. This approach does not require one to forcefully fit hypothetical generation of documents into a single pre-defined/custom prompt. It does not even require one to know what custom prompts to write, unlike the LangChain/LlamaIndex implementations.</p><p>Instead, an enhanced version of HyDE will automatically <strong>learn a greater variety of relevance patterns</strong> above and beyond a baseline retrieval, and generate hypothetical documents that are aligned with different clusters of indexed chunks. I call this approach <strong>AutoHyDE</strong>.</p><p>Before talking more about AutoHyDE, I have one final point to bring up regarding with the current HyDE implementation. I will not be addressing it in AutoHyDE, but I think it deserves some attention.</p><p>Recall again the averaging equation across embeddings, which manifests as the following line of code:</p><pre><span>list(np.array(embeddings).mean(axis=0))</span></pre><p>It‚Äôs a simple aggregation but does not make much sense to me, especially when you consider the multi-modal distribution of the embeddings. Let‚Äôs say you manage to construct the chained encoding function <em>f(g(q, INST))</em> that nicely captures the diversity of the relevance patterns across all your document chunks. When you take averages across these multi-modal distributed encodings, you end up with an average that may not be relevant at all (think Jensen‚Äôs inequality kind of averaging). Of course, this all works well when you have a uni-modal distribution, but this is rarely a realistic assumption, and there is no way to verify it universally.</p><p>Anyway, on to <strong>AutoHyDE</strong>.</p></div></div></div><div><span></span><span></span><span></span></div><div><div><div><h1><strong>Section 3: AutoHyDE</strong></h1><h2>Overview</h2><p>To recap, the main point of AutoHyDE is to <strong>automatically discover the variety of relevance patterns</strong> in your vector database and <strong>generate a variety of documents </strong>to improve coverage of these patterns.</p><p>Technically, I took the existing implementation of<strong> class HypotheticalDocumentEmbedder </strong>in LangChain and created new functions to achieve AutoHyDE, so that it works immediately as part of any RAG chain that you might have. This is the updated <code>embed_query</code> function that I wrote, and will explore each of the sub-methods in the following walkthrough.</p><figure><div><div><picture><source/><source/><img/></picture></div></div><figcaption>Image by Author: Src Code Snippet from AutoHyDE Repo</figcaption></figure><p>üßëüèª‚Äçüíª Check out the implementation here if you are interested to try it out:<strong> </strong><a><strong>https://github.com/ianhojy/auto-hyde/tree/main</strong></a></p><p>Here‚Äôs a quick demo of how to use it:</p><figure><div><div></div></div><figcaption>Demo code for AutoHyDE</figcaption></figure><p>To illustrate how AutoHyDE works and, I‚Äôm going to be using J. S. Mill‚Äôs <a>Utilitarianism</a>. I‚Äôm going to chunk it, index it and try to receive the relevant chunks based on a query.</p><p>I chose this text not just because I spent countless hours struggling with it as an undergraduate back in the good old days of Philosophy 101, but also because it‚Äôs a good example of a text where it‚Äôs really difficult to write a custom prompt to generate hypothetical documents, even if you hold a PhD in Moral Philosophy with endless knowledge of Mill. Let‚Äôs just say that Mill‚Äôs writing is not the most straightforward and consistent, he‚Äôs really an expert at run-on sentences (so am I).</p><p>As I walk through my implementation of AutoHyDE below, I‚Äôll have both high-level explanations and more technical descriptions for those interested in the <a>repo</a>.</p><h2><strong>Step 1: Extract Key Words from Query</strong></h2><p>Imagine you received this essay assignment:</p><blockquote><p>What is the relationship between justice and happiness?</p></blockquote><p>At this step 1, the LLM will be prompted to extract the key words, in this case:</p><pre><span>&gt;&gt;&gt; Extracting Keywords from your Query‚Ä¶<br/>&gt;&gt;&gt; ‚Ä¶Keywords Extracted: ['relationship', 'justice', 'happiness']</span></pre><p>The key words are important in this case because they will be utilized in Step 3, in order to quickly (and dirtily) identify documents that you would have left out in a normal retrieval.</p><p><strong>Technical Implementation</strong></p><p>This is the function I wrote to extract the keywords:</p><pre><span>@retry(tries=5)<br/>def extract_keywords(<br/>    self, <br/>    text: str, <br/>    hypo_params: dict<br/>    ) -&gt; List[str]:<br/><br/>    if hypo_params['verbose']:<br/>        print(f"\n&gt;&gt;&gt; Extracting Keywords from your Query...")<br/>    <br/>    KEYWORD_EXTRACTION_PROMPT = """<br/>    Your goal is to extract a list of keywords from an input phrase, sentence, or several sentences.<br/><br/>    - You can only generate 1 to 5 keywords.<br/>    - Keywords should be nouns, issues, concepts<br/>    - Keywords should not include verbs, prepositions, pronouns<br/>    - Each keyword can only be one word long.<br/>    - If the input is just a single word, return that word as the only keyword.<br/><br/>    {format_instructions}<br/><br/>    The input is:<br/>    {input}<br/>    """<br/><br/>    class KeywordListSchema(BaseModel):<br/>        keywordList: list[str] = Field(description="list of one-word keywords based on a given phrase")<br/><br/>    parser = JsonOutputParser(pydantic_object=KeywordListSchema)<br/><br/>    prompt = ChatPromptTemplate.from_template(<br/>        template=KEYWORD_EXTRACTION_PROMPT,<br/>        intput_variables = ["input"],<br/>        partial_variables = {<br/>            'format_instructions': parser.get_format_instructions()<br/>        }<br/>    )<br/><br/>    keyword_extraction_chain = (<br/>        {'input': RunnablePassthrough()}<br/>        | prompt<br/>        | self.llm_chain<br/>        | parser<br/>    )<br/>    <br/>    keywords = keyword_extraction_chain.invoke(text)['keywordList']<br/>    if hypo_params['verbose']:<br/>        print(f"&gt;&gt;&gt; ...Keywords Extracted: {keywords}\n")<br/>    <br/>    return keywords</span></pre><p>Nothing fancy here, just using the JsonOutputParser to ensure that I get the exact output which is a list of keywords.</p><h2><strong>Step 2: Do initial retrieval</strong></h2><p>Apart from extracting the keywords, we‚Äôll also use the original query to do an initial retrieval. You‚Äôll have to ask yourself, how many chunks do you want to retrieve as part of your workflow?</p><p>Assuming that you initially intended to retrieve 20 documents to use as context for answering your question in RAG, what this step 2 will then do is to retrieve more than 20 documents so that we can explore potential documents that you would have <em>neglected</em> (for lack of a better word) due to your cut off of 20. This can be toggled using <code>exploration_multiplier</code>. With a <code><em>baseline_k</em></code> of 20 and <code>exploration_multiplier</code> of 5, we will be exploring 20 x 5 = 100 top documents based on cosine similarity.</p><p><strong>Technical Implementation</strong></p><pre><span>def do_init_retrieval(<br/>    self,<br/>    db: VectorStore, <br/>    text: str, <br/>    hypo_params: dict<br/>    ) -&gt; List[Tuple[Document, float]]:<br/>    <br/>    k = hypo_params['baseline_k'] * hypo_params['exploration_multiplier']<br/>    if hypo_params['verbose']:<br/>        print(f"\n&gt;&gt;&gt; Performing Initial Retrieval of {k} documents...\n")<br/>    docs = db.similarity_search_with_score(<br/>        text, <br/>        k=k<br/>    )<br/>    return docs</span></pre><h2><strong>Step 3: Get neglected documents that contain the keywords</strong></h2><p>So above and beyond the 20 documents that you would have retrieved from a straightforward retrieval, there are 80 documents that are now up for consideration that you would have <em>neglected</em>.</p><p>Out of these 80 documents, we want to find the subset of documents that would have been directly relevant to your original query. How do we do this efficiently? One simple way would just be to select the documents that contain any of the keywords extracted from step 1. There are ways to make this better, but I think keyword matching is already a quick and rather effective way to identify the first cut of neglected documents. Maybe we‚Äôll explore better ways to do this identification, but this will do for now.</p><p>This is what you will see as output from the code:</p><pre><span>&gt;&gt;&gt; Checking 80 Docs ranked after 20 for presence of keyword‚Ä¶<br/>&gt;&gt;&gt; ‚Ä¶69 neglected Docs identified</span></pre><p>In the output above, we see that out of the 80 Documents, 69 contained one or more of the keywords [‚Äòrelationship‚Äô, ‚Äòjustice‚Äô, ‚Äòhappiness‚Äô].</p><p>Intuitively, we can understand these as potentially relevant documents that would have been neglected in a normal retrieval of the top 20 documents.</p><p><strong>Technical Implementation</strong></p><pre><span>def get_remaining_docs_with_keywords(<br/>    self, <br/>    text: str, <br/>    init_docs: List[Tuple[Document, float]], <br/>    keywords: List[str], <br/>    hypo_params: dict<br/>    ) -&gt; List[Document]:<br/><br/>    remaining_docs_with_keywords = list()<br/>    <br/>    if hypo_params['verbose']:<br/>        print(f"""\n&gt;&gt;&gt; Checking {len(init_docs[hypo_params['baseline_k']:])} <br/>                Docs ranked after {hypo_params['baseline_k']} for presence of keyword...""")<br/><br/>    for r in init_docs[hypo_params['baseline_k']:]:<br/>        page_content = r[0].page_content.lower()<br/>        for keyword in keywords:<br/>            if keyword.lower() in page_content:<br/>                remaining_docs_with_keywords.append(r)<br/>                continue<br/>                <br/>    if hypo_params['verbose']:<br/>        print(f"&gt;&gt;&gt; ...{len(remaining_docs_with_keywords)} neglected Docs identified\n")<br/>    return remaining_docs_with_keywords</span></pre><p>For each document, if any of the keywords are found in it, I add it to my list of neglected docs (<code>remaining_docs_with_keywords</code>).</p><h2><strong>Step 4: Cluster Neglected Documents</strong></h2><p>This is an important step. We might be tempted to ask the LLM to reference each of these 69 neglected documents and write a hypothetical document for each of them based on the original user query.</p><p>However, this is computationally expensive and not the most efficient way to discover the main relevance patterns that exist across these documents.</p><p>Instead, using the embeddings, we will cluster the 69 documents. This allows us to discover the key relevant patterns that exist across the datasets. In the output below, we see that the 69 documents were subsequently clustered into 6 different groups.</p><pre><span>&gt;&gt;&gt; Clustering neglected Docs...<br/>&gt;&gt;&gt; ...6 Clusters identifiedTechnical Implementation</span></pre><p><strong>Technical Implementation</strong></p><p>How do we choose the number of clusters? Here I use <a>HDBSCAN</a>, which is a semi-supervised hierarchical clustering version of DBSCAN. This is how HDBSCAN is described in the docs:</p><blockquote><p>The algorithm starts off much the same as DBSCAN: we transform the space according to density, exactly as DBSCAN does, and perform single linkage clustering on the transformed space. Instead of taking an epsilon value as a cut level for the dendrogram however, a different approach is taken: the dendrogram is condensed by viewing splits that result in a small number of points splitting off as points ‚Äòfalling out of a cluster‚Äô. This results in a smaller tree with fewer clusters that ‚Äòlose points‚Äô. That tree can then be used to select the most stable or persistent clusters.</p></blockquote><p>Intuitively, <code>min_samples</code> is a parameter that controls the minimum number of neighbours to a core point. The greater <code>min_samples</code> is, more points will be labelled as noise from the clustering. For now, I will keep it as 1 and leave the experimentation to later, but in general <em>"HDBSCAN is not that sensitive to it and we can choose some sensible defaults, but this remains the biggest weakness of the algorithm."</em> <code>min_cluster_size</code> is used to identify points ‚Äòfalling out of a cluster‚Äô or splitting to form two new clusters.</p><p>After labelling the documents, we drop those labelled as -1 (no clusters), and then create a dictionary with keys being the labelled group number, and the values being the page contents for each document.</p><pre><span>def cluster_docs(<br/>    self, <br/>    remaining_docs_with_keywords: List[Document], <br/>    hypo_params: dict<br/>    ) -&gt; Dict[int, List[str]]:<br/>    <br/>    from hdbscan import HDBSCAN<br/>    <br/>    if hypo_params['verbose']:<br/>        print(f"\n&gt;&gt;&gt; Clustering neglected Docs...")<br/>    <br/>    embeddings = self.embed_documents([<br/>        r[0].page_content <br/>        for r in remaining_docs_with_keywords], <br/>        {'verbose': False})<br/>    hdb = HDBSCAN(min_samples=1, min_cluster_size=3).fit(embeddings)<br/>    remaining_docs_with_cat = filter(lambda x: x[1] != -1, zip([r[0].page_content for r in remaining_docs_with_keywords], hdb.labels_))<br/>    <br/>    cat_dict = {}<br/><br/>    for page_content, cat in remaining_docs_with_cat:<br/>        if cat not in cat_dict:<br/>            cat_dict[cat] = [page_content]<br/>        else:<br/>            cat_dict[cat].append(page_content)<br/>            <br/>    if hypo_params['verbose']:<br/>        print(f"&gt;&gt;&gt; ...{len(cat_dict)} Clusters identified\n")<br/>            <br/>    return cat_dict</span></pre><h2><strong>Step 5: Generate Hypothetical Documents</strong></h2><p>Now, for each of our 6 clusters that represent groups of relevance patterns, we will ask our LLM to reference the documents belonging to the cluster to create a new hypothetical document that is similar to those documents. This is an important step that differentiates the AutoHyDE approach from existing HyDE approaches. In AutoHyDE, the relevance patterns are automatically learned and then used to generate the hypothetical documents. No customized prompt writing is required at all.</p><p>Here‚Äôs an example for the first cluster in the code below. You can see that the LLM referenced every neglected in the first neglected document cluster, and created a hypothetical document based on the original query. This is done for all the clusters. A quick look at the text below, and you can see that it really does emulate the convluted style of Mill quite well, and you did not have to write a custom prompt to try and capture this style. All it took was few-shot prompting under the hood.</p><figure><div><div><picture><source/><source/><img/></picture></div></div><figcaption>Image by Author: Hypothetical Document generated from AutoHyDE</figcaption></figure><p><strong>Technical Implementation</strong></p><p>What the function below does is to generate hypothetical documents for each identified cluster of documents. Similar to before, we use the JsonOutputParser to ensure that we get the desired output. I found that a few-shot approach generally works as you see in <code>HYPOTHETICAL_DOCUMENT_PROMPT</code> below:</p><pre><span>@retry(tries=5)<br/>def generate_hypo_docs(<br/>    self, <br/>    text: str, <br/>    cat_dict: Dict[int, List[str]], <br/>    hypo_params: dict<br/>    ) -&gt; List[str]:<br/>    <br/>    hypo_docs = list()<br/>    <br/>    if hypo_params['verbose']:<br/>        print(f"\n&gt;&gt;&gt; Generating Hypothetical Documents for each Doc Cluster...\n")<br/><br/>    HYPOTHETICAL_DOCUMENT_PROMPT = """<br/>    Your instruction is to generate a single hypothetical document from an input.<br/>    - This hypothetical document must be similar in style, tone and voice as examples you are provided with.<br/>    - This hypothetical document must appear like it was written by the same author as the examples you are provided with.<br/>    - This hypothetical document must also be similar in length with the examples you are provided with.<br/><br/>    {format_instructions}<br/><br/>    ### EXAMPLES ###<br/>    Below are some examples of hypothetical documents, all written by the same author, in pairs of &lt;Input&gt; and &lt;Hypothetical Document&gt;:<br/><br/>    {ref_documents}<br/><br/>    ### INSTRUCTION ###<br/>    Now generate a new hypothetical document. <br/><br/>    &lt;Input&gt;<br/>    {input}<br/>    &lt;Hypothetical Document&gt;<br/><br/>    """<br/><br/>    class HypotheticalDocumentSchema(BaseModel):<br/>        hypotheticalDocument: str = Field(description="a hypothetical document given an input word, phrase or question")<br/><br/>    parser = JsonOutputParser(pydantic_object=HypotheticalDocumentSchema)<br/><br/>    prompt = ChatPromptTemplate.from_template(<br/>        template=HYPOTHETICAL_DOCUMENT_PROMPT,<br/>        intput_variables = ["input", "ref_documents"],<br/>        partial_variables = {<br/>            'format_instructions': parser.get_format_instructions()<br/>        }<br/>    )<br/><br/>    hypothetical_document_chain = (<br/>        {'input': RunnablePassthrough(), 'ref_documents': RunnablePassthrough()}<br/>        | prompt<br/>        | self.llm_chain<br/>        | parser<br/>    )<br/><br/>    cat_ii = 1<br/>    for cat in cat_dict.keys():<br/><br/>        ref_doc_string = ""<br/>        doc_ii = 1<br/>        for doc in cat_dict[cat]:<br/>            ref_doc_string += f"\n\n&lt;Input&gt;"<br/>            ref_doc_string += text<br/>            ref_doc_string += f"\n\n&lt;Hypothetical Document&gt;\n"<br/>            ref_doc_string += f'{{"hypotheticalDocument": "{doc}"}}'<br/>            doc_ii += 1<br/><br/>        hypo_doc = hypothetical_document_chain.invoke(<br/>            {'input': text, 'ref_documents': ref_doc_string}<br/>        )['hypotheticalDocument']<br/><br/>        if hypo_params['verbose']:<br/>            print(f"\n### Hypo Doc {cat_ii} ###")<br/>            print(hypo_doc+'\n')<br/>        <br/>        hypo_docs.append(hypo_doc)<br/>        <br/>        cat_ii += 1<br/>        <br/>    return hypo_docs</span></pre><h2>Step 6+7: Embed &amp; Combine</h2><p>Finally, with the list of hypothetical documents, one for each cluster, we embed the hypothetical documents (step 6) and combine them into a single embedding vector (step 7).</p><p>This step is similar to the original implementation, where hypothetical documents are embedded and then averaged to get a final vector.</p><p>‚ö†Ô∏è As I mentioned earlier, I don‚Äôt think this step really makes sense to capture the broader heterogeneity of documents, especially when we‚Äôre trying to discover and generate documents with different relevance patterns. But to keep it simpler and maintain compatibility with the existing LangChain implementation (the point being that you can use this immediately with other component in LangChain RAG), we‚Äôll keep this for now to output a single embedding vector, and perhaps deal with the aggregation limitation later. If you‚Äôre convinced that simple averaging is not a good idea, you can always just stop at Step 6.</p><pre><span>&gt;&gt;&gt; Generating embeddings for hypothetical documents...<br/>&gt;&gt;&gt; Combining embeddings for hypothetical documents...</span></pre><pre><span>hyde_embedding[:10]<br/>&gt;&gt;&gt; [<br/>   0.015876703333653572,<br/>  -0.013635452586265312,<br/>   0.021941788843565697,<br/>  -0.02570370381768275,<br/>  -0.015861831315729033,<br/>  -0.0003427757204382006,<br/>   0.0027591148428962454,<br/>  -0.00883308151544041,<br/>  -0.014893267477206646,<br/>  -0.020748802766928837<br/>]</span></pre><p><strong>Technical Implementation</strong></p><pre><span>def embed_documents(self, texts: List[str], hypo_params) -&gt; List[List[float]]:<br/>    """Call the base embeddings."""<br/>    if hypo_params['verbose']:<br/>        print("\n&gt;&gt;&gt; Generating embeddings for hypothetical documents...\n")<br/>    return self.base_embeddings.embed_documents(texts)<br/><br/>def combine_embeddings(self, embeddings: List[List[float]], hypo_params) -&gt; List[float]:<br/>    """Combine embeddings into final embeddings."""<br/>    if hypo_params['verbose']:<br/>        print("\n&gt;&gt;&gt; Combining embeddings for hypothetical documents...\n")<br/>    return list(np.array(embeddings).mean(axis=0))</span></pre><p><em>As before, the repo where I implemented this enhanced version of HyDE can be found here at this </em><a><em>repo</em></a><em>.</em></p></div></div></div><div><span></span><span></span><span></span></div><div><div><div><h1><strong>Section 4: Conclusion</strong></h1><p>And that‚Äôs the end of the <strong>AutoHyDE</strong> demo!</p><p>To recap, the main improvement being made in AutoHyde is the way the hypothetical documents are being generated. Instead of using a fixed prompt to generate these documents (whether it be pre-defined in LangChain or customised by the user), I have devised a framework to automatically discover the underlying relevance patterns across all the documents that may be <em>neglected </em>by a baseline retrieval, and to generate hypothetical documents for each of these patterns.</p><p>In this manner, we are able to adapt HyDE for a wider variety of tasks and contexts, as well as to accommodate retrieval over an index which is heterogeneous in relevance pattern.</p><p>If you‚Äôve made it to the end, I‚Äôm always happy to hear your thoughts on this approach, and feel free to make suggestions to my implementation as well!</p><blockquote><p>Give a üëçüèª, leave a üí¨ and ü§ùüèª with me on <a>LinkedIn</a>!</p></blockquote></div></div></div><div><span></span><span></span><span></span></div><div><div><div><h2>References</h2><ul><li>Unsupervised Dense Information Retrieval with Contrastive Learning (<a>https://arxiv.org/pdf/2112.09118.pdf</a>)</li><li>Precise Zero-Shot Dense Retrieval without Relevance Labels (<a>https://arxiv.org/pdf/2212.10496.pdf</a>)</li><li>LangChain HyDE Repo (<a>https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/hyde</a>)</li><li>Original HyDE Implementation (<a>https://github.com/texttron/hyde/blob/74101c5157e04f7b57559e7da8ef4a4e5b6da82b/src/hyde/hyde.py</a>)</li></ul></div></div></div></div></section></div></div></article><div><div></div></div></div><div><div><div><div><a><div>Llm</div></a></div><div><a><div>AI</div></a></div><div><a><div>Langchain</div></a></div><div><a><div>Data Science</div></a></div><div><a><div>Hands On Tutorials</div></a></div></div></div></div><div></div><footer><div><div><div><div><div><div><span><div><div><div><div><svg><path></path><path></path></svg></div></div></div><div><p><span>--</span></p></div></div></span><span><div><div><div><div><svg><path></path><path></path></svg></div></div></div><div><p><span>--</span></p></div></div></span></div><div><div><div><button><svg><path></path></svg><p><span>6</span></p></button></div></div></div></div><div><div><div><div><button><svg><path></path></svg></button></div></div></div><div><div><div><div><button><svg><path></path></svg></button></div></div></div></div><div><div><div><button><svg><path></path></svg></button></div></div></div></div></div></div></div></div></footer><div><div><div><div><div><a><div><div><img/><div></div></div></div></a><a><div><div><div><div><div><img/><div></div></div></div></div></div></div></a></div><div><div><button>Follow</button><div><div><div><div><div><button><svg><rect></rect><rect></rect><path></path><path></path></svg></button></div></div></div></div></div></div></div></div><div><div><div><a><h2><span>Written by <!-- -->Ian Ho</span></h2></a></div><div><div><span><a>549 Followers</a></span></div><div><span><span>¬∑</span></span><span>Writer for </span><div><div><a><p>Towards Data Science</p></a></div></div></div></div><div><p><span>Data Scientist @ GovTech</span></p></div></div><div><div><button>Follow</button><div><div><div><div><div><button><svg><rect></rect><rect></rect><path></path><path></path></svg></button></div></div></div></div></div></div></div></div><div></div></div></div><div><div></div><div><div><div><div><a><p>Help</p></a></div><div><a><p>Status</p></a></div><div><a><p>About</p></a></div><div><a><p>Careers</p></a></div><div><a><p>Blog</p></a></div><div><a><p>Privacy</p></a></div><div><a><p>Terms</p></a></div><div><a><p>Text to speech</p></a></div><div><a><p>Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20240426-213717-501a5f7ae6"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"User is logged in","group":"disabled","tags":["group-edgeCachePosts","post-619e58cdbd8e","user-ca061af8c7b7","collection-7f60cf5620c9"],"serverVariantState":"","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":false,"vary":[],"loHomepageEnabled":false,"updatedPostPreviewsEnabled":false,"customMocPreviewWeightThreshold":"control","recommendedTagsQueryEnabled":false,"enableLohpWithSearch":"control","enableLohpFocused":"control","enableMobileLohpShortHero":"control","enableSpamBuster":"control"},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true},"viewerIsBot":false},"debug":{"requestId":"f8cd27d7-c62b-472b-af14-b19fd6b71d62","hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"31759738003c5db8","ot-tracer-traceid":"220c1812b24fe62a","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fautohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"config":{"nodeEnv":"production","version":"main-20240426-213717-501a5f7ae6","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20240426-213717-501a5f7ae6","commit":"501a5f7ae6ad1c8201b23123c9376836f6af3ec3"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don‚Äôt fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":"b0726029b979"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"android_two_hour_refresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier_badge","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_switch_plan_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_easy_resubscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_legacy_feed_in_iceland","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_google_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_dynamic_programming_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_partner_program_enrollment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_auto_follow_on_subscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_premium_plan","valueType":{"__typename":"VariantFlagString","value":"12a660186432"}},{"__typename":"VariantFlag","name":"enable_recaptcha_enterprise","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_autorefresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_programming","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_simplified_digest_v2_b","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expired_membership_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_in_app_free_trial","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"onboarding_tags_from_top_views","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_widget","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_aspiriational","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_eventstats_event_processing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_diversification_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sprig","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_moc_preview_weight_threshold_li","valueType":{"__typename":"VariantFlagString","value":"group_2"}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_entities_to_follow_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lohp_focused","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"ios_enable_verified_book_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_send_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_dashboard_referred_earnings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reengagement_notification_duration","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"enable_android_dynamic_aspirational_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_deprecate_legacy_providers_v3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recirc_model","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_syntax_highlight","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"crm_send_contact_to_sendgrid","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound-source-serif-pro"}},{"__typename":"VariantFlag","name":"enable_seamless_social_sharing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_play_purchase_on_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_validate_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automod","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pill_based_home_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_spam_buster","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"can_receive_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_new_push_notification_endpoint","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lohp_with_search","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_pre_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_display_paywall_after_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_image_sharer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tag_recs","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefined_top_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members_username_selection","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_moc_preview_weight_threshold","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"available_annual_premium_plan","valueType":{"__typename":"VariantFlagString","value":"4a442ace1476"}},{"__typename":"VariantFlag","name":"enable_newsletter_lo_flow_custom_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cache_less_following_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_response_markup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reader_fair_distribution_non_qp","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"textshots_userid","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_pp_writer_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_iceland_forced_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_verifications_service","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_topic_portals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_group_gifting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_miro_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_maim_the_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_updated_new_user_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mobile_lohp_short_hero","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_rex_aggregator_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"price_smoke_test_yearly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_avatar_upload","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_reading_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"web_enable_syntax_highlighting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_fs_cache_user_vals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_verified_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_members_only_audio","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_rating_prompt_stories_read_threshold","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_create_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_lists_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"enable_creator_welcome_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_archive_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_c","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_remove_twitter_onboarding_step","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_social_share_sheet","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"price_smoke_test_monthly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"rex_generator_max_candidates","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_user_onboarding_emails_flow","valueType":{"__typename":"VariantFlagString","value":"group_1"}},{"__typename":"VariantFlag","name":"android_enable_editor_new_publishing_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"isLoggedIn":true,"viewer":{"__ref":"User:2ae680787402"},"collectionByDomainOrSlug({\"domainOrSlug\":\"towardsdatascience.com\"})":{"__ref":"Collection:7f60cf5620c9"},"postResult({\"id\":\"619e58cdbd8e\"})":{"__ref":"Post:619e58cdbd8e"}},"Membership:9a4bea77cdac":{"__typename":"Membership","id":"9a4bea77cdac","tier":"MEMBER","memberSince":1713637217000,"friendSince":null},"UserViewerEdge:userId:2ae680787402-viewerId:2ae680787402":{"__typename":"UserViewerEdge","id":"userId:2ae680787402-viewerId:2ae680787402","createdAt":1568800581191,"membership":{"__ref":"MembershipStatus:{}"}},"User:2ae680787402":{"__typename":"User","id":"2ae680787402","allowEmailAddressSharingEditorWriter":false,"atsQualifiedAt":0,"dismissableFlags":["TOOLTIP_ABOUT_EDITOR"],"emailObfuscated":"ak‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢@gmail.com","geolocation":{"__typename":"Geolocation","country":"IN"},"hasGroupGiftingEnabled":false,"hasPastMemberships":true,"hasSubdomain":false,"imageId":"0*kMyeT04xC4BXpBc6.jpg","isEligibleToImportEmails":false,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"membership":{"__ref":"Membership:9a4bea77cdac"},"name":"Akash Mishra","partnerProgramEnrollment":null,"postSubscribeMembershipUpsellShownAt":0,"styleEditorOnboardingVersionSeen":0,"twitterScreenName":"","unverifiedEmail":"","username":"akashm1219","viewerEdge":{"__ref":"UserViewerEdge:userId:2ae680787402-viewerId:2ae680787402"}},"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png":{"__typename":"ImageMetadata","id":"1*VzTUkfeGymHP4Bvav-T-lA.png"},"Collection:7f60cf5620c9":{"__typename":"Collection","id":"7f60cf5620c9","favicon":{"__ref":"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png"},"googleAnalyticsId":null,"editors":[{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:7e12c71dfa81"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:e6ad8abedec9"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:895063a310f4"}}],"name":"Towards Data Science","avatar":{"__ref":"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg"},"domain":"towardsdatascience.com","slug":"towards-data-science","description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","subscriberCount":688958,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:2ae680787402"},"twitterUsername":"TDataScience","facebookPageId":null,"logo":{"__ref":"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png"},"customStyleSheet":null,"colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}}},"User:7e12c71dfa81":{"__typename":"User","id":"7e12c71dfa81"},"User:e6ad8abedec9":{"__typename":"User","id":"e6ad8abedec9"},"User:895063a310f4":{"__typename":"User","id":"895063a310f4"},"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg":{"__typename":"ImageMetadata","id":"1*CJe3891yB1A1mzMdqemkdg.jpeg"},"LinkedAccounts:ca061af8c7b7":{"__typename":"LinkedAccounts","mastodon":null,"id":"ca061af8c7b7"},"UserViewerEdge:userId:ca061af8c7b7-viewerId:2ae680787402":{"__typename":"UserViewerEdge","id":"userId:ca061af8c7b7-viewerId:2ae680787402","isFollowing":false,"isUser":false,"isMuting":false},"NewsletterV3:b1dfe19cf495":{"__typename":"NewsletterV3","id":"b1dfe19cf495","type":"NEWSLETTER_TYPE_AUTHOR","slug":"ca061af8c7b7","name":"ca061af8c7b7","collection":null,"user":{"__ref":"User:ca061af8c7b7"}},"User:ca061af8c7b7":{"__typename":"User","id":"ca061af8c7b7","name":"Ian Ho","username":"ianhojy","newsletterV3":{"__ref":"NewsletterV3:b1dfe19cf495"},"linkedAccounts":{"__ref":"LinkedAccounts:ca061af8c7b7"},"isSuspended":false,"imageId":"0*x4qNNXqMnKmaw-9P","mediumMemberAt":1691566395000,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":549},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"ianhojy.medium.com"}},"hasSubdomain":true,"bio":"Data Scientist @ GovTech","isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:ca061af8c7b7-viewerId:2ae680787402"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"allowNotes":true,"membership":{"__ref":"Membership:34d3b27be11e"},"twitterScreenName":""},"Topic:1eca0103fff3":{"__typename":"Topic","slug":"machine-learning","id":"1eca0103fff3","name":"Machine Learning"},"Paragraph:f02f576c9bc6_0":{"__typename":"Paragraph","id":"f02f576c9bc6_0","name":"9b6e","type":"H3","href":null,"layout":null,"metadata":null,"text":"AutoHyDE: Making HyDE Better for Advanced LLM RAG","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_1":{"__typename":"Paragraph","id":"f02f576c9bc6_1","name":"08c4","type":"H4","href":null,"layout":null,"metadata":null,"text":"üîé A deep-dive into HyDE for Advanced LLM RAG + üí° Introducing AutoHyDE, a semi-supervised framework to improve the effectiveness, coverage and applicability of HyDE","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BoEb2eI8BFOectBr5y0zjA.png":{"__typename":"ImageMetadata","id":"1*BoEb2eI8BFOectBr5y0zjA.png","originalHeight":874,"originalWidth":1230,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_2":{"__typename":"Paragraph","id":"f02f576c9bc6_2","name":"ac60","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*BoEb2eI8BFOectBr5y0zjA.png"},"text":"Image by Author with the help of DALL-E","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_3":{"__typename":"Paragraph","id":"f02f576c9bc6_3","name":"8b4d","type":"H3","href":null,"layout":null,"metadata":null,"text":"Introduction","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_4":{"__typename":"Paragraph","id":"f02f576c9bc6_4","name":"745d","type":"P","href":null,"layout":null,"metadata":null,"text":"In the field of Retrieval Augmented Generation (RAG), Hypothetical Document Embeddings (HyDE) have proven to be a powerful form of query rewriting to improve the relevance of retrieved documents.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":53,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_5":{"__typename":"Paragraph","id":"f02f576c9bc6_5","name":"9ef5","type":"P","href":null,"layout":null,"metadata":null,"text":"For the uninitiated, whilst traditional retrieval simply uses the original input to create embedding vectors for retrieval, HyDE is a methodology to generate embedding vectors that are more relevant to the embedding space of the indexed documents to be retrieved.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_6":{"__typename":"Paragraph","id":"f02f576c9bc6_6","name":"09d2","type":"P","href":null,"layout":null,"metadata":null,"text":"The super high-level summary is: (1) Create hypothetical documents from user input, (2) Convert hypothetical documents to embeddings, (3) Use embeddings to retrieve similar documents","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_7":{"__typename":"Paragraph","id":"f02f576c9bc6_7","name":"fea6","type":"P","href":null,"layout":null,"metadata":null,"text":"I‚Äôve been using RAG and basic HyDE in some of my work and personal projects, and after some time, I‚Äôve realized that the existing implementation of HyDE does not always work well out of the box and it is not as flexible as I hoped it would be. So after doing my research on the methodology and digging through the papers and source codes, I wanted to share my thoughts on the current approach, and to suggest an enhanced version of HyDE (I call it AutoHyDE) that has the potential to be more effective and adaptable across a variety of use-cases.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":412,"end":546,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_8":{"__typename":"Paragraph","id":"f02f576c9bc6_8","name":"e2d0","type":"P","href":null,"layout":null,"metadata":null,"text":"In this article, Section 1 will be a deep dive into the original HyDE paper and we will go through how it has been translated into the LangChain implementation. If you are already familiar with HyDE, feel free to skip ahead.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":17,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_9":{"__typename":"Paragraph","id":"f02f576c9bc6_9","name":"76c1","type":"P","href":null,"layout":null,"metadata":null,"text":"In Section 2, I will discuss what I believe are the major limitations of the current HyDE approach.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":3,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_10":{"__typename":"Paragraph","id":"f02f576c9bc6_10","name":"8d44","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, I will introduce AutoHyDE in Section 3, an attempt by me to create what I believe can be a better version of HyDE. Here is the tldr:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":26,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_11":{"__typename":"Paragraph","id":"f02f576c9bc6_11","name":"6ba5","type":"BQ","href":null,"layout":null,"metadata":null,"text":"AutoHyDE is a framework to automatically discover underlying relevance patterns in your indexed documents and to generate hypothetical documents that directly represent these relevance patterns.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":194,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_12":{"__typename":"Paragraph","id":"f02f576c9bc6_12","name":"a06c","type":"P","href":null,"layout":null,"metadata":null,"text":"I have also directly tweaked the LangChain HypotheticalDocumentEmbedder class to realize AutoHyDE, so that it can still be chained with other parts of LangChain.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":43,"end":71,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_13":{"__typename":"Paragraph","id":"f02f576c9bc6_13","name":"295a","type":"P","href":null,"layout":null,"metadata":null,"text":"Check out the repo for more details, demo & source code.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":14,"end":18,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":37,"end":41,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Fblob\u002Fmain\u002Fauto-hyde-demo.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":44,"end":55,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Fblob\u002Fmain\u002Fsrc\u002Fauto_hyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_14":{"__typename":"Paragraph","id":"f02f576c9bc6_14","name":"9335","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Give a üëçüèª, leave a üí¨ and ü§ùüèª with me on LinkedIn!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":44,"end":52,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fianhojy\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg":{"__typename":"ImageMetadata","id":"1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg","originalHeight":1206,"originalWidth":1696,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_15":{"__typename":"Paragraph","id":"f02f576c9bc6_15","name":"2e6e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*wPk_jWcqgqKRwjbq0RgJuQ.jpeg"},"text":"Image by Author: Code Snippet for AutoHyDE","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_16":{"__typename":"Paragraph","id":"f02f576c9bc6_16","name":"2fc9","type":"P","href":null,"layout":null,"metadata":null,"text":"For a higher-level overview of RAG and Query Rewriting, this is a good article written by Florian June. This repo also contains some good practical implementations of RAG, HyDE and other enhancements.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":71,"end":78,"href":"https:\u002F\u002Fmedium.com\u002F@florian_algo\u002Fadvanced-rag-06-exploring-query-rewriting-23997297f2d1","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":109,"end":113,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Frag-from-scratch\u002Fblob\u002Fmain\u002Frag_from_scratch_5_to_9.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":200,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_17":{"__typename":"Paragraph","id":"f02f576c9bc6_17","name":"7cf8","type":"H3","href":null,"layout":null,"metadata":null,"text":"Section 1: What is HyDE?","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_18":{"__typename":"Paragraph","id":"f02f576c9bc6_18","name":"4ac8","type":"H4","href":null,"layout":null,"metadata":null,"text":"HyDE: The Original Paper","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_19":{"__typename":"Paragraph","id":"f02f576c9bc6_19","name":"4b56","type":"P","href":null,"layout":null,"metadata":null,"text":"Hypothetical Document Embeddings (HyDE) were first introduced in the 2022 paper by Gao et al. titled Precise Zero-Shot Dense Retrieval without Relevance Labels.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":101,"end":159,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_20":{"__typename":"Paragraph","id":"f02f576c9bc6_20","name":"cda4","type":"P","href":null,"layout":null,"metadata":null,"text":"The paper set out to find a way to improve zero-shot dense retrieval (i.e. using semantic embedding similarities). To that end, they came up with a two-step methodology called HyDE.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_21":{"__typename":"Paragraph","id":"f02f576c9bc6_21","name":"e645","type":"P","href":null,"layout":null,"metadata":null,"text":"Step 1 involved instruction prompting a language model (in the paper they use GPT-3) to generate a hypothetical document given the original query (specifically limited to a question in the paper).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_22":{"__typename":"Paragraph","id":"f02f576c9bc6_22","name":"3524","type":"P","href":null,"layout":null,"metadata":null,"text":"Step 2 involved using a Contriever, or an ‚Äúunsupervised contrastive encoder‚Äù, to turn this hypothetical document into an embedding vector, which is then used for downstream similarity search and retrieval.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":24,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":42,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UFRWRzsuBxwRypjA_y9tJQ.png":{"__typename":"ImageMetadata","id":"1*UFRWRzsuBxwRypjA_y9tJQ.png","originalHeight":734,"originalWidth":2592,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_23":{"__typename":"Paragraph","id":"f02f576c9bc6_23","name":"1928","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*UFRWRzsuBxwRypjA_y9tJQ.png"},"text":"Illustration of HyDE methodology from the original paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":42,"end":56,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_24":{"__typename":"Paragraph","id":"f02f576c9bc6_24","name":"5521","type":"H4","href":null,"layout":null,"metadata":null,"text":"[Optional] An aside on the Contriever","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":1,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":27,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_25":{"__typename":"Paragraph","id":"f02f576c9bc6_25","name":"c72c","type":"P","href":null,"layout":null,"metadata":null,"text":"The Contriever used in the original HyDE paper was derived from an earlier paper in August 2022 by Izacard et al. called Unsupervised Dense Information Retrieval with Contrastive Learning. In this paper, the authors claim that while neural networks had emerged as good alternatives to term-frequency methods for retrieval, they required large amounts of data and did not always transfer well to new areas of application. Therefore, they devised a way to train the embedding network in an unsupervised manner through contrastive learning.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":121,"end":187,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2112.09118.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":515,"end":536,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":4,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_26":{"__typename":"Paragraph","id":"f02f576c9bc6_26","name":"363f","type":"P","href":null,"layout":null,"metadata":null,"text":"For those interested in Contrastive Learning, you can check out Section 3 of the paper for information, but here‚Äôs a high-level summary:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_27":{"__typename":"Paragraph","id":"f02f576c9bc6_27","name":"b36a","type":"P","href":null,"layout":null,"metadata":null,"text":"First, they define a loss function (contrastive InfoNCE loss) that rewards positive pairs of documents and penalizes negative pairs. Next, they build positive and negative pairs of document representations. Positive pairs are constructed using a data augmentation method called the Inverse Cloze Task (ICT), and through Independent Cropping. Negative pairs are constructed using in-batch negative sampling, and across-batch negative sampling (also called MoCo). Finally, they train this over the BERT base uncased architecture.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_28":{"__typename":"Paragraph","id":"f02f576c9bc6_28","name":"f412","type":"P","href":null,"layout":null,"metadata":null,"text":"The results of the paper showed that this method was able to match unsupervised term-frequency methods like BM25, and also performed well on cross-lingual retrieval, an option which is not possible for term matching methods.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_29":{"__typename":"Paragraph","id":"f02f576c9bc6_29","name":"ffb0","type":"P","href":null,"layout":null,"metadata":null,"text":"For the purposes of this article, I will not be going further into the details of training and data augmentation, but do check out the paper if you are interested. The contriever can also be found on HuggingFace at this link, and the research repo can be found here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":220,"end":224,"href":"https:\u002F\u002Fhuggingface.co\u002Ffacebook\u002Fcontriever","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":261,"end":265,"href":"https:\u002F\u002Fgithub.com\u002Ffacebookresearch\u002Fcontriever","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_30":{"__typename":"Paragraph","id":"f02f576c9bc6_30","name":"388f","type":"P","href":null,"layout":null,"metadata":null,"text":"Anyway, It is this Contriever that the HyDE paper uses to embed generated documents. They also explore ContrieverFT, which is in-domain fine-tuned in a supervised manner. Now, back to the main focus of the article, which is the HyDE Paper.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":19,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":103,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_31":{"__typename":"Paragraph","id":"f02f576c9bc6_31","name":"96f3","type":"H4","href":null,"layout":null,"metadata":null,"text":"Back to the HyDE Paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_32":{"__typename":"Paragraph","id":"f02f576c9bc6_32","name":"b4ef","type":"P","href":null,"layout":null,"metadata":null,"text":"Previously, I provided an overview of the HyDE methodology and the origins of the underlying contriever. Now, let‚Äôs take a closer look at what‚Äôs going on in HyDE by reviewing some key excerpts from the paper.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":93,"end":105,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_33":{"__typename":"Paragraph","id":"f02f576c9bc6_33","name":"3a4f","type":"BQ","href":null,"layout":null,"metadata":null,"text":"‚Ä¶In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder‚Äôs dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_34":{"__typename":"Paragraph","id":"f02f576c9bc6_34","name":"c9d7","type":"P","href":null,"layout":null,"metadata":null,"text":"More formally, the hypothetical document embedding vector is defined as:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UBvSsoJUagB0lh7r4j661A.png":{"__typename":"ImageMetadata","id":"1*UBvSsoJUagB0lh7r4j661A.png","originalHeight":126,"originalWidth":1012,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_35":{"__typename":"Paragraph","id":"f02f576c9bc6_35","name":"9ead","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*UBvSsoJUagB0lh7r4j661A.png"},"text":"Formula for encoding the generated document in the HyDE Paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":51,"end":61,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_36":{"__typename":"Paragraph","id":"f02f576c9bc6_36","name":"d2b4","type":"ULI","href":null,"layout":null,"metadata":null,"text":"where g is the InstructLM(query, INST) function in the first step of hypothetical document generation; and","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":15,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_37":{"__typename":"Paragraph","id":"f02f576c9bc6_37","name":"d525","type":"ULI","href":null,"layout":null,"metadata":null,"text":"where f is the document encoder (contriever)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":33,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_38":{"__typename":"Paragraph","id":"f02f576c9bc6_38","name":"0eb9","type":"P","href":null,"layout":null,"metadata":null,"text":"‚ÄúFormally, g defines a probability distribution based on the chain rule‚Äù, and V is estimated through the following equation:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*y9lHVX24zORuojIz8GMiMA.png":{"__typename":"ImageMetadata","id":"1*y9lHVX24zORuojIz8GMiMA.png","originalHeight":454,"originalWidth":878,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_39":{"__typename":"Paragraph","id":"f02f576c9bc6_39","name":"e666","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*y9lHVX24zORuojIz8GMiMA.png"},"text":"Formula for estimating the embedding vector in the HyDE Paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":51,"end":61,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_40":{"__typename":"Paragraph","id":"f02f576c9bc6_40","name":"a4a5","type":"P","href":null,"layout":null,"metadata":null,"text":"In the original paper, you can also find the python implementation at this repo, where:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":75,"end":79,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_41":{"__typename":"Paragraph","id":"f02f576c9bc6_41","name":"f227","type":"P","href":null,"layout":null,"metadata":null,"text":"f is the generative function as we see in the repo:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:01ab5468a67531ae11e216e18ce1ba16":{"__typename":"MediaResource","id":"01ab5468a67531ae11e216e18ce1ba16","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"hyde-generate.py"},"Paragraph:f02f576c9bc6_42":{"__typename":"Paragraph","id":"f02f576c9bc6_42","name":"881e","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"generate function from texttron\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":23,"end":36,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:01ab5468a67531ae11e216e18ce1ba16"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_43":{"__typename":"Paragraph","id":"f02f576c9bc6_43","name":"a2a6","type":"P","href":null,"layout":null,"metadata":null,"text":"g is the encoding function as we see in the repo:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:3c8f50e3614243b14f4eab2837713b63":{"__typename":"MediaResource","id":"3c8f50e3614243b14f4eab2837713b63","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"hyde-encode.py"},"Paragraph:f02f576c9bc6_44":{"__typename":"Paragraph","id":"f02f576c9bc6_44","name":"c833","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"encode function from texttron\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":21,"end":34,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:3c8f50e3614243b14f4eab2837713b63"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_45":{"__typename":"Paragraph","id":"f02f576c9bc6_45","name":"2139","type":"P","href":null,"layout":null,"metadata":null,"text":"Just as in the estimation equation, we see that a simple averaging is used to estimate the embedding vector across the various hypothetical documents. This mean-aggregated embedding is then used to do similarity search:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:84f3be025edaa08ac709e2943e3e8f2e":{"__typename":"MediaResource","id":"84f3be025edaa08ac709e2943e3e8f2e","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"hyde-search.py"},"Paragraph:f02f576c9bc6_46":{"__typename":"Paragraph","id":"f02f576c9bc6_46","name":"c5b7","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"search function from texttron\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":21,"end":34,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:84f3be025edaa08ac709e2943e3e8f2e"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_47":{"__typename":"Paragraph","id":"f02f576c9bc6_47","name":"20e0","type":"P","href":null,"layout":null,"metadata":null,"text":"Overall, we see that HyDE is really not a complicated implementation. Generate, embed, average, retrieve.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":70,"end":104,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_48":{"__typename":"Paragraph","id":"f02f576c9bc6_48","name":"b3bf","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, let‚Äôs take a look at popular implementations of HyDE that have surfaced recently.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_49":{"__typename":"Paragraph","id":"f02f576c9bc6_49","name":"252e","type":"H4","href":null,"layout":null,"metadata":null,"text":"LangChain Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_50":{"__typename":"Paragraph","id":"f02f576c9bc6_50","name":"b66b","type":"P","href":null,"layout":null,"metadata":null,"text":"In both LangChain & LlamaIndex, there are implementations of HyDE that are similar to what we‚Äôve seen in the paper and repo above. For most of us who are playing around with LLM capabilities, these are the most likely ways that we‚Äôll be looking to use HyDE. We‚Äôll also have a closer look at the prompts being used under the hood for these HyDE classes. Let‚Äôs focus on the LangChain implementation. The repo for reference is here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":424,"end":428,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde\u002Fbase.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:4693b29b6b93f92975dcb520235bea23":{"__typename":"MediaResource","id":"4693b29b6b93f92975dcb520235bea23","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"langchain-hyde-from-llm.py"},"Paragraph:f02f576c9bc6_51":{"__typename":"Paragraph","id":"f02f576c9bc6_51","name":"760e","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"from_llm constructor for HypotheticalDocumentEmbedder in langchain\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":57,"end":71,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde\u002Fbase.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:4693b29b6b93f92975dcb520235bea23"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_52":{"__typename":"Paragraph","id":"f02f576c9bc6_52","name":"5087","type":"P","href":null,"layout":null,"metadata":null,"text":"The first step is to create the HypotheticalDocumentEmbedder using from_llm ‚Äî you can choose one of the existing prompt templates such as the one you see below for writing a news passage, or define your own custom prompt.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":32,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":67,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_53":{"__typename":"Paragraph","id":"f02f576c9bc6_53","name":"09de","type":"PRE","href":null,"layout":null,"metadata":null,"text":"trec_news_template = \"\"\"Please write a news passage about the topic.\nTopic: {TOPIC}\nPassage:\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_54":{"__typename":"Paragraph","id":"f02f576c9bc6_54","name":"1185","type":"P","href":null,"layout":null,"metadata":null,"text":"Then, you can execute embed_query , which first generates the hypothetical document(s) based on the prompt template, and then translates these documents into embeddings and combines them. The embed_documents function simply uses the defined base_embeddings from the from_llm definition, while the combine_embeddings is a mean aggregation.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":22,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":192,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":241,"end":256,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":266,"end":274,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":297,"end":315,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:66bcb6e766294aa26a85adc1de110f9b":{"__typename":"MediaResource","id":"66bcb6e766294aa26a85adc1de110f9b","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"langchain-hyde-embed-query.py"},"Paragraph:f02f576c9bc6_55":{"__typename":"Paragraph","id":"f02f576c9bc6_55","name":"4e31","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"embed_query function for langchain\u002Fhyde","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":25,"end":39,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde\u002Fbase.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:66bcb6e766294aa26a85adc1de110f9b"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_56":{"__typename":"Paragraph","id":"f02f576c9bc6_56","name":"cfe7","type":"P","href":null,"layout":null,"metadata":null,"text":"You will also notice that documents is of List[str] type because you can generate multiple documents. All you have to do is define the LLM accordingly in the from_llm step. Here‚Äôs an example:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":42,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":158,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_57":{"__typename":"Paragraph","id":"f02f576c9bc6_57","name":"3464","type":"PRE","href":null,"layout":null,"metadata":null,"text":"multi_llm = OpenAI(n=4, best_of=4)\nembeddings = HypotheticalDocumentEmbedder.from_llm(\n    multi_llm, base_embeddings, \"web_search\"\n)\nresult = embeddings.embed_query(\"Where is the Taj Mahal?\")","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_58":{"__typename":"Paragraph","id":"f02f576c9bc6_58","name":"f364","type":"P","href":null,"layout":null,"metadata":null,"text":"For a simple walkthrough of how to use HyDE, you can refer to the notebook. It‚Äôs nothing too complicated and LangChain has made it really easy to use by abstracting away most of the underlying functionality.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":66,"end":74,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Fblob\u002Fmain\u002Flangchain-hyde-demo.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_59":{"__typename":"Paragraph","id":"f02f576c9bc6_59","name":"2fe3","type":"H3","href":null,"layout":null,"metadata":null,"text":"Section 2: Limitations of HyDE","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_60":{"__typename":"Paragraph","id":"f02f576c9bc6_60","name":"381b","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we have a good idea of how HyDE was originally designed and how it was eventually implemented in the popular LangChain & LlamaIndex libraries, I want to talk about their limitations.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_61":{"__typename":"Paragraph","id":"f02f576c9bc6_61","name":"c51b","type":"P","href":null,"layout":null,"metadata":null,"text":"The first thing I noticed is that whilst the HyDE paper does experiment with different instruction LLMs, it does not spend too much time talking about further optimizing this aspect of the HyDE process. From table 4 in the paper, we see that using different models results in rather huge result variance:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*u9xHr0QqHbCMaPw-C-hvkQ.png":{"__typename":"ImageMetadata","id":"1*u9xHr0QqHbCMaPw-C-hvkQ.png","originalHeight":648,"originalWidth":780,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_62":{"__typename":"Paragraph","id":"f02f576c9bc6_62","name":"16ad","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*u9xHr0QqHbCMaPw-C-hvkQ.png"},"text":"HyDE results from using different LLMs in the HyDE Paper","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":46,"end":56,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_63":{"__typename":"Paragraph","id":"f02f576c9bc6_63","name":"92bc","type":"P","href":null,"layout":null,"metadata":null,"text":"The key implication here is not to use the biggest models with the most parameters. Instead, the fact that the underlying LLMs bring about such huge differences for the overall tasks of retrieval relevance convinces me that apart from using different LLMs, the generative task is one area worth further optimizing.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":257,"end":313,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_64":{"__typename":"Paragraph","id":"f02f576c9bc6_64","name":"b957","type":"P","href":null,"layout":null,"metadata":null,"text":"In fact, the researchers themselves do state that:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_65":{"__typename":"Paragraph","id":"f02f576c9bc6_65","name":"2730","type":"BQ","href":null,"layout":null,"metadata":null,"text":"InstructGPT model able to further bring up the performance‚Ä¶ this suggests that there may still exist certain factors not captured by the fine-tuned encoder but only by the generative model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_66":{"__typename":"Paragraph","id":"f02f576c9bc6_66","name":"7f84","type":"P","href":null,"layout":null,"metadata":null,"text":"With that possibility in mind, we have also seen from the LangChain deep-dive in Section 1 that the prompt templates are pre-defined to be generic in instructing hypothetical documents. For example, ‚ÄúPlease write a scientific paper passage to support\u002Frefute the claim‚Äù. This only works when your query\u002Finput nicely fits these pre-defined templates. Otherwise, you can also provide a custom template to generate hypothetical documents, but this still has its limitations.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":199,"end":268,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_67":{"__typename":"Paragraph","id":"f02f576c9bc6_67","name":"8f71","type":"P","href":null,"layout":null,"metadata":null,"text":"Firstly, the existing implementations are largely limited to a Q&A framework. In the real world, not all retrieval use-cases are for answering questions, and thus asking the LLM to hypothetically answer your question does not make make sense. So in all these other situations, what do we ask the LLM to generate hypothetically then? Yes, custom prompting is available, but there is no guarantee that the prompt you write is actually relevant to the existing documents. Unless the underlying documents are very uniform in terms of relevance patterns, writing a prompt to capture these patterns would be a very manual and imperfect process.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_68":{"__typename":"Paragraph","id":"f02f576c9bc6_68","name":"af16","type":"P","href":null,"layout":null,"metadata":null,"text":"More importantly, when you chunk and index your documents, the chunks are not always homogenous in terms of style, tone, structure etc. Consequently, it may be insufficient to use a single generalized prompt to generate hypothetical documents given your input, and it may be intractable to write multiple prompts to cater to different kinds of chunks. For instance, imagine you are trying to generate hypothetical documents for blog posts. With different authors, tones, writing styles, how do you create a generalized prompt to capture this variety? Thus, the current implementation of HyDE appears to be rather rigid and suboptimal in terms of coverage.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":63,"end":95,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":160,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_69":{"__typename":"Paragraph","id":"f02f576c9bc6_69","name":"8bfa","type":"P","href":null,"layout":null,"metadata":null,"text":"On a more technical level, we also note that regarding the final vector embedding, the paper says that:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_70":{"__typename":"Paragraph","id":"f02f576c9bc6_70","name":"d779","type":"BQ","href":null,"layout":null,"metadata":null,"text":"We simply consider the expectation value, assuming the distribution of v_qij is uni-modal, i.e. the query is not ambiguous.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_71":{"__typename":"Paragraph","id":"f02f576c9bc6_71","name":"c205","type":"P","href":null,"layout":null,"metadata":null,"text":"But if you think about it, most humans approach search problems with ambiguity.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_72":{"__typename":"Paragraph","id":"f02f576c9bc6_72","name":"9134","type":"P","href":null,"layout":null,"metadata":null,"text":"This is an important caveat. In real use-cases, I can imagine that the distribution of v is not in fact nicely uni-modal, especially when the query is as ambiguous as just one or two keywords. What this means is that the same underlying query can be mapped to more than one mode of vector embedding representations.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_73":{"__typename":"Paragraph","id":"f02f576c9bc6_73","name":"56aa","type":"P","href":null,"layout":null,"metadata":null,"text":"How might we begin to improve HyDE?","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_74":{"__typename":"Paragraph","id":"f02f576c9bc6_74","name":"8504","type":"BQ","href":null,"layout":null,"metadata":null,"text":"HyDE appears unsupervised. No model is trained in HyDE: both the generative model and the contrastive encoder remain intact.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_75":{"__typename":"Paragraph","id":"f02f576c9bc6_75","name":"4894","type":"P","href":null,"layout":null,"metadata":null,"text":"The unsupervised nature of HyDE is important because it does away with the need to have huge datasets or generous amounts of compute for any supervised variant of methodology to improve dense retrieval.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_76":{"__typename":"Paragraph","id":"f02f576c9bc6_76","name":"3ed3","type":"P","href":null,"layout":null,"metadata":null,"text":"However, by remaining strictly unsupervised, we lose some benefits. Specifically, there is no way to dynamically define multiple generative functions g(q, INST) that are more aligned with the document embeddings.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":150,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_77":{"__typename":"Paragraph","id":"f02f576c9bc6_77","name":"6bd8","type":"P","href":null,"layout":null,"metadata":null,"text":"Instead, it might be worth considering a semi-supervised approach that is more adaptive and generalizable. This approach does not require one to forcefully fit hypothetical generation of documents into a single pre-defined\u002Fcustom prompt. It does not even require one to know what custom prompts to write, unlike the LangChain\u002FLlamaIndex implementations.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":39,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_78":{"__typename":"Paragraph","id":"f02f576c9bc6_78","name":"29ba","type":"P","href":null,"layout":null,"metadata":null,"text":"Instead, an enhanced version of HyDE will automatically learn a greater variety of relevance patterns above and beyond a baseline retrieval, and generate hypothetical documents that are aligned with different clusters of indexed chunks. I call this approach AutoHyDE.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":56,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":258,"end":266,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_79":{"__typename":"Paragraph","id":"f02f576c9bc6_79","name":"5a22","type":"P","href":null,"layout":null,"metadata":null,"text":"Before talking more about AutoHyDE, I have one final point to bring up regarding with the current HyDE implementation. I will not be addressing it in AutoHyDE, but I think it deserves some attention.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_80":{"__typename":"Paragraph","id":"f02f576c9bc6_80","name":"97cb","type":"P","href":null,"layout":null,"metadata":null,"text":"Recall again the averaging equation across embeddings, which manifests as the following line of code:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_81":{"__typename":"Paragraph","id":"f02f576c9bc6_81","name":"4206","type":"PRE","href":null,"layout":null,"metadata":null,"text":"list(np.array(embeddings).mean(axis=0))","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_82":{"__typename":"Paragraph","id":"f02f576c9bc6_82","name":"20f0","type":"P","href":null,"layout":null,"metadata":null,"text":"It‚Äôs a simple aggregation but does not make much sense to me, especially when you consider the multi-modal distribution of the embeddings. Let‚Äôs say you manage to construct the chained encoding function f(g(q, INST)) that nicely captures the diversity of the relevance patterns across all your document chunks. When you take averages across these multi-modal distributed encodings, you end up with an average that may not be relevant at all (think Jensen‚Äôs inequality kind of averaging). Of course, this all works well when you have a uni-modal distribution, but this is rarely a realistic assumption, and there is no way to verify it universally.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":203,"end":216,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_83":{"__typename":"Paragraph","id":"f02f576c9bc6_83","name":"826e","type":"P","href":null,"layout":null,"metadata":null,"text":"Anyway, on to AutoHyDE.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":14,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_84":{"__typename":"Paragraph","id":"f02f576c9bc6_84","name":"8099","type":"H3","href":null,"layout":null,"metadata":null,"text":"Section 3: AutoHyDE","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_85":{"__typename":"Paragraph","id":"f02f576c9bc6_85","name":"9e95","type":"H4","href":null,"layout":null,"metadata":null,"text":"Overview","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_86":{"__typename":"Paragraph","id":"f02f576c9bc6_86","name":"4d61","type":"P","href":null,"layout":null,"metadata":null,"text":"To recap, the main point of AutoHyDE is to automatically discover the variety of relevance patterns in your vector database and generate a variety of documents to improve coverage of these patterns.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":43,"end":99,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":128,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_87":{"__typename":"Paragraph","id":"f02f576c9bc6_87","name":"8dd7","type":"P","href":null,"layout":null,"metadata":null,"text":"Technically, I took the existing implementation of class HypotheticalDocumentEmbedder in LangChain and created new functions to achieve AutoHyDE, so that it works immediately as part of any RAG chain that you might have. This is the updated embed_query function that I wrote, and will explore each of the sub-methods in the following walkthrough.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":241,"end":252,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":50,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*oenoqdYqCyvlHYGQ9GEn1A.jpeg":{"__typename":"ImageMetadata","id":"1*oenoqdYqCyvlHYGQ9GEn1A.jpeg","originalHeight":1206,"originalWidth":1696,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_88":{"__typename":"Paragraph","id":"f02f576c9bc6_88","name":"e9e9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*oenoqdYqCyvlHYGQ9GEn1A.jpeg"},"text":"Image by Author: Src Code Snippet from AutoHyDE Repo","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_89":{"__typename":"Paragraph","id":"f02f576c9bc6_89","name":"eec7","type":"P","href":null,"layout":null,"metadata":null,"text":"üßëüèª‚Äçüíª Check out the implementation here if you are interested to try it out: https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":79,"end":125,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":78,"end":125,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_90":{"__typename":"Paragraph","id":"f02f576c9bc6_90","name":"b457","type":"P","href":null,"layout":null,"metadata":null,"text":"Here‚Äôs a quick demo of how to use it:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:0b8e21808f3ee4ec622424dfecaf7ed5":{"__typename":"MediaResource","id":"0b8e21808f3ee4ec622424dfecaf7ed5","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"auto-hyde-demo.py"},"Paragraph:f02f576c9bc6_91":{"__typename":"Paragraph","id":"f02f576c9bc6_91","name":"0e1c","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"Demo code for AutoHyDE","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:0b8e21808f3ee4ec622424dfecaf7ed5"}},"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_92":{"__typename":"Paragraph","id":"f02f576c9bc6_92","name":"c16d","type":"P","href":null,"layout":null,"metadata":null,"text":"To illustrate how AutoHyDE works and, I‚Äôm going to be using J. S. Mill‚Äôs Utilitarianism. I‚Äôm going to chunk it, index it and try to receive the relevant chunks based on a query.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":73,"end":87,"href":"https:\u002F\u002Fsacred-texts.com\u002Fphi\u002Fmill\u002Futil.txt","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_93":{"__typename":"Paragraph","id":"f02f576c9bc6_93","name":"7bc6","type":"P","href":null,"layout":null,"metadata":null,"text":"I chose this text not just because I spent countless hours struggling with it as an undergraduate back in the good old days of Philosophy 101, but also because it‚Äôs a good example of a text where it‚Äôs really difficult to write a custom prompt to generate hypothetical documents, even if you hold a PhD in Moral Philosophy with endless knowledge of Mill. Let‚Äôs just say that Mill‚Äôs writing is not the most straightforward and consistent, he‚Äôs really an expert at run-on sentences (so am I).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_94":{"__typename":"Paragraph","id":"f02f576c9bc6_94","name":"525f","type":"P","href":null,"layout":null,"metadata":null,"text":"As I walk through my implementation of AutoHyDE below, I‚Äôll have both high-level explanations and more technical descriptions for those interested in the repo.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":154,"end":158,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_95":{"__typename":"Paragraph","id":"f02f576c9bc6_95","name":"5aaf","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 1: Extract Key Words from Query","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_96":{"__typename":"Paragraph","id":"f02f576c9bc6_96","name":"720c","type":"P","href":null,"layout":null,"metadata":null,"text":"Imagine you received this essay assignment:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_97":{"__typename":"Paragraph","id":"f02f576c9bc6_97","name":"16fa","type":"BQ","href":null,"layout":null,"metadata":null,"text":"What is the relationship between justice and happiness?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_98":{"__typename":"Paragraph","id":"f02f576c9bc6_98","name":"c8ef","type":"P","href":null,"layout":null,"metadata":null,"text":"At this step 1, the LLM will be prompted to extract the key words, in this case:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_99":{"__typename":"Paragraph","id":"f02f576c9bc6_99","name":"a352","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003E\u003E\u003E Extracting Keywords from your Query‚Ä¶\n\u003E\u003E\u003E ‚Ä¶Keywords Extracted: ['relationship', 'justice', 'happiness']","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"plaintext"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_100":{"__typename":"Paragraph","id":"f02f576c9bc6_100","name":"c761","type":"P","href":null,"layout":null,"metadata":null,"text":"The key words are important in this case because they will be utilized in Step 3, in order to quickly (and dirtily) identify documents that you would have left out in a normal retrieval.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_101":{"__typename":"Paragraph","id":"f02f576c9bc6_101","name":"2511","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_102":{"__typename":"Paragraph","id":"f02f576c9bc6_102","name":"e9b2","type":"P","href":null,"layout":null,"metadata":null,"text":"This is the function I wrote to extract the keywords:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_103":{"__typename":"Paragraph","id":"f02f576c9bc6_103","name":"a8ec","type":"PRE","href":null,"layout":null,"metadata":null,"text":"@retry(tries=5)\ndef extract_keywords(\n    self, \n    text: str, \n    hypo_params: dict\n    ) -\u003E List[str]:\n\n    if hypo_params['verbose']:\n        print(f\"\\n\u003E\u003E\u003E Extracting Keywords from your Query...\")\n    \n    KEYWORD_EXTRACTION_PROMPT = \"\"\"\n    Your goal is to extract a list of keywords from an input phrase, sentence, or several sentences.\n\n    - You can only generate 1 to 5 keywords.\n    - Keywords should be nouns, issues, concepts\n    - Keywords should not include verbs, prepositions, pronouns\n    - Each keyword can only be one word long.\n    - If the input is just a single word, return that word as the only keyword.\n\n    {format_instructions}\n\n    The input is:\n    {input}\n    \"\"\"\n\n    class KeywordListSchema(BaseModel):\n        keywordList: list[str] = Field(description=\"list of one-word keywords based on a given phrase\")\n\n    parser = JsonOutputParser(pydantic_object=KeywordListSchema)\n\n    prompt = ChatPromptTemplate.from_template(\n        template=KEYWORD_EXTRACTION_PROMPT,\n        intput_variables = [\"input\"],\n        partial_variables = {\n            'format_instructions': parser.get_format_instructions()\n        }\n    )\n\n    keyword_extraction_chain = (\n        {'input': RunnablePassthrough()}\n        | prompt\n        | self.llm_chain\n        | parser\n    )\n    \n    keywords = keyword_extraction_chain.invoke(text)['keywordList']\n    if hypo_params['verbose']:\n        print(f\"\u003E\u003E\u003E ...Keywords Extracted: {keywords}\\n\")\n    \n    return keywords","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_104":{"__typename":"Paragraph","id":"f02f576c9bc6_104","name":"eaa0","type":"P","href":null,"layout":null,"metadata":null,"text":"Nothing fancy here, just using the JsonOutputParser to ensure that I get the exact output which is a list of keywords.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_105":{"__typename":"Paragraph","id":"f02f576c9bc6_105","name":"f548","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 2: Do initial retrieval","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_106":{"__typename":"Paragraph","id":"f02f576c9bc6_106","name":"6206","type":"P","href":null,"layout":null,"metadata":null,"text":"Apart from extracting the keywords, we‚Äôll also use the original query to do an initial retrieval. You‚Äôll have to ask yourself, how many chunks do you want to retrieve as part of your workflow?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_107":{"__typename":"Paragraph","id":"f02f576c9bc6_107","name":"44d9","type":"P","href":null,"layout":null,"metadata":null,"text":"Assuming that you initially intended to retrieve 20 documents to use as context for answering your question in RAG, what this step 2 will then do is to retrieve more than 20 documents so that we can explore potential documents that you would have neglected (for lack of a better word) due to your cut off of 20. This can be toggled using exploration_multiplier. With a baseline_k of 20 and exploration_multiplier of 5, we will be exploring 20 x 5 = 100 top documents based on cosine similarity.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":338,"end":360,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":369,"end":379,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":390,"end":412,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":247,"end":256,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":369,"end":379,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_108":{"__typename":"Paragraph","id":"f02f576c9bc6_108","name":"4759","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_109":{"__typename":"Paragraph","id":"f02f576c9bc6_109","name":"6ec0","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def do_init_retrieval(\n    self,\n    db: VectorStore, \n    text: str, \n    hypo_params: dict\n    ) -\u003E List[Tuple[Document, float]]:\n    \n    k = hypo_params['baseline_k'] * hypo_params['exploration_multiplier']\n    if hypo_params['verbose']:\n        print(f\"\\n\u003E\u003E\u003E Performing Initial Retrieval of {k} documents...\\n\")\n    docs = db.similarity_search_with_score(\n        text, \n        k=k\n    )\n    return docs","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_110":{"__typename":"Paragraph","id":"f02f576c9bc6_110","name":"3b37","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 3: Get neglected documents that contain the keywords","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_111":{"__typename":"Paragraph","id":"f02f576c9bc6_111","name":"01a2","type":"P","href":null,"layout":null,"metadata":null,"text":"So above and beyond the 20 documents that you would have retrieved from a straightforward retrieval, there are 80 documents that are now up for consideration that you would have neglected.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":178,"end":187,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_112":{"__typename":"Paragraph","id":"f02f576c9bc6_112","name":"2463","type":"P","href":null,"layout":null,"metadata":null,"text":"Out of these 80 documents, we want to find the subset of documents that would have been directly relevant to your original query. How do we do this efficiently? One simple way would just be to select the documents that contain any of the keywords extracted from step 1. There are ways to make this better, but I think keyword matching is already a quick and rather effective way to identify the first cut of neglected documents. Maybe we‚Äôll explore better ways to do this identification, but this will do for now.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_113":{"__typename":"Paragraph","id":"f02f576c9bc6_113","name":"35a6","type":"P","href":null,"layout":null,"metadata":null,"text":"This is what you will see as output from the code:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_114":{"__typename":"Paragraph","id":"f02f576c9bc6_114","name":"bede","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003E\u003E\u003E Checking 80 Docs ranked after 20 for presence of keyword‚Ä¶\n\u003E\u003E\u003E ‚Ä¶69 neglected Docs identified","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"plaintext"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_115":{"__typename":"Paragraph","id":"f02f576c9bc6_115","name":"3601","type":"P","href":null,"layout":null,"metadata":null,"text":"In the output above, we see that out of the 80 Documents, 69 contained one or more of the keywords [‚Äòrelationship‚Äô, ‚Äòjustice‚Äô, ‚Äòhappiness‚Äô].","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_116":{"__typename":"Paragraph","id":"f02f576c9bc6_116","name":"6f84","type":"P","href":null,"layout":null,"metadata":null,"text":"Intuitively, we can understand these as potentially relevant documents that would have been neglected in a normal retrieval of the top 20 documents.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_117":{"__typename":"Paragraph","id":"f02f576c9bc6_117","name":"600b","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_118":{"__typename":"Paragraph","id":"f02f576c9bc6_118","name":"1f87","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def get_remaining_docs_with_keywords(\n    self, \n    text: str, \n    init_docs: List[Tuple[Document, float]], \n    keywords: List[str], \n    hypo_params: dict\n    ) -\u003E List[Document]:\n\n    remaining_docs_with_keywords = list()\n    \n    if hypo_params['verbose']:\n        print(f\"\"\"\\n\u003E\u003E\u003E Checking {len(init_docs[hypo_params['baseline_k']:])} \n                Docs ranked after {hypo_params['baseline_k']} for presence of keyword...\"\"\")\n\n    for r in init_docs[hypo_params['baseline_k']:]:\n        page_content = r[0].page_content.lower()\n        for keyword in keywords:\n            if keyword.lower() in page_content:\n                remaining_docs_with_keywords.append(r)\n                continue\n                \n    if hypo_params['verbose']:\n        print(f\"\u003E\u003E\u003E ...{len(remaining_docs_with_keywords)} neglected Docs identified\\n\")\n    return remaining_docs_with_keywords","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_119":{"__typename":"Paragraph","id":"f02f576c9bc6_119","name":"f8b9","type":"P","href":null,"layout":null,"metadata":null,"text":"For each document, if any of the keywords are found in it, I add it to my list of neglected docs (remaining_docs_with_keywords).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":98,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_120":{"__typename":"Paragraph","id":"f02f576c9bc6_120","name":"f9ac","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 4: Cluster Neglected Documents","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_121":{"__typename":"Paragraph","id":"f02f576c9bc6_121","name":"1b26","type":"P","href":null,"layout":null,"metadata":null,"text":"This is an important step. We might be tempted to ask the LLM to reference each of these 69 neglected documents and write a hypothetical document for each of them based on the original user query.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_122":{"__typename":"Paragraph","id":"f02f576c9bc6_122","name":"6229","type":"P","href":null,"layout":null,"metadata":null,"text":"However, this is computationally expensive and not the most efficient way to discover the main relevance patterns that exist across these documents.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_123":{"__typename":"Paragraph","id":"f02f576c9bc6_123","name":"af09","type":"P","href":null,"layout":null,"metadata":null,"text":"Instead, using the embeddings, we will cluster the 69 documents. This allows us to discover the key relevant patterns that exist across the datasets. In the output below, we see that the 69 documents were subsequently clustered into 6 different groups.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_124":{"__typename":"Paragraph","id":"f02f576c9bc6_124","name":"6014","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003E\u003E\u003E Clustering neglected Docs...\n\u003E\u003E\u003E ...6 Clusters identifiedTechnical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"plaintext"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_125":{"__typename":"Paragraph","id":"f02f576c9bc6_125","name":"6be7","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_126":{"__typename":"Paragraph","id":"f02f576c9bc6_126","name":"03b7","type":"P","href":null,"layout":null,"metadata":null,"text":"How do we choose the number of clusters? Here I use HDBSCAN, which is a semi-supervised hierarchical clustering version of DBSCAN. This is how HDBSCAN is described in the docs:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":52,"end":59,"href":"https:\u002F\u002Flink.springer.com\u002Fchapter\u002F10.1007\u002F978-3-642-37456-2_14","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_127":{"__typename":"Paragraph","id":"f02f576c9bc6_127","name":"e813","type":"BQ","href":null,"layout":null,"metadata":null,"text":"The algorithm starts off much the same as DBSCAN: we transform the space according to density, exactly as DBSCAN does, and perform single linkage clustering on the transformed space. Instead of taking an epsilon value as a cut level for the dendrogram however, a different approach is taken: the dendrogram is condensed by viewing splits that result in a small number of points splitting off as points ‚Äòfalling out of a cluster‚Äô. This results in a smaller tree with fewer clusters that ‚Äòlose points‚Äô. That tree can then be used to select the most stable or persistent clusters.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_128":{"__typename":"Paragraph","id":"f02f576c9bc6_128","name":"366f","type":"P","href":null,"layout":null,"metadata":null,"text":"Intuitively, min_samples is a parameter that controls the minimum number of neighbours to a core point. The greater min_samples is, more points will be labelled as noise from the clustering. For now, I will keep it as 1 and leave the experimentation to later, but in general \"HDBSCAN is not that sensitive to it and we can choose some sensible defaults, but this remains the biggest weakness of the algorithm.\" min_cluster_size is used to identify points ‚Äòfalling out of a cluster‚Äô or splitting to form two new clusters.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":13,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":116,"end":127,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":411,"end":427,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":275,"end":410,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_129":{"__typename":"Paragraph","id":"f02f576c9bc6_129","name":"cd40","type":"P","href":null,"layout":null,"metadata":null,"text":"After labelling the documents, we drop those labelled as -1 (no clusters), and then create a dictionary with keys being the labelled group number, and the values being the page contents for each document.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_130":{"__typename":"Paragraph","id":"f02f576c9bc6_130","name":"2a3a","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def cluster_docs(\n    self, \n    remaining_docs_with_keywords: List[Document], \n    hypo_params: dict\n    ) -\u003E Dict[int, List[str]]:\n    \n    from hdbscan import HDBSCAN\n    \n    if hypo_params['verbose']:\n        print(f\"\\n\u003E\u003E\u003E Clustering neglected Docs...\")\n    \n    embeddings = self.embed_documents([\n        r[0].page_content \n        for r in remaining_docs_with_keywords], \n        {'verbose': False})\n    hdb = HDBSCAN(min_samples=1, min_cluster_size=3).fit(embeddings)\n    remaining_docs_with_cat = filter(lambda x: x[1] != -1, zip([r[0].page_content for r in remaining_docs_with_keywords], hdb.labels_))\n    \n    cat_dict = {}\n\n    for page_content, cat in remaining_docs_with_cat:\n        if cat not in cat_dict:\n            cat_dict[cat] = [page_content]\n        else:\n            cat_dict[cat].append(page_content)\n            \n    if hypo_params['verbose']:\n        print(f\"\u003E\u003E\u003E ...{len(cat_dict)} Clusters identified\\n\")\n            \n    return cat_dict","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_131":{"__typename":"Paragraph","id":"f02f576c9bc6_131","name":"b1f8","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 5: Generate Hypothetical Documents","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_132":{"__typename":"Paragraph","id":"f02f576c9bc6_132","name":"6ac3","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, for each of our 6 clusters that represent groups of relevance patterns, we will ask our LLM to reference the documents belonging to the cluster to create a new hypothetical document that is similar to those documents. This is an important step that differentiates the AutoHyDE approach from existing HyDE approaches. In AutoHyDE, the relevance patterns are automatically learned and then used to generate the hypothetical documents. No customized prompt writing is required at all.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_133":{"__typename":"Paragraph","id":"f02f576c9bc6_133","name":"2cea","type":"P","href":null,"layout":null,"metadata":null,"text":"Here‚Äôs an example for the first cluster in the code below. You can see that the LLM referenced every neglected in the first neglected document cluster, and created a hypothetical document based on the original query. This is done for all the clusters. A quick look at the text below, and you can see that it really does emulate the convluted style of Mill quite well, and you did not have to write a custom prompt to try and capture this style. All it took was few-shot prompting under the hood.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*wyhHIToAespRTk63DHagjA.png":{"__typename":"ImageMetadata","id":"1*wyhHIToAespRTk63DHagjA.png","originalHeight":874,"originalWidth":1068,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:f02f576c9bc6_134":{"__typename":"Paragraph","id":"f02f576c9bc6_134","name":"7a8d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*wyhHIToAespRTk63DHagjA.png"},"text":"Image by Author: Hypothetical Document generated from AutoHyDE","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_135":{"__typename":"Paragraph","id":"f02f576c9bc6_135","name":"d2cf","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_136":{"__typename":"Paragraph","id":"f02f576c9bc6_136","name":"b85b","type":"P","href":null,"layout":null,"metadata":null,"text":"What the function below does is to generate hypothetical documents for each identified cluster of documents. Similar to before, we use the JsonOutputParser to ensure that we get the desired output. I found that a few-shot approach generally works as you see in HYPOTHETICAL_DOCUMENT_PROMPT below:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":261,"end":289,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_137":{"__typename":"Paragraph","id":"f02f576c9bc6_137","name":"364f","type":"PRE","href":null,"layout":null,"metadata":null,"text":"@retry(tries=5)\ndef generate_hypo_docs(\n    self, \n    text: str, \n    cat_dict: Dict[int, List[str]], \n    hypo_params: dict\n    ) -\u003E List[str]:\n    \n    hypo_docs = list()\n    \n    if hypo_params['verbose']:\n        print(f\"\\n\u003E\u003E\u003E Generating Hypothetical Documents for each Doc Cluster...\\n\")\n\n    HYPOTHETICAL_DOCUMENT_PROMPT = \"\"\"\n    Your instruction is to generate a single hypothetical document from an input.\n    - This hypothetical document must be similar in style, tone and voice as examples you are provided with.\n    - This hypothetical document must appear like it was written by the same author as the examples you are provided with.\n    - This hypothetical document must also be similar in length with the examples you are provided with.\n\n    {format_instructions}\n\n    ### EXAMPLES ###\n    Below are some examples of hypothetical documents, all written by the same author, in pairs of \u003CInput\u003E and \u003CHypothetical Document\u003E:\n\n    {ref_documents}\n\n    ### INSTRUCTION ###\n    Now generate a new hypothetical document. \n\n    \u003CInput\u003E\n    {input}\n    \u003CHypothetical Document\u003E\n\n    \"\"\"\n\n    class HypotheticalDocumentSchema(BaseModel):\n        hypotheticalDocument: str = Field(description=\"a hypothetical document given an input word, phrase or question\")\n\n    parser = JsonOutputParser(pydantic_object=HypotheticalDocumentSchema)\n\n    prompt = ChatPromptTemplate.from_template(\n        template=HYPOTHETICAL_DOCUMENT_PROMPT,\n        intput_variables = [\"input\", \"ref_documents\"],\n        partial_variables = {\n            'format_instructions': parser.get_format_instructions()\n        }\n    )\n\n    hypothetical_document_chain = (\n        {'input': RunnablePassthrough(), 'ref_documents': RunnablePassthrough()}\n        | prompt\n        | self.llm_chain\n        | parser\n    )\n\n    cat_ii = 1\n    for cat in cat_dict.keys():\n\n        ref_doc_string = \"\"\n        doc_ii = 1\n        for doc in cat_dict[cat]:\n            ref_doc_string += f\"\\n\\n\u003CInput\u003E\"\n            ref_doc_string += text\n            ref_doc_string += f\"\\n\\n\u003CHypothetical Document\u003E\\n\"\n            ref_doc_string += f'{{\"hypotheticalDocument\": \"{doc}\"}}'\n            doc_ii += 1\n\n        hypo_doc = hypothetical_document_chain.invoke(\n            {'input': text, 'ref_documents': ref_doc_string}\n        )['hypotheticalDocument']\n\n        if hypo_params['verbose']:\n            print(f\"\\n### Hypo Doc {cat_ii} ###\")\n            print(hypo_doc+'\\n')\n        \n        hypo_docs.append(hypo_doc)\n        \n        cat_ii += 1\n        \n    return hypo_docs","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_138":{"__typename":"Paragraph","id":"f02f576c9bc6_138","name":"7037","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 6+7: Embed & Combine","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_139":{"__typename":"Paragraph","id":"f02f576c9bc6_139","name":"561f","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, with the list of hypothetical documents, one for each cluster, we embed the hypothetical documents (step 6) and combine them into a single embedding vector (step 7).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_140":{"__typename":"Paragraph","id":"f02f576c9bc6_140","name":"929e","type":"P","href":null,"layout":null,"metadata":null,"text":"This step is similar to the original implementation, where hypothetical documents are embedded and then averaged to get a final vector.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_141":{"__typename":"Paragraph","id":"f02f576c9bc6_141","name":"bc1a","type":"P","href":null,"layout":null,"metadata":null,"text":"‚ö†Ô∏è As I mentioned earlier, I don‚Äôt think this step really makes sense to capture the broader heterogeneity of documents, especially when we‚Äôre trying to discover and generate documents with different relevance patterns. But to keep it simpler and maintain compatibility with the existing LangChain implementation (the point being that you can use this immediately with other component in LangChain RAG), we‚Äôll keep this for now to output a single embedding vector, and perhaps deal with the aggregation limitation later. If you‚Äôre convinced that simple averaging is not a good idea, you can always just stop at Step 6.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_142":{"__typename":"Paragraph","id":"f02f576c9bc6_142","name":"75d7","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003E\u003E\u003E Generating embeddings for hypothetical documents...\n\u003E\u003E\u003E Combining embeddings for hypothetical documents...","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"plaintext"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_143":{"__typename":"Paragraph","id":"f02f576c9bc6_143","name":"4eb6","type":"PRE","href":null,"layout":null,"metadata":null,"text":"hyde_embedding[:10]\n\u003E\u003E\u003E [\n   0.015876703333653572,\n  -0.013635452586265312,\n   0.021941788843565697,\n  -0.02570370381768275,\n  -0.015861831315729033,\n  -0.0003427757204382006,\n   0.0027591148428962454,\n  -0.00883308151544041,\n  -0.014893267477206646,\n  -0.020748802766928837\n]","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_144":{"__typename":"Paragraph","id":"f02f576c9bc6_144","name":"0b10","type":"P","href":null,"layout":null,"metadata":null,"text":"Technical Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_145":{"__typename":"Paragraph","id":"f02f576c9bc6_145","name":"05ef","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def embed_documents(self, texts: List[str], hypo_params) -\u003E List[List[float]]:\n    \"\"\"Call the base embeddings.\"\"\"\n    if hypo_params['verbose']:\n        print(\"\\n\u003E\u003E\u003E Generating embeddings for hypothetical documents...\\n\")\n    return self.base_embeddings.embed_documents(texts)\n\ndef combine_embeddings(self, embeddings: List[List[float]], hypo_params) -\u003E List[float]:\n    \"\"\"Combine embeddings into final embeddings.\"\"\"\n    if hypo_params['verbose']:\n        print(\"\\n\u003E\u003E\u003E Combining embeddings for hypothetical documents...\\n\")\n    return list(np.array(embeddings).mean(axis=0))","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_146":{"__typename":"Paragraph","id":"f02f576c9bc6_146","name":"06df","type":"P","href":null,"layout":null,"metadata":null,"text":"As before, the repo where I implemented this enhanced version of HyDE can be found here at this repo.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":96,"end":100,"href":"https:\u002F\u002Fgithub.com\u002Fianhojy\u002Fauto-hyde\u002Ftree\u002Fmain","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_147":{"__typename":"Paragraph","id":"f02f576c9bc6_147","name":"c6a3","type":"H3","href":null,"layout":null,"metadata":null,"text":"Section 4: Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_148":{"__typename":"Paragraph","id":"f02f576c9bc6_148","name":"ecb9","type":"P","href":null,"layout":null,"metadata":null,"text":"And that‚Äôs the end of the AutoHyDE demo!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":26,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_149":{"__typename":"Paragraph","id":"f02f576c9bc6_149","name":"00f7","type":"P","href":null,"layout":null,"metadata":null,"text":"To recap, the main improvement being made in AutoHyde is the way the hypothetical documents are being generated. Instead of using a fixed prompt to generate these documents (whether it be pre-defined in LangChain or customised by the user), I have devised a framework to automatically discover the underlying relevance patterns across all the documents that may be neglected by a baseline retrieval, and to generate hypothetical documents for each of these patterns.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":365,"end":375,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_150":{"__typename":"Paragraph","id":"f02f576c9bc6_150","name":"fd00","type":"P","href":null,"layout":null,"metadata":null,"text":"In this manner, we are able to adapt HyDE for a wider variety of tasks and contexts, as well as to accommodate retrieval over an index which is heterogeneous in relevance pattern.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_151":{"__typename":"Paragraph","id":"f02f576c9bc6_151","name":"64f4","type":"P","href":null,"layout":null,"metadata":null,"text":"If you‚Äôve made it to the end, I‚Äôm always happy to hear your thoughts on this approach, and feel free to make suggestions to my implementation as well!","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_152":{"__typename":"Paragraph","id":"f02f576c9bc6_152","name":"9639","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Give a üëçüèª, leave a üí¨ and ü§ùüèª with me on LinkedIn!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":44,"end":52,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fianhojy\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_153":{"__typename":"Paragraph","id":"f02f576c9bc6_153","name":"bec6","type":"H4","href":null,"layout":null,"metadata":null,"text":"References","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_154":{"__typename":"Paragraph","id":"f02f576c9bc6_154","name":"885c","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Unsupervised Dense Information Retrieval with Contrastive Learning (https:\u002F\u002Farxiv.org\u002Fpdf\u002F2112.09118.pdf)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":68,"end":104,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2112.09118.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_155":{"__typename":"Paragraph","id":"f02f576c9bc6_155","name":"18b5","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Precise Zero-Shot Dense Retrieval without Relevance Labels (https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":60,"end":96,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2212.10496.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_156":{"__typename":"Paragraph","id":"f02f576c9bc6_156","name":"a917","type":"ULI","href":null,"layout":null,"metadata":null,"text":"LangChain HyDE Repo (https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":21,"end":111,"href":"https:\u002F\u002Fgithub.com\u002Flangchain-ai\u002Flangchain\u002Fblob\u002Fmaster\u002Flibs\u002Flangchain\u002Flangchain\u002Fchains\u002Fhyde\u002Fbase.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:f02f576c9bc6_157":{"__typename":"Paragraph","id":"f02f576c9bc6_157","name":"2a45","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Original HyDE Implementation (https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":30,"end":125,"href":"https:\u002F\u002Fgithub.com\u002Ftexttron\u002Fhyde\u002Fblob\u002F74101c5157e04f7b57559e7da8ef4a4e5b6da82b\u002Fsrc\u002Fhyde\u002Fhyde.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:2ae680787402":{"__typename":"CollectionViewerEdge","id":"collectionId:7f60cf5620c9-viewerId:2ae680787402","isEditor":false,"isMuting":false},"Membership:34d3b27be11e":{"__typename":"Membership","tier":"MEMBER","id":"34d3b27be11e"},"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png":{"__typename":"ImageMetadata","id":"1*cFFKn8rFH4ZndmaYeAs6iQ.png","originalWidth":2381,"originalHeight":743},"Tag:llm":{"__typename":"Tag","id":"llm","displayTitle":"Llm","normalizedTagSlug":"llm"},"Tag:ai":{"__typename":"Tag","id":"ai","displayTitle":"AI","normalizedTagSlug":"ai"},"Tag:langchain":{"__typename":"Tag","id":"langchain","displayTitle":"Langchain","normalizedTagSlug":"langchain"},"Tag:data-science":{"__typename":"Tag","id":"data-science","displayTitle":"Data Science","normalizedTagSlug":"data-science"},"Tag:hands-on-tutorials":{"__typename":"Tag","id":"hands-on-tutorials","displayTitle":"Hands On Tutorials","normalizedTagSlug":"hands-on-tutorials"},"Post:619e58cdbd8e":{"__typename":"Post","id":"619e58cdbd8e","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"8add","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"fed5","startIndex":17,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"d447","startIndex":59,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"35ec","startIndex":84,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"52bd","startIndex":147,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"6cf4","startIndex":153,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:f02f576c9bc6_0"},{"__ref":"Paragraph:f02f576c9bc6_1"},{"__ref":"Paragraph:f02f576c9bc6_2"},{"__ref":"Paragraph:f02f576c9bc6_3"},{"__ref":"Paragraph:f02f576c9bc6_4"},{"__ref":"Paragraph:f02f576c9bc6_5"},{"__ref":"Paragraph:f02f576c9bc6_6"},{"__ref":"Paragraph:f02f576c9bc6_7"},{"__ref":"Paragraph:f02f576c9bc6_8"},{"__ref":"Paragraph:f02f576c9bc6_9"},{"__ref":"Paragraph:f02f576c9bc6_10"},{"__ref":"Paragraph:f02f576c9bc6_11"},{"__ref":"Paragraph:f02f576c9bc6_12"},{"__ref":"Paragraph:f02f576c9bc6_13"},{"__ref":"Paragraph:f02f576c9bc6_14"},{"__ref":"Paragraph:f02f576c9bc6_15"},{"__ref":"Paragraph:f02f576c9bc6_16"},{"__ref":"Paragraph:f02f576c9bc6_17"},{"__ref":"Paragraph:f02f576c9bc6_18"},{"__ref":"Paragraph:f02f576c9bc6_19"},{"__ref":"Paragraph:f02f576c9bc6_20"},{"__ref":"Paragraph:f02f576c9bc6_21"},{"__ref":"Paragraph:f02f576c9bc6_22"},{"__ref":"Paragraph:f02f576c9bc6_23"},{"__ref":"Paragraph:f02f576c9bc6_24"},{"__ref":"Paragraph:f02f576c9bc6_25"},{"__ref":"Paragraph:f02f576c9bc6_26"},{"__ref":"Paragraph:f02f576c9bc6_27"},{"__ref":"Paragraph:f02f576c9bc6_28"},{"__ref":"Paragraph:f02f576c9bc6_29"},{"__ref":"Paragraph:f02f576c9bc6_30"},{"__ref":"Paragraph:f02f576c9bc6_31"},{"__ref":"Paragraph:f02f576c9bc6_32"},{"__ref":"Paragraph:f02f576c9bc6_33"},{"__ref":"Paragraph:f02f576c9bc6_34"},{"__ref":"Paragraph:f02f576c9bc6_35"},{"__ref":"Paragraph:f02f576c9bc6_36"},{"__ref":"Paragraph:f02f576c9bc6_37"},{"__ref":"Paragraph:f02f576c9bc6_38"},{"__ref":"Paragraph:f02f576c9bc6_39"},{"__ref":"Paragraph:f02f576c9bc6_40"},{"__ref":"Paragraph:f02f576c9bc6_41"},{"__ref":"Paragraph:f02f576c9bc6_42"},{"__ref":"Paragraph:f02f576c9bc6_43"},{"__ref":"Paragraph:f02f576c9bc6_44"},{"__ref":"Paragraph:f02f576c9bc6_45"},{"__ref":"Paragraph:f02f576c9bc6_46"},{"__ref":"Paragraph:f02f576c9bc6_47"},{"__ref":"Paragraph:f02f576c9bc6_48"},{"__ref":"Paragraph:f02f576c9bc6_49"},{"__ref":"Paragraph:f02f576c9bc6_50"},{"__ref":"Paragraph:f02f576c9bc6_51"},{"__ref":"Paragraph:f02f576c9bc6_52"},{"__ref":"Paragraph:f02f576c9bc6_53"},{"__ref":"Paragraph:f02f576c9bc6_54"},{"__ref":"Paragraph:f02f576c9bc6_55"},{"__ref":"Paragraph:f02f576c9bc6_56"},{"__ref":"Paragraph:f02f576c9bc6_57"},{"__ref":"Paragraph:f02f576c9bc6_58"},{"__ref":"Paragraph:f02f576c9bc6_59"},{"__ref":"Paragraph:f02f576c9bc6_60"},{"__ref":"Paragraph:f02f576c9bc6_61"},{"__ref":"Paragraph:f02f576c9bc6_62"},{"__ref":"Paragraph:f02f576c9bc6_63"},{"__ref":"Paragraph:f02f576c9bc6_64"},{"__ref":"Paragraph:f02f576c9bc6_65"},{"__ref":"Paragraph:f02f576c9bc6_66"},{"__ref":"Paragraph:f02f576c9bc6_67"},{"__ref":"Paragraph:f02f576c9bc6_68"},{"__ref":"Paragraph:f02f576c9bc6_69"},{"__ref":"Paragraph:f02f576c9bc6_70"},{"__ref":"Paragraph:f02f576c9bc6_71"},{"__ref":"Paragraph:f02f576c9bc6_72"},{"__ref":"Paragraph:f02f576c9bc6_73"},{"__ref":"Paragraph:f02f576c9bc6_74"},{"__ref":"Paragraph:f02f576c9bc6_75"},{"__ref":"Paragraph:f02f576c9bc6_76"},{"__ref":"Paragraph:f02f576c9bc6_77"},{"__ref":"Paragraph:f02f576c9bc6_78"},{"__ref":"Paragraph:f02f576c9bc6_79"},{"__ref":"Paragraph:f02f576c9bc6_80"},{"__ref":"Paragraph:f02f576c9bc6_81"},{"__ref":"Paragraph:f02f576c9bc6_82"},{"__ref":"Paragraph:f02f576c9bc6_83"},{"__ref":"Paragraph:f02f576c9bc6_84"},{"__ref":"Paragraph:f02f576c9bc6_85"},{"__ref":"Paragraph:f02f576c9bc6_86"},{"__ref":"Paragraph:f02f576c9bc6_87"},{"__ref":"Paragraph:f02f576c9bc6_88"},{"__ref":"Paragraph:f02f576c9bc6_89"},{"__ref":"Paragraph:f02f576c9bc6_90"},{"__ref":"Paragraph:f02f576c9bc6_91"},{"__ref":"Paragraph:f02f576c9bc6_92"},{"__ref":"Paragraph:f02f576c9bc6_93"},{"__ref":"Paragraph:f02f576c9bc6_94"},{"__ref":"Paragraph:f02f576c9bc6_95"},{"__ref":"Paragraph:f02f576c9bc6_96"},{"__ref":"Paragraph:f02f576c9bc6_97"},{"__ref":"Paragraph:f02f576c9bc6_98"},{"__ref":"Paragraph:f02f576c9bc6_99"},{"__ref":"Paragraph:f02f576c9bc6_100"},{"__ref":"Paragraph:f02f576c9bc6_101"},{"__ref":"Paragraph:f02f576c9bc6_102"},{"__ref":"Paragraph:f02f576c9bc6_103"},{"__ref":"Paragraph:f02f576c9bc6_104"},{"__ref":"Paragraph:f02f576c9bc6_105"},{"__ref":"Paragraph:f02f576c9bc6_106"},{"__ref":"Paragraph:f02f576c9bc6_107"},{"__ref":"Paragraph:f02f576c9bc6_108"},{"__ref":"Paragraph:f02f576c9bc6_109"},{"__ref":"Paragraph:f02f576c9bc6_110"},{"__ref":"Paragraph:f02f576c9bc6_111"},{"__ref":"Paragraph:f02f576c9bc6_112"},{"__ref":"Paragraph:f02f576c9bc6_113"},{"__ref":"Paragraph:f02f576c9bc6_114"},{"__ref":"Paragraph:f02f576c9bc6_115"},{"__ref":"Paragraph:f02f576c9bc6_116"},{"__ref":"Paragraph:f02f576c9bc6_117"},{"__ref":"Paragraph:f02f576c9bc6_118"},{"__ref":"Paragraph:f02f576c9bc6_119"},{"__ref":"Paragraph:f02f576c9bc6_120"},{"__ref":"Paragraph:f02f576c9bc6_121"},{"__ref":"Paragraph:f02f576c9bc6_122"},{"__ref":"Paragraph:f02f576c9bc6_123"},{"__ref":"Paragraph:f02f576c9bc6_124"},{"__ref":"Paragraph:f02f576c9bc6_125"},{"__ref":"Paragraph:f02f576c9bc6_126"},{"__ref":"Paragraph:f02f576c9bc6_127"},{"__ref":"Paragraph:f02f576c9bc6_128"},{"__ref":"Paragraph:f02f576c9bc6_129"},{"__ref":"Paragraph:f02f576c9bc6_130"},{"__ref":"Paragraph:f02f576c9bc6_131"},{"__ref":"Paragraph:f02f576c9bc6_132"},{"__ref":"Paragraph:f02f576c9bc6_133"},{"__ref":"Paragraph:f02f576c9bc6_134"},{"__ref":"Paragraph:f02f576c9bc6_135"},{"__ref":"Paragraph:f02f576c9bc6_136"},{"__ref":"Paragraph:f02f576c9bc6_137"},{"__ref":"Paragraph:f02f576c9bc6_138"},{"__ref":"Paragraph:f02f576c9bc6_139"},{"__ref":"Paragraph:f02f576c9bc6_140"},{"__ref":"Paragraph:f02f576c9bc6_141"},{"__ref":"Paragraph:f02f576c9bc6_142"},{"__ref":"Paragraph:f02f576c9bc6_143"},{"__ref":"Paragraph:f02f576c9bc6_144"},{"__ref":"Paragraph:f02f576c9bc6_145"},{"__ref":"Paragraph:f02f576c9bc6_146"},{"__ref":"Paragraph:f02f576c9bc6_147"},{"__ref":"Paragraph:f02f576c9bc6_148"},{"__ref":"Paragraph:f02f576c9bc6_149"},{"__ref":"Paragraph:f02f576c9bc6_150"},{"__ref":"Paragraph:f02f576c9bc6_151"},{"__ref":"Paragraph:f02f576c9bc6_152"},{"__ref":"Paragraph:f02f576c9bc6_153"},{"__ref":"Paragraph:f02f576c9bc6_154"},{"__ref":"Paragraph:f02f576c9bc6_155"},{"__ref":"Paragraph:f02f576c9bc6_156"},{"__ref":"Paragraph:f02f576c9bc6_157"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:ca061af8c7b7"},"inResponseToEntityType":null,"isLocked":true,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_UGC","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fautohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e","primaryTopic":{"__ref":"Topic:1eca0103fff3"},"topics":[{"__typename":"Topic","slug":"artificial-intelligence"},{"__typename":"Topic","slug":"machine-learning"},{"__typename":"Topic","slug":"data-science"},{"__typename":"Topic","slug":"programming"}],"isPublished":true,"latestPublishedVersion":"f02f576c9bc6","visibility":"LOCKED","postResponses":{"__typename":"PostResponses","count":6},"createdAt":1711300283807,"firstPublishedAt":1712250360920,"latestPublishedAt":1712250360920,"clapCount":529,"allowResponses":true,"isLimitedState":false,"title":"AutoHyDE: Making HyDE Better for Advanced LLM RAG","isSeries":false,"sequence":null,"uniqueSlug":"autohyde-making-hyde-better-for-advanced-llm-rag-619e58cdbd8e","socialTitle":"","socialDek":"","noIndex":null,"canonicalUrl":"","metaDescription":"","readingTime":18.423899371069183,"previewContent":{"__typename":"PreviewContent","subtitle":"Introducing AutoHyDE, a framework for improving the effectiveness, coverage and adaptability of HyDE for Advanced LLM RAG Applications"},"previewImage":{"__ref":"ImageMetadata:1*BoEb2eI8BFOectBr5y0zjA.png"},"isShortform":false,"seoTitle":"","updatedAt":1713750504768,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:llm"},{"__ref":"Tag:ai"},{"__ref":"Tag:langchain"},{"__ref":"Tag:data-science"},{"__ref":"Tag:hands-on-tutorials"}],"isNewsletter":false,"statusForCollection":"APPROVED","pendingCollection":null,"detectedLanguage":"en","wordCount":4582,"layerCake":1},"CreditCard:{}":{"__typename":"CreditCard","expirationMonth":5,"expirationYear":2028},"MembershipStatus:{}":{"__typename":"MembershipStatus","isCancelled":false,"paymentProvider":"BRAINTREE","currentPeriodEndsAt":1716163200000,"hasRecurringPaymentFailedMoreThanOnce":false,"hasCreditCardExpired":false,"paymentFailedAt":null,"paymentMethod":{"__ref":"CreditCard:{}"}}}</script><script></script><script></script><script></script><script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script>
<script></script><script>window.main();</script><script></script>
</body>